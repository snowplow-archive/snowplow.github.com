<!DOCTYPE html>
<html>
<head>
	
	<title>Writing Hive UDFs - a tutorial - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="description" content="" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->
	<div id="universe">
		<div id="container">
			<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
			<div id="contents">
		<div class="post">
			08 Feb 2013
			<h1>Writing Hive UDFs - a tutorial</h1>
			 <span class="author">Author: <a href="/alex.html" rel="author">Alex Dean </a></span>
			<p><em>Snowplow&#8217;s own <a href='https://github.com/alexanderdean'>Alexander Dean</a> was recently asked to write an article for the <a href='http://sdjournal.org/apache-hadoop-ecosystem/?a_aid=bartoszmiedeksza&amp;a_bid=45f0d439'>Software Developer&#8217;s Journal edition on Hadoop</a> The kind folks at the Software Developer&#8217;s Journal have allowed us to reprint his article in full below.</em></p>

<p><em>Alex started writing Hive UDFs as part of the process to write the <a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'>Snowplow log deserializer</a> - the custom SerDe used to parse Snowplow logs generated by the Cloudfront and Clojure collectors so they can be processed in the Snowplow ETL step.</em></p>

<h2 id='article_synopsis'>Article Synopsis</h2>

<p>In this article you will learn how to write a user-defined function (&#8220;UDF&#8221;) to work with the Apache Hive platform. We will start gently with an introduction to Hive, then move on to developing the UDF and writing tests for it. We will write our UDF in Java, but use Scala&#8217;s SBT as our build tool and write our tests in Scala with Specs2.</p>

<p>In order to get the most out of this article, you should be comfortable programming in Java. You do not need to have any experience with Apache Hive, HiveQL (the Hive query language) or indeed Hive UDFs - I will introduce all of these concepts from first principles. Experience with Scala is advantageous, but not necessary.</p>
<!--more-->
<h2 id='introduction'>Introduction</h2>

<p>Before we start: my name is Alex Dean, and I am the co-founder of Snowplow (http://snowplowanalytics.com/), an open-source web analytics platform built on top of Apache Hadoop and Apache Hive. My experience writing Java code to extend the Hive platform comes from Snowplow, where we built a core piece of our launch platform using a Hive deserializer (https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers).</p>

<p>So, what is Apache Hive, and what would you want a Hive UDF for? Hive is a data warehouse system built on top of Hadoop for ad-hoc queries and processing of large datasets. Now an Apache Software Foundation project, Hive was originally developed at Facebook, where analysts and data scientists wanted a SQL-like abstraction over traditional Hadoop MapReduce. As such, the key distinguishing feature of Hive is the SQL-like query language HiveQL. An example HiveQL query might look like this:</p>

<p>Listing 1: An example HiveQL query</p>
<div class='highlight'><pre><code class='mysql'><span class='k'>SELECT</span> 
<span class='n'>dt</span><span class='p'>,</span>
<span class='nf'>COUNT</span><span class='p'>(</span><span class='k'>DISTINCT</span> <span class='p'>(</span><span class='n'>user_id</span><span class='p'>))</span>
<span class='k'>FROM</span> <span class='n'>events</span>
<span class='k'>GROUP</span> <span class='k'>BY</span> <span class='n'>dt</span> <span class='p'>;</span>
</code></pre></div>
<p>This is actually a standard Snowplow query to calculate the number of unique visitors to a website by day. So what happens when an analyst runs this query in Hive? Simply this:</p>

<ol>
<li>Hive converts the query into the simplest possible set of MapReduce jobs</li>

<li>The MapReduce job or jobs is run on the Hadoop platform</li>

<li>The generated result set is then returned to the user&#8217;s console</li>
</ol>

<p>Certainly this is a powerful abstraction over MapReduce jobs, which can be tedious and difficult to write by hand. And HiveQL has a lot of power - there is very little in the ANSI SQL standard which is not available in HiveQL. Nonetheless, sometimes the Hive user will need more power, and for these occasions Hive has three main extension points:</p>

<ol>
<li>User-defined functions (&#8220;UDFs&#8221;), which provide a way of extending the functionality of Hive with a function (written in Java) that can be evaluated in HiveQL statements</li>

<li>Custom serializers and/or deserializers (&#8220;serdes&#8221;), which provide a way of either deserializing a custom file format stored on HDFS to a POJO (plain old Java object), or serializing a POJO to a custom file format (or both)</li>

<li>Custom mappers/reducers, which allow you to add custom map or reduce steps into your Hive query. These map/reduce steps can be written in any programming language - not just Java</li>
</ol>

<p>We will not consider serdes or custom mappers/reducers further in this article - we hope to write further articles on each of these in the future.</p>

<p>Now that we understand why you might write a UDF for Hive, let&#8217;s crack on and start writing one!</p>

<h2 id='setting_up_our_project'>Setting up our project</h2>

<p>We will be writing a relatively simple UDF - one which generates a converts a string in Hive to upper-case. Note that a version of this function is actually built into Hive as the UPPER function - for a full list of built-in UDFs in Hive, please see: https://cwiki.apache.org/Hive/languagemanual-udf.html</p>

<p>As mentioned previously, we will write our UDF in Java - but we will wrap our Java core in a Scala project (with Scala tests), because at Snowplow we much prefer writing Scala to Java. We will use SBT, the Scala build tool, to configure our project - this is an alternative to Maven or similar; SBT handles mixed Java and Scala projects perfectly well.</p>

<p>First, let&#8217;s create a directory for our project, and add a file, project.sbt into the project root, which contains:</p>

<p>Listing 2: Our project.sbt build file</p>
<div class='highlight'><pre><code class='scala'><span class='n'>name</span> <span class='o'>:=</span> <span class='s'>&quot;hive-example-udf&quot;</span>
<span class='n'>version</span> <span class='o'>:=</span> <span class='s'>&quot;0.0.1&quot;</span>
<span class='n'>organization</span> <span class='o'>:=</span> <span class='s'>&quot;com.snowplowanalytics&quot;</span>
<span class='n'>scalaVersion</span> <span class='o'>:=</span> <span class='s'>&quot;2.9.2&quot;</span>
<span class='n'>scalacOptions</span> <span class='o'>++=</span> <span class='nc'>Seq</span><span class='o'>(</span><span class='s'>&quot;-unchecked&quot;</span><span class='o'>,</span> <span class='s'>&quot;-deprecation&quot;</span><span class='o'>)</span>
<span class='n'>resolvers</span> <span class='o'>+=</span> <span class='s'>&quot;CDH4&quot;</span> <span class='n'>at</span> <span class='s'>&quot;https://repository.cloudera.com/artifactory/cloudera-repos/&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.apache.hadoop&quot;</span> <span class='o'>%</span>  <span class='s'>&quot;hadoop-core&quot;</span>        <span class='o'>%</span> <span class='s'>&quot;0.20.2&quot;</span>      <span class='o'>%</span> <span class='s'>&quot;provided&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.apache.hive&quot;</span>   <span class='o'>%</span>  <span class='s'>&quot;hive-exec&quot;</span>          <span class='o'>%</span> <span class='s'>&quot;0.8.1&quot;</span>       <span class='o'>%</span> <span class='s'>&quot;provided&quot;</span>
<span class='n'>libraryDependencies</span> <span class='o'>+=</span> <span class='s'>&quot;org.specs2&quot;</span>        <span class='o'>%%</span> <span class='s'>&quot;specs2&quot;</span>             <span class='o'>%</span> <span class='s'>&quot;1.12.1&quot;</span>      <span class='o'>%</span> <span class='s'>&quot;test&quot;</span>
</code></pre></div>
<p>This is a simple project configuration which names and versions our project, and also adds Hadoop, Hive and Specs2 (our testing library) as dependencies. If you do not have SBT already installed, you can find instructions here http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html</p>

<h2 id='writing_our_udf'>Writing our UDF</h2>

<p>Done? Onto the code. First let&#8217;s create a folder for it:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>mkdir -p src/main/java/com/snowplowanalytics/hive/udf
</code></pre></div>
<p>Now let&#8217;s add a file into our udf folder called ToUpper.java, containing:</p>

<p>Listing 3: Our ToUpper.java UDF definition</p>
<div class='highlight'><pre><code class='java'><span class='kn'>package</span> <span class='n'>com</span><span class='o'>.</span><span class='na'>snowplowanalytics</span><span class='o'>.</span><span class='na'>hive</span><span class='o'>.</span><span class='na'>udf</span><span class='o'>;</span>

<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.hive.ql.exec.UDF</span><span class='o'>;</span>
<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.hive.ql.exec.Description</span><span class='o'>;</span>
<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.io.Text</span><span class='o'>;</span>

<span class='nd'>@Description</span><span class='o'>(</span>
	<span class='n'>name</span> <span class='o'>=</span> <span class='s'>&quot;toupper&quot;</span><span class='o'>,</span>
	<span class='n'>value</span> <span class='o'>=</span> <span class='s'>&quot;_FUNC_(str) - Converts a string to uppercase&quot;</span><span class='o'>,</span>
	<span class='n'>extended</span> <span class='o'>=</span> <span class='s'>&quot;Example:\n&quot;</span> <span class='o'>+</span>
	<span class='s'>&quot;  &gt; SELECT toupper(author_name) FROM authors a;\n&quot;</span> <span class='o'>+</span>
	<span class='s'>&quot;  STEPHEN KING&quot;</span>
	<span class='o'>)</span>
<span class='kd'>public</span> <span class='kd'>class</span> <span class='nc'>ToUpper</span> <span class='kd'>extends</span> <span class='n'>UDF</span> <span class='o'>{</span>

    <span class='kd'>public</span> <span class='n'>Text</span> <span class='nf'>evaluate</span><span class='o'>(</span><span class='n'>Text</span> <span class='n'>s</span><span class='o'>)</span> <span class='o'>{</span>
		<span class='n'>Text</span> <span class='n'>to_value</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='s'>&quot;&quot;</span><span class='o'>);</span>
		<span class='k'>if</span> <span class='o'>(</span><span class='n'>s</span> <span class='o'>!=</span> <span class='kc'>null</span><span class='o'>)</span> <span class='o'>{</span>
		    <span class='k'>try</span> <span class='o'>{</span> 
				<span class='n'>to_value</span><span class='o'>.</span><span class='na'>set</span><span class='o'>(</span><span class='n'>s</span><span class='o'>.</span><span class='na'>toString</span><span class='o'>().</span><span class='na'>toUpperCase</span><span class='o'>());</span>
		    <span class='o'>}</span> <span class='k'>catch</span> <span class='o'>(</span><span class='n'>Exception</span> <span class='n'>e</span><span class='o'>)</span> <span class='o'>{</span> <span class='c1'>// Should never happen</span>
				<span class='n'>to_value</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='n'>s</span><span class='o'>);</span>
		    <span class='o'>}</span>
		<span class='o'>}</span>
		<span class='k'>return</span> <span class='n'>to_value</span><span class='o'>;</span>
    <span class='o'>}</span>
<span class='o'>}</span>
</code></pre></div>
<p>This file defines our UDF, ToUpper. The package definition and imports should be self-explanatory; the <code>@Description</code> annotation is a useful Hive-specific annotation to provide usage information for our UDF in the Hive console.</p>

<p>All user-defined functions extend the Hive UDF class; a UDF sub-class must then implement one or more methods named &#8220;evaluate&#8221; which will be called by Hive. We implement an evaluate method which takes one Hadoop Text (which stores text using UTF8) and returns the same Hadoop Text, but now in upper-case. The only complexity is some exception handling, which we include for safety&#8217;s sake.</p>

<p>Now let&#8217;s check that this compiles. In the root folder, run SBT like so:</p>

<p>Listing 4: Compiling in SBT</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>sbt
&gt; compile
<span class='o'>[</span>success<span class='o'>]</span> Total <span class='nb'>time</span>: 0 s, completed 28-Jan-2013 16:41:53
</code></pre></div>
<h2 id='testing_our_udf'>Testing our UDF</h2>

<p>Okay great, now time to write a test to make sure this is doing what we expect! First we create a folder for it:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>mkdir -p src/test/scala/com/snowplowanalytics/hive/udf
</code></pre></div>
<p>Now let&#8217;s add a file into our udf test folder called ToUpperTest.scala, containing:</p>

<p>Listing 5: Our ToUpperTest.scala Specs2 test</p>
<div class='highlight'><pre><code class='java'><span class='kn'>package</span> <span class='n'>com</span><span class='o'>.</span><span class='na'>snowplowanalytics</span><span class='o'>.</span><span class='na'>hive</span><span class='o'>.</span><span class='na'>udf</span>

<span class='kn'>import</span> <span class='nn'>org.apache.hadoop.io.Text</span>

<span class='kn'>import</span> <span class='nn'>org.specs2._</span>

<span class='kd'>class</span> <span class='nc'>ToUpperSpec</span> <span class='kd'>extends</span> <span class='n'>mutable</span><span class='o'>.</span><span class='na'>Specification</span> <span class='o'>{</span>
  <span class='n'>val</span> <span class='n'>toUpper</span> <span class='o'>=</span> <span class='k'>new</span> <span class='n'>ToUpper</span>

  <span class='s'>&quot;ToUpper#evaluate&quot;</span> <span class='n'>should</span> <span class='o'>{</span>
    <span class='s'>&quot;return an empty string if passed a null value&quot;</span> <span class='n'>in</span> <span class='o'>{</span>
      <span class='n'>toUpper</span><span class='o'>.</span><span class='na'>evaluate</span><span class='o'>(</span><span class='kc'>null</span><span class='o'>).</span><span class='na'>toString</span> <span class='n'>mustEqual</span> <span class='s'>&quot;&quot;</span>
    <span class='o'>}</span>

    <span class='s'>&quot;return a capitalised string if passed a mixed-case string&quot;</span> <span class='n'>in</span> <span class='o'>{</span>
      <span class='n'>toUpper</span><span class='o'>.</span><span class='na'>evaluate</span><span class='o'>(</span><span class='k'>new</span> <span class='n'>Text</span><span class='o'>(</span><span class='s'>&quot;Stephen King&quot;</span><span class='o'>)).</span><span class='na'>toString</span> <span class='n'>mustEqual</span> <span class='s'>&quot;STEPHEN KING&quot;</span>
    <span class='o'>}</span>
  <span class='o'>}</span>
<span class='o'>}</span>
</code></pre></div>
<p>This is a Specs2 unit test (http://etorreborre.github.com/specs2/), written in Scala, which checks that ToUpper is performing correctly: we test that an empty string is handled correctly, and then we test that a mixed-case string (&#8220;Stephen King&#8221;) is successfully converted to &#8220;STEPHEN KING&#8221;.</p>

<p>So let&#8217;s run this next from SBT:</p>

<p>Listing 6: Testing in SBT</p>
<div class='highlight'><pre><code class='text'>&gt; test
[info] Compiling 1 Scala source to /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/test-classes...
[info] ToUpperSpec
[info] 
[info] ToUpper#evaluate should
[info] + return an empty string if passed a null value
[info] + return a capitalised string if passed a mixed-case string
[info]  
[info]  
[info] Total for specification ToUpperSpec
[info] Finished in 742 ms
[info] 2 examples, 0 failure, 0 error
[info] 
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 8 s, completed 28-Jan-2013 17:11:45
</code></pre></div>
<h2 id='building_and_using_our_udf'>Building and using our UDF</h2>

<p>Our tests passed! Now we&#8217;re ready to use our function &#8220;in anger&#8221; from Hive. First, still from SBT, let&#8217;s build our jarfile:</p>

<p>Listing 7: Packaging our jar</p>
<div class='highlight'><pre><code class='text'>&gt; package
[info] Packaging /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/hive-example-udf_2.9.2-0.0.1.jar ...
[info] Done packaging.
[success] Total time: 1 s, completed 28-Jan-2013 17:21:02
</code></pre></div>
<p>Now take the jarfile (hive-example-udf_2.9.2-0.0.1.jar) and upload it to our Hive cluster - on Amazon&#8217;s Elastic MapReduce, for example, you could upload it to S3.</p>

<p>From your Hive console, you can now add our new UDF like so:</p>
<div class='highlight'><pre><code class='mysql'><span class='o'>&gt;</span> <span class='k'>add</span> <span class='n'>jar</span> <span class='o'>/</span><span class='n'>path</span><span class='o'>/</span><span class='k'>to</span><span class='o'>/</span><span class='n'>HiveSwarm</span><span class='p'>.</span><span class='n'>jar</span><span class='p'>;</span>
<span class='o'>&gt;</span> <span class='k'>create</span> <span class='n'>temporary</span> <span class='n'>function</span> <span class='n'>to_upper</span> <span class='k'>as</span> <span class='s1'>&#39;com.snowplowanalytics.hive.udf.ToUpper&#39;</span><span class='p'>;</span>
</code></pre></div>
<p>And then finally you can use our new UDF in your HiveQL queries, something like this:</p>
<div class='highlight'><pre><code class='mysql'><span class='o'>&gt;</span> <span class='k'>SELECT</span> <span class='nf'>toupper</span><span class='p'>(</span><span class='n'>author_name</span><span class='p'>)</span> <span class='k'>FROM</span> <span class='n'>authors</span> <span class='n'>a</span><span class='p'>;</span>
  <span class='n'>STEPHEN</span> <span class='n'>KING</span>
</code></pre></div>
<p>That completes our article. If you would like to download the example code above as a working project, you can find it on GitHub here: https://github.com/snowplow/hive-example-udf</p>

<p>I hope to return with further articles about Hive and Hadoop in the future - potentially one on writing a custom serde - an area where we have a lot of experience at Snowplow Analytics.</p>

<h2 id='looking_for_help_performing_analytics_or_developing_data_pipelines_using_hive_and_other_hadooppowered_tools'>Looking for help performing analytics or developing data pipelines using Hive and other Hadoop-powered tools</h2>

<p><a href='/services/pipelines.html'>Learn more</a> about services offered by the <a href='/services/pipelines.html'>Snowplow Professional Services team</a>.</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh6.googleusercontent.com/-4Ydq6ygNbgQ/AAAAAAAAAAI/AAAAAAAAAF4/SX2Fn3veqp4/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/alex.html">Alex</a> is co-founder and technical lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/113518635920914092796" rel="author">Google+</a>, <a href="https://twitter.com/alexatkeplar">Twitter</a> and <a href="http://uk.linkedin.com/in/alexdean">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2014/01/17/scala-forex-library-released">Scala Forex library by wintern Jiawen Zhou released</a></li>
		
			<li><a href="/blog/2014/01/15/amazon-kinesis-tutorial-getting-started-guide">Amazon Kinesis tutorial - a getting started guide</a></li>
		
			<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
		
			<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
		
			<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/04/snowplow-at-the-graduate-data-science-initiative">The first Graduate Data Science Initiative event in London</a></li>
			
				<li><a href="/blog/2013/11/11/round-up-and-thank-you-for-the-budapest-bi-conference-last-week">A round up of our trip to the Budapest BI Conference last week, and a thank you to the many people who made the trip so worthwhile</a></li>
			
				<li><a href="/blog/2013/10/28/yali-and-alex-introduce-snowplow-to-code-n">Our video introduction of Snowplow to code_n</a></li>
			
				<li><a href="/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference">Join the Snowplow team in Budapest the first week of November</a></li>
			
				<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2014/01/17/scala-forex-library-released">Scala Forex library by wintern Jiawen Zhou released</a></li>
			
				<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
			
				<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
			
				<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
			
				<li><a href="/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements">Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data">Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</a></li>
			
				<li><a href="/blog/2013/11/19/quickstart-guide-to-using-sql-with-snowplow-data-published">Quick start guide to learning SQL to query Snowplow data published</a></li>
			
				<li><a href="/blog/2013/10/28/call-for-data-this-winter">Call for data! Support us develop experimental analyses. Have us help you answer your toughest business questions.</a></li>
			
				<li><a href="/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio">Using the new SQL views to perform cohort analysis with ChartIO</a></li>
			
				<li><a href="/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data">Using Qubole to crunch your Snowplow web data using Apache Hive</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2014/01/15/amazon-kinesis-tutorial-getting-started-guide">Amazon Kinesis tutorial - a getting started guide</a></li>
			
				<li><a href="/blog/2013/11/20/loading-json-data-into-redshift">Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges</a></li>
			
				<li><a href="/blog/2013/09/27/how-much-does-snowplow-cost-to-run">How much does Snowplow cost to run, vs the competition?</a></li>
			
				<li><a href="/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar">Towards universal event analytics - building an event grammar</a></li>
			
				<li><a href="/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs">Unpicking the Snowplow data pipeline and how it drives AWS costs</a></li>
			
		
		</ul>		
	
		<h1>Recruitment</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/20/introducing-our-snowplow-winterns">Introducing our Snowplow winterns</a></li>
			
				<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
			
		
		</ul>		
	
		<h1>Research</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

			<div id="footer">
	<p>Copyright Â© Snowplow Analytics Limited 2012 - 2014.  All rights reserved</p>
</div>
		</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>