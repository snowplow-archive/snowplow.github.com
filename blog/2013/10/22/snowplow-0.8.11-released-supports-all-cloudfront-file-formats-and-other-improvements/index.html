<!DOCTYPE html>
<html>
<head>
	
	<title>Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->

	<div id="container">
		<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
		<div id="contents">
		<div class="post">
			22 Oct 2013
			<h1>Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</h1>
			 <span class="author">Author: <a href="/alex.html" rel="author">Alex Dean </a></span>
			<p>We&#8217;re very pleased to announce the release of Snowplow 0.8.11. This releases includes two different sets of updates:</p>

<ol>
<li><strong>Critical update</strong>: support for Amazon&#8217;s new Cloudfront log file format (rolled out by Amazon during 21st October 2013)</li>

<li><strong>Nice-to-have</strong> additions - the most significant of which is <strong>IP anonymization</strong></li>
</ol>

<p>We&#8217;ll discuss the updates one at a time, before covering how to <a href='#upgrade'>upgrade to the latest version</a>.</p>

<ol>
<li><a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#critical'>Critical upgrade: support for Amazon&#8217;s new CloudFront log file format</a></li>

<li><a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#ip'>IP address anonymization</a></li>

<li><a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#other'>Other updates</a></li>

<li><a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#upgrade'>Upgrading</a></li>
</ol>

<p>Before we dive into the detail, thanks to community members <a href='https://github.com/kingo55'>Rob Kingson</a> and <a href='https://github.com/shermozle'>Simon Rumble</a> for contributing to this release.</p>
<a name='critical'><h2>1. Critical upgrade: support for Amazon's new CloudFront log file format</h2></a>
<p>Since August, Amazon has made a number of changes to their CloudFront log file format, the most recent of which was pushed live yesterday:</p>
<table><thead><tr><th><strong>Cloudfront log file format</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td style='text-align: left;'>Original format</td><td style='text-align: left;'>The original CloudFront log file format, around which Snowplow was originally developed.</td>
</tr><tr><td style='text-align: left;'>12 Sep 2012 - 17 Aug 2013 format</td><td style='text-align: left;'>The original format with three new fields appended.</td>
</tr><tr><td style='text-align: left;'>August 17 unnanounced change</td><td style='text-align: left;'>Surprise change around the URI encoding of fields. See <a href='https://groups.google.com/forum/#!topic/snowplow-user/HWeSkiiXbdQ'>the Google Group</a> for details.</td>
</tr><tr><td style='text-align: left;'>September 14 resolution</td><td style='text-align: left;'>A new approach to URI encoding, different to the previous two. See <a href='https://forums.aws.amazon.com/message.jspa?messageID=491582'>this forum thread</a> for details.</td>
</tr><tr><td style='text-align: left;'>October 21 update</td><td style='text-align: left;'>Amazon updated the latest log file format with three new fields. See <a href='https://forums.aws.amazon.com/ann.jspa?annID=2174&amp;ref_=pe_411040_33444690_11#'>this post</a> for details.</td>
</tr></tbody></table><!--more-->
<p>The latest version of Snowplow supports <em>all</em> the different versions of the file format listed above, including the new format that was rolled out yesterday. It is important to note that the October 21st CloudFront file format is <strong>not</strong> supported by previous Snowplow versions: as a result, we&#8217;d expect existing Snowplow users using the CloudFront collector to see a significant number of lines in their bad rows bucket in S3 with the following format:</p>

<p>Once you have <a href='#upgrade'>upgraded your Snowplow installation to the latest version</a>, you will need to reprocess those bad rows. Instructions on how to do so are given <a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'>in this blog post</a>.</p>
<a name='ip'><h2>2. IP address anonymization</h2></a>
<p>As well as the critical update, there are a number of <em>nice-to-have</em> features bundled in this release. Chief amongst them is IP anonymization. The enrichment process can now be configured to mask IP addresses, so that privacy-conscious Snowplow users can prevent IP addresses being visible to analysts.</p>

<p>Snowplow administrators can setup IP masking via the EmrEtlRunner config file. Instructions on how to do this can be found in the <a href='#upgrade'>section on upgrading</a> below.</p>
<a name='other'><h2>3. Other updates</h2></a>
<p>Under the hood, there are a large number of updates we&#8217;ve made to make Snowplow more robust, performant and support developments on our <a href='https://github.com/snowplow/snowplow/wiki/Product%20roadmap'>product roadmap</a>. You can view the <a href='https://github.com/snowplow/snowplow/blob/feature/improve-etl/CHANGELOG'>complete changelog here</a>.</p>

<p>The most important of these updates is an update to the StorageLoader to make loading into PostgreSQL more robust, by fixing an issue where Postgres was accidentally escaping tabs in the file format, breaking the load. Many thanks to community member <a href='https://github.com/kingo55'>Rob Kingston</a> for contributing this update.</p>

<p>There are also some additional command-line options for our two Ruby apps which should make the Snowplow Enrichment process more flexible:</p>

<ul>
<li>Run EmrEtlRunner with <code>--debug</code> to make Elastic MapReduce&#8217;s job debugging available</li>

<li>Run StorageLoader with <code>--include vacuum</code> if you want to include a <code>VACUUM</code> step after your table load</li>

<li>Run StorageLoader with <code>--skip analyze</code> if you <strong>don&#8217;t</strong> need to run a table <code>ANALYZE</code> step after your table load</li>

<li>Run StorageLoader with <code>--include compupdate</code> if you want to (re-)generate the compression encodings on your table&#8217;s fields. This setting uses the new <code>:comprows:</code> parameter in the <code>config.yml</code> file - see <a href='#storage-loader'>section 4.2</a> below for details</li>
</ul>

<p>Finally, there are a set of &#8220;under the hood&#8221; stability and performance and improvements in this release.</p>

<p><strong>For the definitive list of updates in this release, please see the <a href='https://github.com/snowplow/snowplow/releases/tag/0.8.11'>v0.8.11 Release Notes</a> on GitHub.</strong></p>
<a name='upgrade'><h2>4. Upgrading</h2></a>
<p>Upgrading is a three step process:</p>

<ol>
<li>1 <a href='#emr-etl-runner'>Update EmrEtlRunner</a></li>

<li>2 <a href='#storage-loader'>Update StorageLoader</a></li>

<li>3 <a href='#reprocess'>Reprocess any &#8220;bad rows&#8221;</a> collected from before the upgrade was completed</li>
</ol>

<p>Let&#8217;s take these in term:</p>
<a name='emr-etl-runner'><h3>4.1 Update the EmrEtlRunner</h3></a>
<p>You need to update EmrEtlRunner to the latest code (0.8.11 release) on Github:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>git clone git://github.com/snowplow/snowplow.git
<span class='nv'>$ </span>git checkout 0.8.11
<span class='nv'>$ </span><span class='nb'>cd </span>snowplow/3-enrich/emr-etl-runner
<span class='nv'>$ </span>bundle install --deployment
</code></pre>
</div>
<p>You also need to update the <code>config.yml</code> file for EmrEtlRunner to use the latest version of the Hadoop ETL (0.3.5):</p>
<div class='highlight'><pre><code class='yaml'><span class='l-Scalar-Plain'>:snowplow</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>:hadoop_etl_version</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>0.3.5</span>
</code></pre>
</div>
<p>In addition, you need to add a new &#8220;enrichments&#8221; section in the <code>config.yml</code> file:</p>
<div class='highlight'><pre><code class='yaml'><span class='l-Scalar-Plain'>:enrichments</span><span class='p-Indicator'>:</span>
  <span class='l-Scalar-Plain'>:anon_ip</span><span class='p-Indicator'>:</span>
    <span class='l-Scalar-Plain'>:enabled</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>false</span>
    <span class='l-Scalar-Plain'>:anon_octets</span><span class='p-Indicator'>:</span> <span class='l-Scalar-Plain'>1</span> <span class='c1'># Or 2, 3 or 4. 0 is same as enabled: false</span>
</code></pre>
</div>
<p>To enable IP enrichment, you need to set <code>anon_ip.enabled</code> to true, and specify the level of anonymization with <code>anon_ip.anon_octets</code> field. If, for example, my IP address is &#8216;37.157.33.178&#8217;, then setting it to different values between 0 and 4 would anonymize my IP address as follows:</p>
<table><thead><tr><th><code>anon_ip.anon_octets</code> value</th><th>IP address displayed in Snowplow</th></tr></thead><tbody><tr><td style='text-align: left;'>0</td><td style='text-align: left;'>37.157.33.178</td>
</tr><tr><td style='text-align: left;'>1</td><td style='text-align: left;'>37.157.33.x</td>
</tr><tr><td style='text-align: left;'>2</td><td style='text-align: left;'>37.157.x.x</td>
</tr><tr><td style='text-align: left;'>3</td><td style='text-align: left;'>37.x.x.x</td>
</tr><tr><td style='text-align: left;'>4</td><td style='text-align: left;'>x.x.x.x</td>
</tr></tbody></table>
<p>To see a complete example of the EmrEtlRunner <code>config.yml</code> file, see the <a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'>Github repo</a>.</p>
<a name='storage-loader'><h3>4.2 Update the StorageLoader</h3></a>
<p>You need to upgrade your StorageLoader installation to the latest code (0.8.11) on Github:</p>
<div class='highlight'><pre><code class='bash'><span class='nv'>$ </span>git clone git://github.com/snowplow/snowplow.git
<span class='nv'>$ </span>git checkout 0.8.11
<span class='nv'>$ </span><span class='nb'>cd </span>snowplow/4-storage/storage-loader
<span class='nv'>$ </span>bundle install --deployment
</code></pre>
</div>
<p>The StorageLoader <code>config.yml</code> file includes a new <code>:comprows:</code> option for Redshift users. This determines the number of rows that Amazon analyzes in order to determine the best compression encoding format to use for each of the fields in your Redshift event table. Note that this is <strong>only</strong> used if the <code>--include compupdate</code> option is specified when running the StorageLoader. For more information on Amazon&#8217;s <code>comprows</code> functionality, see the <a href='http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html'>Redshift documentation</a>.</p>

<p>An example <code>config.yml</code> for StorageLoader for Redshift users, including the new setting, can be found <a href='https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/config/redshift.yml.sample'>on Github</a>.</p>
<a name='reprocess'><h3>4.3 Reprocess any "bad rows" generated between October 21st and your upgrade</h3></a>
<p>As described above, if you have been using the CloudFront collector, you will have a number of rows of data in your &#8220;bad bucket&#8221; on S3 generated after the new CloudFront log file format was rolled out on October 21st, because these data rows were not supported by the old version of the Snowplow.</p>

<p>You need to reprocess these rows so they are not missing from your final data set. For detailed instructions on how to do this, see <a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'>our guide to reprocessing bad data in Snowplow</a>.</p>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh6.googleusercontent.com/-4Ydq6ygNbgQ/AAAAAAAAAAI/AAAAAAAAAF4/SX2Fn3veqp4/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/alex.html">Alex</a> is co-founder and technical lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/113518635920914092796" rel="author">Google+</a>, <a href="https://twitter.com/alexatkeplar">Twitter</a> and <a href="http://uk.linkedin.com/in/alexdean">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference">Join the Snowplow team in Budapest the first week of November</a></li>
		
			<li><a href="/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements">Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</a></li>
		
			<li><a href="/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio">Using the new SQL views to perform cohort analysis with ChartIO</a></li>
		
			<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
		
			<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
		
	</ul>

	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/09/27/how-much-does-snowplow-cost-to-run">How much does Snowplow cost to run, vs the competition?</a></li>
			
				<li><a href="/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar">Towards universal event analytics - building an event grammar</a></li>
			
				<li><a href="/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs">Unpicking the Snowplow data pipeline and how it drives AWS costs</a></li>
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
				<li><a href="/blog/2013/04/10/snowplow-event-validation">Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</a></li>
			
		
		</ul>		
	
		<h1>Research</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements">Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</a></li>
			
				<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
			
				<li><a href="/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change">Snowplow 0.8.9 released to handle CloudFront log file format change</a></li>
			
				<li><a href="/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support">Snowplow 0.8.8 released with Postgres and Hive support</a></li>
			
				<li><a href="/blog/2013/07/09/dotnet-support-added-to-referer-parser">.NET (C#) support added to referer-parser</a></li>
			
		
		</ul>		
	
		<h1>Recruitment</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
			
		
		</ul>		
	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference">Join the Snowplow team in Budapest the first week of November</a></li>
			
				<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
			
				<li><a href="/blog/2013/09/30/book-review-instant-hive-essentials-how-to">Book review - Apache Hive Essentials How-to</a></li>
			
				<li><a href="/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole">Reprocessing bad rows of Snowplow data using Hive, the JSON Serde and Qubole</a></li>
			
				<li><a href="/blog/2013/07/19/snowplow-presentation-to-hadoop-user-group-london-aws-event">Snowplow presentation at the Hadoop User Group London AWS event</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio">Using the new SQL views to perform cohort analysis with ChartIO</a></li>
			
				<li><a href="/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data">Using Qubole to crunch your Snowplow web data using Apache Hive</a></li>
			
				<li><a href="/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization">Getting started using R for data analysis</a></li>
			
				<li><a href="/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing">Measuring how much traffic individual items in your catalog drive to your website</a></li>
			
				<li><a href="/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow">Performing market basket analysis on web analytics data with R</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

		<div id="footer">
	<p>Copyright Â© Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>