<!DOCTYPE html>
<html>
<head>
	
	<title>Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges - Snowplow Analytics</title>
	

	<link rel="icon" type="image/x-icon" href="/favicon.ico" />

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="description" content="" />
	<link href="/static/css/styles.css" type="text/css" rel="stylesheet" />
	<link href="/static/css/pygments.css" type="text/css" rel="stylesheet" />
	
	<!--For the homepage slider-->
	<link rel="stylesheet" href="/static/css/nivo-slider.css" type="text/css" media="screen" />
	<link rel="stylesheet" href="/static/css/nivo-slider-theme-default.css" type="text/css" media="screen" />
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.0/jquery.min.js"></script>
	<script src="/static/js/jquery-nivo-slider-pack.js" type="text/javascript" ></script>
	<!--MathJax http://www.mathjax.org/-->
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_HTMLorMML.js"></script>
	<script type="text/javascript">
		MathJax.Hub.Config({
	      tex2jax: {
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
	      }
	    });
	    MathJax.Hub.Queue(function() {
	        var all = MathJax.Hub.getAllJax(), i;
	        for(i=0; i < all.length; i += 1) {
	            all[i].SourceElement().parentNode.className += ' has-jax';
	        }
    	});
	</script>
	<!-- end mathjax -->
	<!-- typekit -->
	<script type="text/javascript" src="//use.typekit.net/noo1diw.js"></script>
	<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<!-- end typekit -->
</head>
<body>
	<!-- Google Tag Manager -->
	<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-DLRG"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-DLRG');</script>
	<!-- End Google Tag Manager -->
	<div id="universe">
		<div id="container">
			<div id="header" class="span-24">
  <div id="logo">
    <h1><a href="/"><img src="/static/img/snowplow-logo-website.png" title="Snowplow Analytics" /></a></h1>
  </div>
  <div id="menu" class="span-15">
    <ul>
      <li ><a href="/product/index.html">Product</a></li>
      <li ><a href="/services/index.html">Services</a></li>
      <li ><a href="/analytics/index.html">Analytics</a></li>
      <li ><a href="/technology/index.html">Technology</a></li>
      <li  class="active" ><a href="/blog.html">Blog</a></li>
      <li ><a id="mail" href="/about/index.html">About</a></li>
    </ul>
  </div>
</div>
	
			<div id="contents">
		<div class="post">
			20 Nov 2013
			<h1>Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges</h1>
			 <span class="author">Author: <a href="/yali.html" rel="author">Yali Sassoon </a></span>
			<p>Very many of our Professional Services projects involve forking the Snowplow codebase so that specific clients can use it to load their event data, stored as JSONs, into Amazon Redshift, so that they can use BI tools to create dashboards and mine that data.</p>

<p><img alt='container-ship-image' src='/static/img/blog/2013/11/container-ship.jpg' /></p>

<p>We&#8217;ve been surprised quite how many companies have gone down the road of using JSONs to store their event data. In this blog post, we look at:</p>

<ul>
<li><a href='/blog/2013/11/20/loading-json-data-into-redshift/#why'>Why logging event data as JSONs has become so popular</a></li>

<li><a href='/blog/2013/11/20/loading-json-data-into-redshift/#weaknesses'>The limitations of this approach</a></li>

<li><a href='/blog/2013/11/20/loading-json-data-into-redshift/#solution'>Using the Snowplow tech stack to load JSON data into Redshift</a></li>
</ul>
<!--more--><h2><a name='why'>Why has logging event data as JSONs has become so popular?</a></h2>
<p>There are four reasons logging event data as JSONs has become as popular as it is:</p>

<h3 id='1_easy_to_implement'>1. Easy to implement</h3>

<p>Representing an event as a JSON is an extremely simple, easy-to-understand approach. Let&#8217;s look at an example, a video play event represented as a JSON:</p>
<div class='highlight'><pre><code class='json'><span class='p'>{</span>
    <span class='nt'>&quot;event_name&quot;</span><span class='p'>:</span> <span class='s2'>&quot;play_video&quot;</span><span class='p'>,</span>
    <span class='nt'>&quot;properties&quot;</span><span class='p'>:</span> <span class='p'>{</span>
        <span class='nt'>&quot;timestamp&quot;</span><span class='p'>:</span> <span class='mi'>1384855393</span><span class='p'>,</span>
        <span class='nt'>&quot;viewer_id&quot;</span><span class='p'>:</span> <span class='s2'>&quot;19A34Bdt1190&quot;</span><span class='p'>,</span>
        <span class='nt'>&quot;video_id&quot;</span><span class='p'>:</span> <span class='mi'>234101234</span><span class='p'>,</span>
        <span class='nt'>&quot;name&quot;</span><span class='p'>:</span> <span class='s2'>&quot;Another skateboarding dog&quot;</span><span class='p'>,</span>
        <span class='nt'>&quot;author_id&quot;</span><span class='p'>:</span> <span class='s2'>&quot;asdf987023s&quot;</span>
    <span class='p'>}</span>
<span class='p'>}</span>
</code></pre></div>
<p>The JSON is very easy to read and comprehend. Crucially for application developers, it is straightforward to compose.</p>

<p>Note that there are many ways we could choose to represent a video play event with a JSON. We might want e.g. to use nesting, to capture a richer data set both about the video itself, and about the user who watched it:</p>
<div class='highlight'><pre><code class='json'><span class='p'>{</span>
    <span class='nt'>&quot;event_name&quot;</span><span class='p'>:</span> <span class='s2'>&quot;play_video&quot;</span><span class='p'>,</span>
    <span class='nt'>&quot;properties&quot;</span><span class='p'>:</span> <span class='p'>{</span>
        <span class='nt'>&quot;timestamp&quot;</span><span class='p'>:</span> <span class='mi'>1384855393</span><span class='p'>,</span>
        <span class='nt'>&quot;viewer&quot;</span><span class='p'>:</span> <span class='p'>{</span>
            <span class='nt'>&quot;id&quot;</span><span class='p'>:</span> <span class='s2'>&quot;19A34Bdt1190&quot;</span><span class='p'>,</span>
            <span class='nt'>&quot;age&quot;</span><span class='p'>:</span> <span class='mi'>28</span><span class='p'>,</span>
            <span class='nt'>&quot;gender&quot;</span><span class='p'>:</span> <span class='s2'>&quot;male&quot;</span><span class='p'>,</span>
            <span class='nt'>&quot;member&quot;</span><span class='p'>:</span> <span class='kc'>true</span>
        <span class='p'>},</span>
        <span class='nt'>&quot;video&quot;</span><span class='p'>:</span> <span class='p'>{</span>
            <span class='nt'>&quot;id&quot;</span><span class='p'>:</span> <span class='mi'>234101234</span><span class='p'>,</span>
            <span class='nt'>&quot;name&quot;</span><span class='p'>:</span> <span class='s2'>&quot;Another skateboarding dog&quot;</span><span class='p'>,</span>
            <span class='nt'>&quot;producer&quot;</span><span class='p'>:</span> <span class='p'>{</span>
                <span class='nt'>&quot;id&quot;</span><span class='p'>:</span> <span class='s2'>&quot;asdf987023s&quot;</span><span class='p'>,</span>
                <span class='nt'>&quot;name&quot;</span><span class='p'>:</span> <span class='s2'>&quot;michaeldouglasboy&quot;</span><span class='p'>,</span>
                <span class='nt'>&quot;joined&quot;</span><span class='p'>:</span> <span class='s2'>&quot;2012-09-29&quot;</span>
            <span class='p'>}</span>
        <span class='p'>}</span>
    <span class='p'>}</span>
<span class='p'>}</span>
</code></pre></div>
<p>In the above example, we&#8217;ve used nesting to group related fields (fields related to the viewer, fields related to the video, and fields related to the producer of the video). Again, even with all the additional data, the JSON is easy for the human eye to parse, and the nesting provides a tidy way of structuring our data.</p>

<h3 id='2_flexible'>2. Flexible</h3>

<p>JSONs are flexible. If one day, an application developer decides she wants to add a new field to the &#8220;play_video&#8221; JSON, there&#8217;s nothing stopping her!</p>

<p>Typically, application developers can create new JSONs to represent new event types over time, and update the structure of JSONs for existing event types over time (adding or dropping fields), as they see fit. If the analytics system is simply logging the JSONs, there&#8217;s no need to update any downstream analytics infrastructure in light of changes to the JSON schema.</p>

<h3 id='3_wellsupported'>3. Well-supported</h3>

<p>Lots of analytics applications store event data as JSONs, including <a href='https://www.kissmetrics.com/'>Kissmetrics</a>, <a href='https://mixpanel.com/'>Mixpanel</a>, <a href='https://keen.io/'>KeenIO</a> and <a href='http://www.swrve.com/'>Swrve</a>. At Snowplow, we&#8217;re in the process of building out support for <a href='https://github.com/snowplow/snowplow/wiki/Developer-FAQ#wiki-unstructtimeline'>unstructured events</a>, where events are represented as JSONs.</p>

<h3 id='4_seemingly_easy_to_analyze_using_hive_and_json_serde'>4. <em>Seemingly</em> easy to analyze using Hive and JSON Serde</h3>

<p>JSONs, stored as flat files in S3 or HDFS <em>should</em> be easy to analyze using Hive and the <a href='https://github.com/rcongiu/Hive-JSON-Serde'>JSON serde</a>. (We&#8217;ve blogged an example of this <a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'>here</a>.) You &#8216;simply&#8217; specify the schema as part of your Hive table definitions, implicit in the JSON. for each event type, and then you can query the data using SQL statements, as if it was a relational database. Easy peasy!</p>
<h2><a name='weaknesses'>The limitations of this approach</a></h2>
<p>Unfortunately, querying the JSON data is not as easy as it first appears:</p>

<ol>
<li>
<p>It is not always obvious, to the analyst creating the Hive table definitions, what the schema / structure for each event type should be. The analyst can visually inspect some of the data to get some idea, but it is impossible to tell:</p>

<ul>
<li>Whether they have spotted all the possible fields that might appear. (E.g. there may be some that are sparsely populated, but very important when they are filled in.)</li>

<li>Which fields are compulsory, and which are optional?</li>

<li>Remember that application developers have been free to keep modifying and updating event schemas over time, with no requirement to document or sense-check any of these updates. The analyst suffers, as a consequence, as she has to work out how that schema has evolved, in order to interrogate the data.</li>
</ul>
</li>

<li>
<p>When exploding nested data into separate tables, it is can be hard to identify on what key that data should be joined to the parent table. A nice feature of JSONs is their ability to nest data, but unless we can explode that nested data out, querying it is not going to be easy.</p>
</li>

<li>
<p>It can be hard (if not impossible) for analysts and application developers to spot &#8220;errors&#8221; in the JSON at the point the event data is generated and captured. It is very easy for mistakes to creep in: JSONs are very fragile (a missing comma or inverted comma will break a JSON.) There&#8217;s also no way to check the type of individual field.</p>
</li>
</ol>

<p>When Amazon Redshift was launched earlier this year, many companies wanted to load their event data into Redshift, to enable faster querying than was possible in Apache Hive, and also so that they could use BI and analytics tools to create dashboards, visaulize and mine the data. Unfortunately, loading JSON data into Redshift is even harder:</p>

<ol>
<li>Redshift tables have traditional schemas where each field has a fixed type. To make loading data into Redshift reliable, you really want to enforce the strong types on variables all the way through the data pipeline, from data collection. However, JSONs do not support strong typing or schemas.</li>

<li>Any input line that does not conform to the Redshift schema fails to load. Many companies are then stuck between two unappealing approaches: junk data that doesn&#8217;t fit the schema (which may be a significant subset of the data), or only load a very small subset of the fields that have been reliably collected across the different event types. (Thereby relaxing the requirements on input data to successfully load, but again, effectively loosing a lot of the richness in the raw JSON data set.)</li>
</ol>
<h2><a name='solution'>Using the Snowplow tech stack to load JSON data into Redshift</a></h2>
<p>The Snowplow stack can be forked so that Snowplow transforms JSON data and loads it into Redshift. We&#8217;ve found this is a much better approach then building an ETL pipeline using e.g. Apache Hive and the JSON serde, because Snowplow has a lot of validation capabilities. We&#8217;ll discuss this in a second - first, let&#8217;s overview the process for adapting Snowplow to load your custom JSONs into Redshift:</p>

<ol>
<li>We develop an event dictionary for the client, which catalogs all the different events in their application and the fields that are captured with each of those events.</li>

<li>We use that event dictionary to define table definitions in Redshift where the data will be loaded. (So we have a mapping of the event dictionary to the output tables.)</li>

<li>We work with the client to map the contents of their event JSONs to the dictionary. (So we have a mapping of the input JSONs to the event dictionary.)</li>

<li>We then modify the Snowplow stack to unpick the JSONs (as per the JSON -&gt; dictionary mapping) and write the data back to S3 in a format suitable for loading directly into Redshift (as per the dictionary -&gt; Redshift table definitions mapping)</li>
</ol>

<p>As mentioned above, the key to making this work is to use Snowplow&#8217;s rich validation capabilities. We use these to:</p>

<ol>
<li>Check that the input data conforms to the schemas specified</li>

<li>Output any data that does not conform to the schema to a &#8220;bad buckeet&#8221;. This means that the &#8220;good data&#8221; will successfully load into Redshift, while we don&#8217;t lose any &#8220;bad data&#8221;. We can now easily spot errors as they arise (by ensuring that the ETL process is run every few hours) and deal with them immediately. It also means that the &#8220;bad&#8221; data can be inspected, updated, reprocessed, and then loaded into Redshift, which is much preferable to simpy dropping it.</li>
</ol>

<p>Going forwards, we plan to build out the validation capability, so that as well as simply checking if incoming JSONs adhere to the schema, Snowplow will also spot &#8220;orphaned data&#8221; (i.e. name / value pairs in the JSON that are not accommodated in the schema) so that the schema can be updated to incorporate this data, and we save it from being lost.</p>

<h2 id='interested_in_talking_to_the_snowplow_team_about_loading_your_json_data_into_redshift'>Interested in talking to the Snowplow team about loading your JSON data into Redshift?</h2>

<p>View our info on the <a href='/services/pipelines.html'>Professional Services pages</a>, or <a href='/about/index.html'>get in touch</a> to discuss your requirements.</p>

<h2 id='references'>References</h2>

<p>We&#8217;re not the only people who think storing event data as JSONs is not a great. For more opinions:</p>

<ul>
<li><a href='http://nathanmarz.com/blog/thrift-graphs-strong-flexible-schemas-on-hadoop.html'>Nathan Marz</a> wrote a blog post back in 2010, highlighting their weaknesses, as part of presenting an alternative, graph-based model.</li>

<li>The <a href='http://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf'>Unified Logging Infrastructure for Data Analytics at Twitter</a> gives a very clear description of how this approach breaks down at scale. (See particularly section 3.1.)</li>
</ul>
			<div class="author_summary">
				<h2>About the author</h2>
				<div class="author_image"><img src="https://lh4.googleusercontent.com/--uMP0uMpzEs/AAAAAAAAAAI/AAAAAAAABH0/lo82KAkjEIU/s120-c/photo.jpg" /></div> <div class="author_spiel">
  <a href="/yali.html">Yali</a> is co-founder and analytics lead at Snowplow Analytics. You can find in him on <a href="https://plus.google.com/u/0/106510540736941709264" rel="author">Google+</a>, <a href="https://twitter.com/yalisassoon">Twitter</a> and <a href="http://uk.linkedin.com/in/yalisassoon">LinkedIn</a>.
</div>

			</div> 
			<div id="comments">
	<h2>Questions? Comments? Join the debate!</h2>
	 <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname
            /* var disqus_identifier =  ; // unique ID so that disqus fetches the correct comments for each post
            var disqus_url =  ;
            var disqus_title =  ; */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
		</div>
		<p>Return to the <a href="/blog.html">main blog page</a></p>
		

</div>

<div id="sidebar">
	<h1>Recent posts</h1>
	<ul>
		
			<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
		
			<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
		
			<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
		
			<li><a href="/blog/2013/12/20/introducing-our-snowplow-winterns">Introducing our Snowplow winterns</a></li>
		
			<li><a href="/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data">Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</a></li>
		
	</ul>

	
		<h1>Other</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/04/snowplow-at-the-graduate-data-science-initiative">The first Graduate Data Science Initiative event in London</a></li>
			
				<li><a href="/blog/2013/11/11/round-up-and-thank-you-for-the-budapest-bi-conference-last-week">A round up of our trip to the Budapest BI Conference last week, and a thank you to the many people who made the trip so worthwhile</a></li>
			
				<li><a href="/blog/2013/10/28/yali-and-alex-introduce-snowplow-to-code-n">Our video introduction of Snowplow to code_n</a></li>
			
				<li><a href="/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference">Join the Snowplow team in Budapest the first week of November</a></li>
			
				<li><a href="/blog/2013/10/01/snowplow-passes-500-stars">Snowplow passes 500 stars on GitHub</a></li>
			
		
		</ul>		
	
		<h1>Releases</h1>
		<ul>
		
			
				<li><a href="/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support">Snowplow 0.8.13 released with Looker support</a></li>
			
				<li><a href="/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure">Five things that make analyzing Snowplow data in Looker an absolute pleasure</a></li>
			
				<li><a href="/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements">Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</a></li>
			
				<li><a href="/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements">Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</a></li>
			
				<li><a href="/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes">Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</a></li>
			
		
		</ul>		
	
		<h1>Analytics</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data">Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</a></li>
			
				<li><a href="/blog/2013/11/19/quickstart-guide-to-using-sql-with-snowplow-data-published">Quick start guide to learning SQL to query Snowplow data published</a></li>
			
				<li><a href="/blog/2013/10/28/call-for-data-this-winter">Call for data! Support us develop experimental analyses. Have us help you answer your toughest business questions.</a></li>
			
				<li><a href="/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio">Using the new SQL views to perform cohort analysis with ChartIO</a></li>
			
				<li><a href="/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data">Using Qubole to crunch your Snowplow web data using Apache Hive</a></li>
			
		
		</ul>		
	
		<h1>Inside the Plow</h1>
		<ul>
		
			
				<li><a href="/blog/2013/11/20/loading-json-data-into-redshift">Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges</a></li>
			
				<li><a href="/blog/2013/09/27/how-much-does-snowplow-cost-to-run">How much does Snowplow cost to run, vs the competition?</a></li>
			
				<li><a href="/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar">Towards universal event analytics - building an event grammar</a></li>
			
				<li><a href="/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs">Unpicking the Snowplow data pipeline and how it drives AWS costs</a></li>
			
				<li><a href="/blog/2013/05/30/dealing-with-hadoops-small-files-problem">Dealing with Hadoop's small files problem</a></li>
			
		
		</ul>		
	
		<h1>Recruitment</h1>
		<ul>
		
			
				<li><a href="/blog/2013/12/20/introducing-our-snowplow-winterns">Introducing our Snowplow winterns</a></li>
			
				<li><a href="/blog/2013/10/07/announcing-our-winter-open-source-internship-program">Announcing our winter open source internship program</a></li>
			
		
		</ul>		
	
		<h1>Research</h1>
		<ul>
		
			
				<li><a href="/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript">Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</a></li>
			
		
		</ul>		
	

	<h1>Useful links</h1>
	<ul>
		<li><a href="/blog/atom.xml">Atom feed</a></li>
	</ul>
	<!--<strong>Tags</strong> -->
</div>

			<div id="footer">
	<p>Copyright Â© Snowplow Analytics Limited 2012 - 2013.  All rights reserved</p>
</div>
		</div>
	</div>
		<!-- Following Javascript function used by Disqus to count the number of comments for each blog post and display in the main index -->
	  	<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'snowplow'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
        </script>
        <!-- begin olark code -->
		<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
		f[z]=function(){
		(a.s=a.s||[]).push(arguments)};var a=f[z]._={
		},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
		f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
		0:+new Date};a.P=function(u){
		a.p[u]=new Date-a.p[0]};function s(){
		a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
		hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
		return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
		b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
		b.contentWindow[g].open()}catch(w){
		c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
		var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
		b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
		loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
		/* custom configuration goes here (www.olark.com/documentation) */
		olark.identify('9752-503-10-5227');/*]]>*/</script><noscript><a href="https://www.olark.com/site/9752-503-10-5227/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
		<!-- end olark code -->
		<!-- Track Olark chats in GTM (so can pass data onto Snowplow) -->
		<script type="text/javascript">
		olark('api.chat.onMessageToOperator', function(event) {
		    dataLayer.push({'event': 'olarkMessageToOperator'});
		});
		olark('api.chat.onMessageToVisitor', function(event) {
		    dataLayer.push({'event': 'olarkMessageToVisitor'});
		});
		</script>
		<!-- end track olark code -->


</body>
</html>