<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
  <title>The Snowplow Analytics Blog</title>
  <link href="http://snowplowanalytics.com/"/>
  <link type="application/atom+xml" rel="self" href="http://snowplowanalytics.com/blog/atom.xml"/>
  <updated>2014-01-27T18:47:30+00:00</updated>
  <id>http://snowplowanalytics.com/</id>
  <author>
    <name>The Snowplow Analytics Team</name>
    <email>contact@snowplowanalytics.com</email>
  </author>

  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts"/>
    <title>Snowplow JavaScript Tracker 0.13.0 released with custom contexts</title>
    <updated>2014-01-27T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to announce the immediate availability of the &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker/releases/tag/0.13.0'&gt;Snowplow JavaScript Tracker version 0.13.0&lt;/a&gt;. This is the first new release of the Snowplow JavaScript Tracker since separating it from the main Snowplow repository last year.&lt;/p&gt;

&lt;p&gt;The primary objective of this release was to introduce some key new tracking capabilities, in preparation for adding these to our Enrichment process. Secondarily, we also wanted to perform some outstanding housekeeping and tidy-up of the newly-independent repository.&lt;/p&gt;

&lt;p&gt;In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#contexts'&gt;New feature: custom contexts&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#currency'&gt;New feature: transaction currencies&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#platform'&gt;New feature: specifying the tracking platform&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#tidyup'&gt;Project tidy-up&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='contexts'&gt;1. New feature: custom contexts&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The most exciting new feature is the addition of custom contexts to all of our &lt;code&gt;track...()&lt;/code&gt; methods for event tracking.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that this release only adds custom contexts to the JavaScript Tracker. Adding custom contexts to our Enrichment process and Storage targets is on the roadmap - but rest assured we are working on it!&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;1.1 What are custom contexts?&lt;/h3&gt;
&lt;p&gt;Context is what describes the circumstances surrounding an individual event - for example, when the event happened, where it happened, or how it happened. For the original blog post where we introduced our ideas around event context, see &lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar/'&gt;Towards universal event analytics - building an event grammar&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our JavaScript Tracker already captures lots of standard web context by default: event time, user timezone, monitor color depth, browser features etc. This new feature will allow you to define and track your own custom contexts - ones which make sense to &lt;em&gt;your&lt;/em&gt; business.&lt;/p&gt;

&lt;p&gt;Think &amp;#8220;custom variables&amp;#8221; but much more powerful and flexible!&lt;/p&gt;
&lt;h3&gt;1.2 When to use custom contexts?&lt;/h3&gt;
&lt;p&gt;Custom contexts are great for a couple of use cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Whenever you want to augment a standard Snowplow event type with some additional data&lt;/li&gt;

&lt;li&gt;If your business has a set of crosscutting data points/models which you want to record against multiple event types&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can attach custom contexts to any Snowplow event type - even custom unstructured events.&lt;/p&gt;
&lt;h3&gt;1.3 Usage&lt;/h3&gt;
&lt;p&gt;To support custom contexts, we have added a new argument, called &lt;code&gt;contexts&lt;/code&gt;, onto the end of each &lt;code&gt;track...()&lt;/code&gt; and &lt;code&gt;add...()&lt;/code&gt; method. For example, here is the new signature for tracking a page view:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='kd'&gt;function&lt;/span&gt; &lt;span class='nx'&gt;trackPageView&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;customTitle&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;contexts&lt;/code&gt; argument is always optional on any event call. If set, it must be a JSON taking the form:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;context1_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='p'&gt;...&lt;/span&gt;
  &lt;span class='p'&gt;},&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;context2_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='p'&gt;...&lt;/span&gt;
  &lt;span class='p'&gt;},&lt;/span&gt;
  &lt;span class='p'&gt;...&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The format of the JSON properties for each individual context follows the exact same rules as our &lt;a href='https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#381-trackunstructevent'&gt;unstructured events&amp;#8217; JSON properties&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are interested in finding out more about custom contexts, we have written a &lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/'&gt;follow-up blog post&lt;/a&gt; - please &lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/'&gt;read this post&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h3&gt;1.4 Roadmap&lt;/h3&gt;
&lt;p&gt;We are well aware that this release is only the start of adding custom contexts to Snowplow. We are working on a common solution to Enriching and Storing both unstructured events and custom contexts.&lt;/p&gt;

&lt;p&gt;Please keep an eye on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap wiki page&lt;/a&gt; to see how Snowplow&amp;#8217;s support for custom contexts (and unstructured events) evolves.&lt;/p&gt;
&lt;h2&gt;&lt;a name='currency'&gt;2. New feature: transaction currencies&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We have updated our ecommerce tracking methods to add support for setting the currency which the transaction took place in.&lt;/p&gt;

&lt;p&gt;The new &lt;code&gt;currency&lt;/code&gt; argument is the penultimate argument (the last before &lt;code&gt;context&lt;/code&gt;, see above) to both the &lt;code&gt;addTrans()&lt;/code&gt; and &lt;code&gt;addItem()&lt;/code&gt; methods. Use it like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;1234&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;           &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;Acme Clothing&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;  &lt;span class='c1'&gt;// affiliation or store name&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;11.99&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;          &lt;span class='c1'&gt;// total - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;1.29&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;           &lt;span class='c1'&gt;// tax&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;5&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;              &lt;span class='c1'&gt;// shipping&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;San Jose&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;       &lt;span class='c1'&gt;// city&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;California&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;     &lt;span class='c1'&gt;// state or province&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;USA&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;            &lt;span class='c1'&gt;// country&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;USD&amp;#39;&lt;/span&gt;             &lt;span class='c1'&gt;// currency&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;1234&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;           &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;DD44&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;           &lt;span class='c1'&gt;// SKU/code - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;T-Shirt&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;        &lt;span class='c1'&gt;// product name&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;Green Medium&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;   &lt;span class='c1'&gt;// category or variation&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;11.99&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;          &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;              &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
    &lt;span class='s1'&gt;&amp;#39;USD&amp;#39;&lt;/span&gt;             &lt;span class='c1'&gt;// currency&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please make sure to pass in the valid &lt;a href='http://en.wikipedia.org/wiki/ISO_4217#Active_codes'&gt;ISO 4217&lt;/a&gt; code for your currency. This will ensure that your ecommerce transactions are compatible with the currency conversion enrichment we are currently developing (see &lt;a href='/blog/2014/01/17/scala-forex-library-released/'&gt;this blog post&lt;/a&gt; for details).&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t forget to set the currency on &lt;strong&gt;both&lt;/strong&gt; the parent transaction and its child items.&lt;/p&gt;
&lt;h2&gt;&lt;a name='platform'&gt;3. New feature: specifying the tracking platform&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/rcs'&gt;Ryan Sorensen&lt;/a&gt; for contributing the new &lt;code&gt;setPlatform()&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;This allows you to override the default tracking platform (&amp;#8220;web&amp;#8221;) with another of the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#11-application-parameters'&gt;platform values supported in the Snowplow Tracker Protocol&lt;/a&gt;. For example, to set the platform to &amp;#8220;mob&amp;#8221; for mobile:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setPlatform&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;mob&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thanks for your contribution Ryan!&lt;/p&gt;
&lt;h2&gt;&lt;a name='tidyup'&gt;4. Project tidy-up&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We have taken advantage of the move to a separate repository to perform some much needed tidy-up of the tracker codebase:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Added a complete &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker/blob/master/CHANGELOG'&gt;historic CHANGELOG&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Back-filled git tags for all of the tracker&amp;#8217;s releases&lt;/li&gt;

&lt;li&gt;Restructured the folders&lt;/li&gt;

&lt;li&gt;Added a package.json&lt;/li&gt;

&lt;li&gt;Added a node.js-friendly .gitignore&lt;/li&gt;

&lt;li&gt;Added some useful helper functions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As well as tidying up the repository, these updates should lay the groundwork for us replacing our custom &lt;code&gt;snowpak.sh&lt;/code&gt; Bash build script with a Grunt-based build process in the &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker/issues?milestone=3&amp;amp;page=1&amp;amp;state=open'&gt;next release&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;5. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.13.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note that as of this release, we are moving the Snowplow JavaScript Tracker to true &lt;a href='http://semver.org/spec/v2.0.0.html'&gt;semantic versioning&lt;/a&gt;. This means that going forwards we are also making this tracker available as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where 0 is the semantic MAJOR version. If you prefer, you can use this URI path and then get new features and bug fixes automatically as we roll-out MINOR and PATCH updates to the tracker. Any breaking changes will mean a new MAJOR version, which will be hosted on &lt;code&gt;/1/sp.js&lt;/code&gt;, i.e. it won&amp;#8217;t break your existing installation.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Check out the &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker/releases/tag/0.13.0'&gt;v0.13.0 release page&lt;/a&gt; on GitHub for the full list of changes made in this version.&lt;/p&gt;

&lt;p&gt;As always, if you run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/27/snowplow-custom-contexts-guide</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/27/snowplow-custom-contexts-guide"/>
    <title>A guide to custom contexts in Snowplow JavaScript Tracker 0.13.0</title>
    <updated>2014-01-27T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier today we &lt;a href='/blog/2014/01/27/snowplow-javascript-tracker-0.13.0-released-with-custom-contexts/'&gt;announced the release of Snowplow JavaScript Tracker 0.13.0&lt;/a&gt;, which updated all of our &lt;code&gt;track...()&lt;/code&gt; methods to support a new argument for setting custom JSON contexts.&lt;/p&gt;

&lt;p&gt;In our earlier blog post we introduced the idea of custom contexts only very briefly. In this blog post, we will take a detailed look at Snowplow&amp;#8217;s custom contexts functionality, so you can understand how best to attach custom contexts to your existing Snowplow events.&lt;/p&gt;

&lt;p&gt;Understanding the custom context format is important because our Enrichment process does not yet extract custom contexts, so you will not get any feedback yet from the Enrichment process as to whether you are adding them correctly; nor do we have validation for custom context properties in our JavaScript Tracker yet.&lt;/p&gt;

&lt;p&gt;In the rest of this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/#sigs'&gt;Updated method signatures&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/#contexts'&gt;The &lt;code&gt;contexts&lt;/code&gt; JavaScript object&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/#eg'&gt;A detailed example&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/27/snowplow-custom-contexts-guide/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;#8217;s get started after the jump.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='sigs'&gt;1. Updated method signatures&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You can add custom contexts to any of the existing &lt;code&gt;track...()&lt;/code&gt; methods, and in the case of our transaction tracking, with the two &lt;code&gt;add...()&lt;/code&gt; methods.&lt;/p&gt;

&lt;p&gt;For completeness, here is the full list of updated tracking methods. Note that &lt;code&gt;contexts&lt;/code&gt; is always the last argument:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;trackPageView&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;customTitle&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='nx'&gt;trackStructEvent&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;action&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;label&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;property&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;value&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='nx'&gt;trackUnstructEvent&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;name&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;properties&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; 
&lt;span class='nx'&gt;addTrans&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;affiliation&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;total&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;tax&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;shipping&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;city&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;state&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;country&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;currency&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; 
&lt;span class='nx'&gt;addItem&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;sku&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;name&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;price&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;quantity&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;currency&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;contexts&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few notes on this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you set the &lt;code&gt;contexts&lt;/code&gt; argument on a &lt;code&gt;trackPageView&lt;/code&gt;, the same context will be attached to all page pings too (assuming you have &lt;code&gt;enableActivityTracking&lt;/code&gt; enabled)&lt;/li&gt;

&lt;li&gt;Remember that transactions and their child items are processed as separate Snowplow events. Duplicate your transaction &lt;code&gt;contexts&lt;/code&gt; argument into your items&amp;#8217; &lt;code&gt;contexts&lt;/code&gt; argument if you need the transaction context available to the transaction item events&lt;/li&gt;

&lt;li&gt;If you want to add custom context but don&amp;#8217;t want to set all the prior arguments, just pad out your function call with empty arguments:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
              &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// &amp;lt;- Skip the custom page title (get page title automatically instead)&lt;/span&gt;
              &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='nx'&gt;page&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
                  &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;sport&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='nx'&gt;last_updated$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2014&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;26&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
              &lt;span class='p'&gt;}&lt;/span&gt;
           &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;a name='contexts'&gt;2. The &quot;contexts&quot; JavaScript object&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;contexts&lt;/code&gt; argument to any &lt;code&gt;track...()&lt;/code&gt; or &lt;code&gt;add...()&lt;/code&gt; method is always optional. If set, it must be a JSON taking the form:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;context1_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='cm'&gt;/* context1 JSON */&lt;/span&gt;
  &lt;span class='p'&gt;},&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;context2_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='cm'&gt;/* context2 JSON */&lt;/span&gt;
  &lt;span class='p'&gt;},&lt;/span&gt;
  &lt;span class='p'&gt;...&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If the &lt;code&gt;contexts&lt;/code&gt; argument is set, it must be a JSON including at least one name: property pair, where:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The name is the name for an individual context entry&lt;/li&gt;

&lt;li&gt;The property is a JSON holding name: property pairs for this context entry&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A few dos and don&amp;#8217;ts for context names:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Do&lt;/strong&gt; name each context entry however you like&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Do&lt;/strong&gt; use a context name to identify a set of associated data points which make sense to your business&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Do&lt;/strong&gt; use the same contexts across multiple different events and event types&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Don&amp;#8217;t&lt;/strong&gt; use multiple different context names to refer to the same set of data points&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few dos and don&amp;#8217;ts for the JSONs inside each context entry JSONs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Do&lt;/strong&gt; use any of the data types supported by &lt;a href='https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#381-trackunstructevent'&gt;custom unstructured events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Do&lt;/strong&gt; use Snowplow datatype suffixes if the data type would otherwise be unclear&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Don&amp;#8217;t&lt;/strong&gt; nest properties - as with custom unstructured events, the structure must be flat&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name='eg'&gt;3. A detailed example&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;A detailed example should make custom contexts a little more real.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take a retailer and Snowplow user who sells movie memorabilia online, particularly movie posters. For every movie poster, our retailer cares about the name of the movie, the country which printed the movie poster and the year the poster was printed. She has also done some work understanding her customer base, and can assign all of her visitors a propensity-to-buy score and a customer segment as soon as they add something to their basket.&lt;/p&gt;
&lt;h3&gt;Definition of custom contexts&lt;/h3&gt;
&lt;p&gt;Based on the above, our retailer will define two custom contexts. The first describes a given movie poster:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='s2'&gt;&amp;quot;movie_poster&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;movie_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='nx'&gt;xxx&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;poster_country&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='nx'&gt;xxx&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;poster_year$dt&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='nx'&gt;xxx&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then the second context describes the customer:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='s2'&gt;&amp;quot;customer&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;p_buy&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='nx'&gt;xxx&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;segment&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='nx'&gt;xxx&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With her two custom contexts defined, our retailer is now ready to update her Snowplow installation to start collecting this additional data.&lt;/p&gt;
&lt;h3&gt;Definition of custom contexts&lt;/h3&gt;
&lt;p&gt;When a visitor arrives on a page advertising a movie poster, our retailer adds this context to the page view event:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
              &lt;span class='p'&gt;,&lt;/span&gt;                            &lt;span class='c1'&gt;// &amp;lt;- No custom page title&lt;/span&gt;
              &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;movie_poster&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;          &lt;span class='c1'&gt;// &amp;lt;- Context entry&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;movie_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Solaris&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;poster_country&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;JP&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;poster_year$dt&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1978&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
              &lt;span class='p'&gt;}&lt;/span&gt;
           &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the visitor clicks the add to basket button on this poster, the website fires a custom unstructured event to track the add to basket. This time we send the &amp;#8220;customer&amp;#8221; context as well as the &amp;#8220;movie_poster&amp;#8221; context:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackUnstructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
              &lt;span class='s1'&gt;&amp;#39;add-to-basket&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;             &lt;span class='c1'&gt;// &amp;lt;- Event name&lt;/span&gt;
              &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;button&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;img-overlay&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;   &lt;span class='c1'&gt;// &amp;lt;- Event properties&lt;/span&gt;
                &lt;span class='s2'&gt;&amp;quot;stock-level&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt; 
              &lt;span class='p'&gt;},&lt;/span&gt;
              &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;movie_poster&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;          &lt;span class='c1'&gt;// &amp;lt;- First context entry&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;movie_name&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Solaris&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;poster_country&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;JP&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;poster_year$dt&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1978&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;},&lt;/span&gt;
                &lt;span class='s2'&gt;&amp;quot;customer&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;              &lt;span class='c1'&gt;// &amp;lt;- Second context entry&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;p_buy&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;0.23&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;segment&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;whale&amp;quot;&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
              &lt;span class='p'&gt;}&lt;/span&gt;
           &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The visitor seems to be having doubts - they head to the contact page and fill out a form. This time we still track the &amp;#8220;customer&amp;#8221; context (with a reduced propensity-to-buy score), but it doesn&amp;#8217;t make sense to record this event against any given &amp;#8220;movie_poster&amp;#8221;, so we don&amp;#8217;t include that context:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackStructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
              &lt;span class='s1'&gt;&amp;#39;comms&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// &amp;lt;- Category&lt;/span&gt;
              &lt;span class='s1'&gt;&amp;#39;feedback-form&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;             &lt;span class='c1'&gt;// &amp;lt;- Action&lt;/span&gt;
              &lt;span class='p'&gt;,&lt;/span&gt;                            &lt;span class='c1'&gt;// &amp;lt;- No label&lt;/span&gt;
              &lt;span class='p'&gt;,&lt;/span&gt;                            &lt;span class='c1'&gt;// &amp;lt;- No property&lt;/span&gt;
              &lt;span class='p'&gt;,&lt;/span&gt;                            &lt;span class='c1'&gt;// &amp;lt;- No value&lt;/span&gt;
              &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;customer&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;              &lt;span class='c1'&gt;// &amp;lt;- Context entry&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;p_buy&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;0.13&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                  &lt;span class='s2'&gt;&amp;quot;segment&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;whale&amp;quot;&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
              &lt;span class='p'&gt;}&lt;/span&gt;
           &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that completes our example. By defining and implementing these two custom contexts, our retailer has promoted some of her most business-critical data points to the level of first class entities in Snowplow.&lt;/p&gt;

&lt;p&gt;This additional context has made standard Snowplow events such as page views much more valuable to her, and should later make it easy to analyze these business-critical data points across multiple event types.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above guide, please do get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any ideas or feedback for Snowplow&amp;#8217;s evolving approach to custom contexts, do please share them, either in the comments below or through the usual channels. And please keep an eye on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap wiki page&lt;/a&gt; to see how Snowplow&amp;#8217;s support for custom contexts evolves!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/21/snowplow-team-in-nyc-and-boston-in-feb-2014</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/21/snowplow-team-in-nyc-and-boston-in-feb-2014"/>
    <title>The Snowplow team will be in New York and Boston in February - get in touch if you'd like to meet</title>
    <updated>2014-01-21T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Both Yali and myself will be visiting New York this February, and I will be heading on to Boston too. If you&amp;#8217;re interested in meeting up to discuss Snowplow, event analytics or big data processing more generally, we&amp;#8217;d love to arrange a meeting.&lt;/p&gt;

&lt;p&gt;&lt;img alt='brooklyn-bridge' src='/static/img/blog/2014/01/brooklyn-bridge.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Yali will be in NYC between February 9th and 12th, including attending the &lt;a href='http://www.meetup.com/Looker-User-Group-NY/events/156298422/'&gt;Looker user group NYC meetup&lt;/a&gt; on February 11th.&lt;/p&gt;

&lt;p&gt;Alex will be visiting later on in the month:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;New York City, February 20th - 25th&lt;/li&gt;

&lt;li&gt;Boston, February 26th - 27th&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reach out to us at yali@ or alex@ (email addresses on our full domain) to setup a meet, suggest an event we should attend or for anything else East Coast-related!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/20/the-three-eras-of-business-data-processing</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/20/the-three-eras-of-business-data-processing"/>
    <title>The three eras of business data processing</title>
    <updated>2014-01-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Every so often, a work emerges that captures and disseminates the bleeding edge so effectively as to define a new norm. For those of us working in eventstream analytics, that moment came late in 2013 with the publication of Jay Kreps&amp;#8217; monograph &lt;a href='http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying'&gt;The Log: What every software engineer should know about real-time data&amp;#8217;s unifying abstraction&lt;/a&gt;. Anyone involved in the operation or analysis of a digital business ought to read Jay&amp;#8217;s piece in its entirety. His central point, convincingly argued, is that every digital business should be (re-)structured around a centralized event firehose which:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;aggregates events from disparate source systems,&lt;/li&gt;

&lt;li&gt;stores them in what Jay calls a &amp;#8220;unified log&amp;#8221;, and&lt;/li&gt;

&lt;li&gt;enables data processing applications to operate on this stream&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sounds familiar? At Snowplow we believe passionately in putting a continuous stream of immutable events at the heart of your digital business, and we want Snowplow to power this &amp;#8220;digital nervous system&amp;#8221; for companies large and small. Jay&amp;#8217;s monograph validated our approach-to-date (e.g. on the importance of &lt;a href='http://snowplowanalytics.com/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar/'&gt;building an event grammar&lt;/a&gt;), but also moved our thinking forwards in a number of ways.&lt;/p&gt;

&lt;p&gt;In the rest of this blog post, rather than simply re-hashing Jay&amp;#8217;s thoughtpiece, I want to create a new baseline by mapping out the historical and ongoing evolution of business data processing, extending up to the unified log espoused by Jay. I have split this evolution into two distinct eras which I have lived through and experienced firsthand, plus a third era which is soon approaching:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/20/the-three-eras-of-business-data-processing/#classic-era'&gt;The Classic Era - the pre-big data, pre-SaaS era of operational systems and batch-loaded data warehouses&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/20/the-three-eras-of-business-data-processing/#hybrid-era'&gt;The Hybrid Era - today&amp;#8217;s hotchpotch of different systems and approaches&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/20/the-three-eras-of-business-data-processing/#unified-era'&gt;The Unified Era - a future enabled by continuous data processing on a unified log&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/20/the-three-eras-of-business-data-processing/#closing-thoughts'&gt;Closing thoughts&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;#8217;ll explore each of these eras in turn:&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='classic-era'&gt;The Classic Era&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When I started work at Deloitte Consulting 12 years ago, even forward-thinking businesses still primarily operated a disparate set of on-premise transactional systems. Each of these systems would feature: an internal &amp;#8220;local loop&amp;#8221; for data processing; its own data silo; and, when unavoidable, point-to-point connections to peer systems. To give the Management team a much-needed view &lt;em&gt;across&lt;/em&gt; these systems, a &amp;#8220;Ralph Kimball&amp;#8221; data warehouse might be added, typically fed from the transactional systems overnight by a set of batch ETL processes. Where present, this provided a &lt;em&gt;single version of the truth&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The whole system looked something like this - using the case of a retailer for our example:&lt;/p&gt;

&lt;p&gt;&lt;img alt='classic-era-img' src='/static/img/blog/2014/01/classic-era.png' /&gt;&lt;/p&gt;

&lt;p&gt;And that was pretty much it. In truth, most businesses still run on a close descendant of this approach, albeit with more SaaS services mixed in. However, some businesses, particularly those in fast-moving sectors like retail and media, have made the leap to what we might call the Hybrid Era:&lt;/p&gt;
&lt;h2&gt;&lt;a name='hybrid-era'&gt;The Hybrid Era&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Hybrid Era is characterized by companies operating a real hotchpotch of different transactional and analytics systems - some on-premise packages, some from SaaS vendors, plus some home-grown systems.&lt;/p&gt;

&lt;p&gt;It is hard to generalize what these architectures look like - again we see strong local loops and data silos, but we also see attempts at &amp;#8220;log everything&amp;#8221; approaches with Hadoop and/or systems monitoring. There tends to be a mix of near-real-time processing for narrow analytics use cases like product recommendations, plus separate batch processing efforts into Hadoop or a classic data warehouse. We see attempts to bulk export data from external SaaS vendors for warehousing, and efforts to feed these external systems with the data they require for their own local loops.&lt;/p&gt;

&lt;p&gt;Keeping our multi-channel retailer in mind, here is what their architecture looks like now:&lt;/p&gt;

&lt;p&gt;&lt;img alt='hybrid-era-img' src='/static/img/blog/2014/01/hybrid-era.png' /&gt;&lt;/p&gt;

&lt;p&gt;This looks complicated, but is really something of a simplification - most businesses will have a much more complex systems landscape. But even with this simplification, we can see some real limitations of this approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;There is no single version of the truth&lt;/strong&gt; - data is now warehoused in multiple places, split depending on the data volumes and the analytics latency required&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Decisioning has become fragmented&lt;/strong&gt; - the number of local systems loops, each operating on siloed data, has grown since the Classic Era. These loops represent a highly fragmented approach to making near-real-time decisions from data&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Point-to-point connections have proliferated&lt;/strong&gt; - as the number of systems has grown, the number of point-to-point connections has exploded. Many of these connections are fragile or incomplete; getting sufficiently-granular and timely data out of external SaaS systems is particularly challenging&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Analytics can have low latency or wide data coverage, but not both&lt;/strong&gt; - where stream processing is selected for low latency, it becomes effectively another local loop. The warehouses aim for much wider data coverage, but at the cost of duplication of data and high latency&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='unified-era'&gt;The Unified Era&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;These two eras bring us up to the present day, and the coming Unified Era of data processing. The key innovation is placing what Jay calls a unified log at the heart of all of our data collection and processing. A unified log is an append-only log to which we write all events generated by our applications. Going further, it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can be read from at low latency&lt;/li&gt;

&lt;li&gt;Is readable by multiple applications simultaneously, and different applications can have different &amp;#8220;cursor positions&amp;#8221;&lt;/li&gt;

&lt;li&gt;Only holds a few days&amp;#8217; worth of data. But we can archive the historic log in HDFS or S3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s see what this architecture looks like conceptually:&lt;/p&gt;

&lt;p&gt;&lt;img alt='unified-era-img' src='/static/img/blog/2014/01/unified-era.png' /&gt;&lt;/p&gt;

&lt;p&gt;A few things should be clear, especially in contrast to the earlier eras:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;All systems can and should write to the unified log&lt;/strong&gt; - third-party SaaS vendors can emit events via webhooks and streaming APIs. In the case where vendors cannot provide an eventstream (e.g. with web analytics), those services are brought back in-house&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;We have a single version of the truth&lt;/strong&gt; - together, the unified log plus Hadoop archive represent our single version of the truth. They contain exactly the same data - our eventstream - they just have different time windows of data&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;The single version of the truth is upstream from the datawarehouse&lt;/strong&gt; - in the classic era, the data warehouse provided the single version of the truth, so all reports generated from it were consistent. In the Unified Era, it is now the log that provides the single version of the truth: as a result, your operational systems (e.g. product recommendations and ad targeting) work off the exact same data as your analysts use for ad hoc analytics and management reporting&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Point-to-point connections have largely gone away&lt;/strong&gt; - in their place, applications can append to the unified log and other applications can read their writes&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Local loops have been unbundled&lt;/strong&gt; - in place of local silos, applications can collaborate on real-time decisioning via the unified log&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='closing-thoughts'&gt;Closing thoughts&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This has been something of a whirlwind tour through the evolution of data processing for business, as we have experienced it here at Snowplow - and before Snowplow.&lt;/p&gt;

&lt;p&gt;I have tried to delineate this evolution into three distinct eras - you may not agree entirely with the terminology or the facets of each era, but I hope it has prompted some ideas about where eventstream analytics is coming from and where it is heading. For us at Snowplow, we are convinced that the unified log is a breakthrough concept in understanding how best to engineer digital businesses. And we plan to evolve Snowplow to help enable this &amp;#8220;digital nervous system&amp;#8221; for &lt;strong&gt;your&lt;/strong&gt; business.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/17/scala-forex-library-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/17/scala-forex-library-released"/>
    <title>Scala Forex library by wintern Jiawen Zhou released</title>
    <updated>2014-01-17T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are proud to announce the release of our new &lt;a href='https://github.com/snowplow/scala-forex'&gt;Scala Forex&lt;/a&gt; library, developed by &lt;a href='/blog/2013/12/20/introducing-our-snowplow-winterns/'&gt;Snowplow wintern Jiawen Zhou&lt;/a&gt;. Jiawen joined us in the Snowplow offices in London this winter and was tasked with taking Scala Forex from a README file to an enterprise-strength Scala library for foreign exchange operations. One month later and we are hugely excited to be sharing her work with the community!&lt;/p&gt;

&lt;p&gt;Scala Forex is a high-performance Scala library for performing exchange rate lookups and currency conversions, leveraging the excellent &lt;a href='https://openexchangerates.org/signup?r=snowplow'&gt;Open Exchange Rates web service&lt;/a&gt;. We are excited to be working with &lt;a href='https://openexchangerates.org/signup?r=snowplow'&gt;Open Exchange Rates&lt;/a&gt;, Snowplow&amp;#8217;s second external data provider after MaxMind.&lt;/p&gt;

&lt;p&gt;In Jiawen&amp;#8217;s own words:&lt;/p&gt;

&lt;p&gt;&lt;img alt='jiawen-img' src='/static/img/blog/2014/01/jiawen-scala-forex.png' /&gt;&lt;/p&gt;

&lt;p&gt;In the rest of this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/17/scala-forex-library-released/#rationale'&gt;Why we wrote this library&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/17/scala-forex-library-released/#architecture'&gt;How the library is architected&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/17/scala-forex-library-released/#usage'&gt;How to use the library&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/17/scala-forex-library-released/#thanks'&gt;Thanks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='rationale'&gt;Why we wrote this library&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Late last year a Snowplow customer asked us if we could add currency conversions into their custom Snowplow implementation. This seemed like a great idea to explore, and we started to sketch out an approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add a new currency field into our ecommerce transaction tracking, to indicate the currency of the transaction (see &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker/issues/34'&gt;this ticket&lt;/a&gt; for the JavaScript)&lt;/li&gt;

&lt;li&gt;In our Enrichment process, convert all transactions to the Snowplow user&amp;#8217;s &amp;#8220;base currency&amp;#8221;&lt;/li&gt;

&lt;li&gt;In our Storage targets, store both the original transaction value and the value converted to the user&amp;#8217;s base currency, for easy revenue analysis&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The big missing piece for the above was a Scala library to handle currency conversions. We had some very specific requirements for this library:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It had to support close to all of the world&amp;#8217;s national currencies&lt;/li&gt;

&lt;li&gt;It had to support historical (&amp;#8220;end of day&amp;#8221;) currency conversions. This is because a Snowplow Enrichment process can be run over many days or months of historical event data&lt;/li&gt;

&lt;li&gt;It had to minimize the number of external web service calls. Running in an environment like Hadoop or Kinesis, we cannot afford to call a web service each time for millions of individual events&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Custom building a Scala library calling out to the &lt;a href='https://openexchangerates.org/signup?r=snowplow'&gt;Open Exchange Rates web service&lt;/a&gt; seemed the right way to meet these requirements, and so Scala Forex was born!&lt;/p&gt;
&lt;h2&gt;&lt;a name='architecture'&gt;How the library is architected&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Scala Forex makes heavy use of the Joda-Money and Joda-Time libraries in its public API. These are enterprise-grade Java libraries for working with currency, money and time, and we were keen not to re-invent the wheel!&lt;/p&gt;

&lt;p&gt;Under the covers, Jiawen&amp;#8217;s library makes heavy use of two &amp;#8220;least recently used&amp;#8221; (LRU) caches, one to hold historic exchange rates and one to hold recent rates. It is these LRU caches which minimize calls out to the &lt;a href='https://openexchangerates.org/signup?r=snowplow'&gt;Open Exchange Rates web service&lt;/a&gt;, which is critical for performance (and cost) reasons.&lt;/p&gt;

&lt;p&gt;The library is thoroughly tested using Specs2 (including Specs2 tables), and Mockito to verify cache behaviour. The library is integrated into Travis CI for continuous testing.&lt;/p&gt;
&lt;h2&gt;&lt;a name='usage'&gt;How to use the library&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Using the library is straightforward: you initialize Scala Forex with general and OER-specific configuration, and then use a simple Scala DSL to both look up exchange rates and perform currency conversions.&lt;/p&gt;

&lt;p&gt;For detailed guidance on configuring the library, please see the &lt;a href='https://github.com/snowplow/scala-forex#22-configuration'&gt;Configuration section&lt;/a&gt; of the Scala Forex README.&lt;/p&gt;
&lt;h3&gt;Exchange rate lookups&lt;/h3&gt;
&lt;p&gt;Once initialized, an exchange rate lookup is as simple as:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;jpy2gbp&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;fx&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;rate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;JPY&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;to&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;GBP&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;nowish&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here&amp;#8217;s a slightly more complex example, of looking up a historic rate:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='c1'&gt;// Base currency is USD&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;tradeDate&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;DateTime&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2011&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;13&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;11&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;39&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;27&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;567&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='nc'&gt;DateTimeZone&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;forID&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;America/New_York&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;usd2yen&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;fx&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;rate&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;to&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;JPY&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;at&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;tradeDate&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For detailed help on currency lookups, please see the &lt;a href='https://github.com/snowplow/scala-forex#31-rate-lookup'&gt;Rate Lookup section&lt;/a&gt; of the Scala Forex README.&lt;/p&gt;
&lt;h3&gt;Currency conversions&lt;/h3&gt;
&lt;p&gt;A currency conversion can be as simple as:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='c1'&gt;// Base currency is GBP&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;gbpPriceInEuros&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;fx&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;convert&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mf'&gt;9.99&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;to&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;EUR&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;now&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here&amp;#8217;s a slightly more complex example, of converting a trade using the end of day rate:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;eodDate&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;DateTime&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2011&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;13&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;0&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;tradeInYen&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;fx&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;convert&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;10000&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;GBP&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;to&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;JPY&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='n'&gt;at&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;eodDate&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For detailed help on currency conversions, please see the &lt;a href='https://github.com/snowplow/scala-forex#32-currency-conversion'&gt;Currency Conversion section&lt;/a&gt; of the Scala Forex README.&lt;/p&gt;
&lt;h2&gt;&lt;a name='thanks'&gt;Thanks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;And that&amp;#8217;s it; huge thanks to Jiawen Zhou for delivering such a sophisticated library in such a short time - and in a language she had never used before!&lt;/p&gt;

&lt;p&gt;We hope that you find the Scala Forex library helpful - and stay tuned for when we can integrate it into Snowplow as part of our planned new currency enrichment capabilities!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/15/amazon-kinesis-tutorial-getting-started-guide</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/15/amazon-kinesis-tutorial-getting-started-guide"/>
    <title>Amazon Kinesis tutorial - a getting started guide</title>
    <updated>2014-01-15T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Of all the developments on the Snowplow roadmap, the one that we are most excited about is porting the Snowplow data pipeline to Amazon Kinesis to deliver real-time data processing. We will publish a separate post outlining why we are so excited about this. (Hint: it&amp;#8217;s about a lot more than simply real-time analytics on Snowplow data.) This blog post is intended to provide a starting point for developers who are interested in diving into Kinesis.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this tutorial, we will walk through the process of getting up and running with Amazon Kinesis using two very simple Kinesis apps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;a href='https://github.com/snowplow/kinesis-example-scala-producer'&gt;kinesis-example-scala-producer&lt;/a&gt;: this will create a Kinesis stream and write records to it&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/snowplow/kinesis-example-scala-producer'&gt;kinesis-example-scala-consumer&lt;/a&gt;: this will consume the Kinesis stream created by the producer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The source code for both is available on the &lt;a href='https://github.com/snowplow'&gt;Snowplow repo&lt;/a&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='setting_up_the_environment_to_run_the_apps'&gt;Setting up the environment to run the apps&lt;/h2&gt;

&lt;p&gt;In general Kinesis apps should run on EC2. However, for this simple example, the apps can be run locally. They require Java 1.7 and SBT 0.13.0 to run. If you use Vagrant, you can run them in the &lt;a href='https://github.com/snowplow/dev-environment'&gt;dev-environment&lt;/a&gt; VM, by setting it up as follows:&lt;/p&gt;

&lt;p&gt;First, clone the &lt;a href='https://github.com/snowplow/dev-environment'&gt;dev-environment&lt;/a&gt; repo (make sure to include the &lt;code&gt;--recursive&lt;/code&gt; flag):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;git clone --recursive https://github.com/snowplow/dev-environment.git
&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd &lt;/span&gt;dev-environment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now build the VM:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;vagrant up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the build is complete, SSH in:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;vagrant ssh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And now install both Java 1.7 and SBT by running the following two Ansible playbook (from within the VM):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;ansible-playbook /vagrant/ansible-playbooks/generic/base.yaml &lt;span class='se'&gt;\&lt;/span&gt;
  --inventory-file&lt;span class='o'&gt;=&lt;/span&gt;/home/vagrant/ansible_hosts --connection&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='nb'&gt;local&lt;/span&gt;

&lt;span class='nv'&gt;$ &lt;/span&gt;ansible-playbook /vagrant/ansible-playbooks/generic/jvm/jvm-7.yaml &lt;span class='se'&gt;\&lt;/span&gt;
  --inventory-file&lt;span class='o'&gt;=&lt;/span&gt;/home/vagrant/ansible_hosts --connection&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='nb'&gt;local&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We&amp;#8217;re now ready to install the example apps and start writing two and reading from Kinesis streams!&lt;/p&gt;

&lt;h2 id='creating_a_stream_and_writing_records_to_it'&gt;Creating a stream and writing records to it&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;re going to use the &lt;a href='https://github.com/snowplow/kinesis-example-scala-producer'&gt;kinesis-example-scala-producer&lt;/a&gt; to create our stream and write records to it.&lt;/p&gt;

&lt;p&gt;First clone the repo, then compile the app:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;git clone git://github.com/snowplow/kinesis-example-scala-producer.git
&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd &lt;/span&gt;kinesis-example-scala-producer
&lt;span class='nv'&gt;$ &lt;/span&gt;sbt compile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we need to create a config file (e.g. by copying the template config file to a new file in the project root):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;cp src/main/resources/default.conf my.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Use your favorite text editor to edit the AWS credentials in the file with your own access key and secret access key. If you are creating a new user in IAM for the purpose of this tutorial, make sure that user has permissions to create and write to Kinesis streams, and create, write to and delete DynamoDB tables.&lt;/p&gt;

&lt;p&gt;You&amp;#8217;re now ready to run the app! Enter the following at the command line - this runs it from SBT, passing in the new config file as an argument:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;sbt &lt;span class='s2'&gt;&amp;quot;run --config ./my.conf&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the app has started, it will create a new stream (if one does not already exist) with the name specified in the config file (this is &lt;code&gt;kinesis_exmaple&lt;/code&gt; as standard). You should be able to view the stream in the AWS management console:&lt;/p&gt;

&lt;p&gt;&lt;img alt='pic-of-aws-console-with-metrics-rising' src='/static/img/blog/2014/01/kinesis/kinesis-example-stream-in-aws-console.png' /&gt;&lt;/p&gt;

&lt;p&gt;If you click on the stream in the management console, you should see be able to see an increase in &lt;strong&gt;Put Requests&lt;/strong&gt; after you started the app. Note that this may take a few minutes before it is visibile in the management console.&lt;/p&gt;

&lt;p&gt;&lt;img alt='put-requests-in-console' src='/static/img/blog/2014/01/kinesis/aws-console-put-requests.png' /&gt;&lt;/p&gt;

&lt;p&gt;The console should look like this, as the app writes new records to the stream:&lt;/p&gt;

&lt;p&gt;&lt;img alt='pic-of-console-for-produer' src='/static/img/blog/2014/01/kinesis/kinesis-example-scala-producer.png' /&gt;&lt;/p&gt;

&lt;p&gt;You are now writing records to your first Kinesis stream!&lt;/p&gt;

&lt;h2 id='consuming_records_from_the_stream'&gt;Consuming records from the stream&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;re going to use the &lt;a href='https://github.com/snowplow/kinesis-example-scala-consumer'&gt;kinesis-example-scala-consumer&lt;/a&gt; to read records from our stream.&lt;/p&gt;

&lt;p&gt;First clone the repo, then compile the app:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd&lt;/span&gt; ..
&lt;span class='nv'&gt;$ &lt;/span&gt;git clone git://github.com/snowplow/kinesis-example-scala-consumer.git
&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd &lt;/span&gt;kinesis-example-scala-consumer
&lt;span class='nv'&gt;$ &lt;/span&gt;sbt compile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As before, we create a config file (e.g. by copying the template config file to a new file in the project root):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;cp src/main/resources/default.conf my.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And edit the &lt;code&gt;my.conf&lt;/code&gt; file in our favorite text editor to add our AWS credentials. The rest of the parameters should be fine, although if you have configured the name of the stream for the producer config, you will need to configure it in the consumer config so that it reads from the same stream that the producer writes to.&lt;/p&gt;

&lt;p&gt;Now run the consumer:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;sbt &lt;span class='s2'&gt;&amp;quot;run --config ./my.conf&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see something like this, as your consumer iterates through each record in the stream:&lt;/p&gt;

&lt;p&gt;&lt;img alt='pic-of-consumer-at-command-line' src='/static/img/blog/2014/01/kinesis/kinesis-example-scala-consumer.png' /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;#8217;re now successfully reading records off the Kinesis stream!&lt;/p&gt;

&lt;h2 id='thanks'&gt;Thanks&lt;/h2&gt;

&lt;p&gt;These two Kinesis apps were written in collaboration with our wintern &lt;a href='https://github.com/bamos'&gt;Brandon Amos&lt;/a&gt;, who has been working exclusively on Kinesis development at Snowplow over his winternship. This is just the start - we hope to release Kinesis enabled modules for the core Snowplow stack that have also been developed with Brandon in the next couple of weeks. Stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support"/>
    <title>Snowplow 0.8.13 released with Looker support</title>
    <updated>2014-01-08T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the release of Snowplow 0.8.13. This release makes it easy for Snowplow users to get started analyzing their Snowplow data with &lt;a href='http://looker.com/'&gt;Looker&lt;/a&gt;, by providing an initial Snowplow data model for Looker so that a whole host of standard dimensions, metrics, entities and events are recognized in the Looker query interface.&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/7-days-dashboard-quickstart.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/7-days-dashboard-quickstart.JPG' title='7 day web analytics dashboard built with Snowplow data and Looker UI' /&gt;&lt;/a&gt;
&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support/#special'&gt;What&amp;#8217;s so special about analyzing Snowplow data with Looker?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support/#what'&gt;What does the Looker metadata model deliver?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support/#install'&gt;How to install the metadata model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='special'&gt;What's so special about analyzing Snowplow data with Looker?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow makes it possible to analyze your granular, event-level data with any BI tool. So what&amp;#8217;s so special about Looker?&lt;/p&gt;

&lt;p&gt;In a nutshell, Looker makes it fast and simple for people with no SQL knowledge to explore Snowplow data via a convenient web UI. We&amp;#8217;ve gone into detail about why Looker is so well suited to analyzing Snowplow data in the following two blog posts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data/'&gt;Introducing Looker - a fresh approach to Business intelligence that works beautifully with Snowplow&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/'&gt;Five things that make analyzing Snowplow data in Looker an absolute pleasure&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='what'&gt;2. What does the Looker metadata model deliver?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;By loading Snowplow&amp;#8217;s metadata model into Looker, you immediately have:&lt;/p&gt;

&lt;h4 id='1_the_ability_to_slice__dice_snowplow_data_across_a_wide_range_of_dimensions_and_metrics_via_the_looker_query_interface'&gt;1. The ability to slice / dice Snowplow data across a wide range of dimensions and metrics via the Looker query interface&lt;/h4&gt;

&lt;p&gt;The model includes a large number of metrics and dimensions - the screenshot below illustrates just &lt;em&gt;some&lt;/em&gt; of the metrics available:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/list-of-metrics.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/list-of-metrics.JPG' title='Just some of the metrics available in the Looker model for Snowplow data' /&gt;&lt;/a&gt;
&lt;h4 id='2_the_ability_to_zoom_up_to_visitorlevel_countrylevel_channel_level_analysis_or_down_to_transactionlevel_eventlevel_data_seamlessly'&gt;2. The ability to zoom up to visitor-level, country-level, channel level analysis or down to transaction-level, event-level data seamlessly&lt;/h4&gt;

&lt;p&gt;The model makes it easy to zoom up to view country-level, channel level data e.g.:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/zoom-up.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/zoom-up.JPG' title='zooming out with Looker on Snowplow data' /&gt;&lt;/a&gt;
&lt;p&gt;And to drill down to individual event-level data, e.g. so that we can view the event stream for a particular visitor over time:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/drill-down.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/drill-down.JPG' title='Drilling down to event-level data with Snowplow and Looker' /&gt;&lt;/a&gt;
&lt;h4 id='3_quickstart_dashboards'&gt;3. Quick-start dashboards&lt;/h4&gt;

&lt;p&gt;The model includes a general-purpose dashboard for reporting on the last 7 days:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/7-days-dashboard-quickstart.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/7-days-dashboard-quickstart.JPG' title='7 day dashboard for web analytics data built in Looker on top of Snowplow data' /&gt;&lt;/a&gt;
&lt;p&gt;And a general purpose dashboard for reporting on the last 6 months:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/6-months-dashboard-quickstart.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/6-months-dashboard-quickstart.JPG' title='6 month dashboard built in Looker on top of Snowplow data' /&gt;&lt;/a&gt;
&lt;h4 id='4_a_solid_basis_to_extend_the_model_to_encompass_your_own_businessspecific_and_productspecific_events_dimensions_and_metrics'&gt;4. A solid basis to extend the model to encompass your own business-specific and product-specific events, dimensions and metrics&lt;/h4&gt;

&lt;p&gt;The purpose of the model is to get you started using Looker on top of Snowplow. One of the best things about Looker is that the metadata model is easy to extend - we hope that you extend it to incorporate:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Events that are specific to your website / business&lt;/li&gt;

&lt;li&gt;Dimensions that are specific to your website / business (e.g. audience segments or stages in funnels)&lt;/li&gt;

&lt;li&gt;Metrics that are specific to your website / business&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='install'&gt;3. How to install the model&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you have a Looker trial setup for you by either the Looker or Snowplow teams, they should be able to install the model for you.&lt;/p&gt;

&lt;p&gt;If you are setting up Looker for yourself (or you already have Looker setup and want to incorporate the model), instructions on doing so can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/Getting-started-with-Looker'&gt;on our setup guide / wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Over time we plan to add Looker-specific recipes to our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;. Stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure"/>
    <title>Five things that make analyzing Snowplow data in Looker an absolute pleasure</title>
    <updated>2014-01-08T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Towards the end of 2013 we published our first blog post on &lt;a href='http://www.looker.com'&gt;Looker&lt;/a&gt; where we explored at a technical level why Looker is so well suited to analyzing Snowplow data. Today we &lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support'&gt;released Snowplow 0.8.13&lt;/a&gt;, the &lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support'&gt;Looker release&lt;/a&gt;. This includes a metadata model to make it easy for Snowplow users to get up and running with Looker on top of Snowplow very quickly. In this post, we get a bit less theoretical, and highlight five very tangible reasons why analyzing Snowplow data with Looker is such an absolute pleasure.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/#any-dimension-or-metric-combination'&gt;Slice and dice any combination of dimension and metrics&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/#define-your-own-metrics-and-dimensions'&gt;Quickly and easily define dimensions and metrics that are specific to your business using Looker&amp;#8217;s lightweight metadata model&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/#drill-up-and-down'&gt;Drill up and drill right down to visitor-level and event-level data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/#dashboards'&gt;Dashboards are a strating point for more involved analysis&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/08/five-things-that-make-analyzing-snowplow-data-with-looker-an-absolute-pleasure/#data-server'&gt;Access your data from &lt;em&gt;any&lt;/em&gt; application: Looker as a general purpose data server&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='any-dimension-or-metric-combination'&gt;&lt;h2&gt;1. Slice and dice any combination of dimension and metrics&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;With Looker, you can slice your Snowplow data by &lt;em&gt;any&lt;/em&gt; dimension / metric combination. To give some illustrative examples - we can plot the number of visits, bounce rate, pages per visit and events per visit by landing page:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/metrics-by-landing-page.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/metrics-by-landing-page.JPG' title='Querying metrics by landing page' /&gt;&lt;/a&gt;&lt;!--more--&gt;
&lt;p&gt;We may want to plot the number of &lt;em&gt;new&lt;/em&gt; visitors by landing page over time:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/new-visitors-by-landing-page-over-time.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/new-visitors-by-landing-page-over-time.JPG' title='Plotting the number of visits by landing page over time' /&gt;&lt;/a&gt;
&lt;p&gt;Or perhaps we want to compare the number of transactions by customers based on the channel they were &lt;em&gt;first&lt;/em&gt; acquired on (first touch referer source):&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/metrics-by-original-mkt-source.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/metrics-by-original-mkt-source.JPG' title='Number of visitor-level metrics plotted by original (first touch) marketing channel' /&gt;&lt;/a&gt;
&lt;p&gt;Creating the above slices of data is as simple as selecting the dimension / metric combination from the long list provided in the Looker UI.&lt;/p&gt;
&lt;a name='define-your-own-metrics-and-dimensions'&gt;&lt;h2&gt;2. Quickly and easily define dimensions and metrics that are specific to your business using Looker's light-weight metadata model&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Looker&amp;#8217;s metadata model makes it very easy to define and analyze busines specific:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Events&lt;/li&gt;

&lt;li&gt;Dimensions (e.g. audience segments, session classification)&lt;/li&gt;

&lt;li&gt;Funnels&lt;/li&gt;

&lt;li&gt;Metrics&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To give a very specific example: at Snowplow we are very interested in whether or not visitors to our website visit the &amp;#8216;services&amp;#8217; pages, for example, as that indicates that they are potentially interested in our Pro Services offering.&lt;/p&gt;

&lt;p&gt;We can add a dimension to our &lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/looker-analytics/lookml/events.lookml'&gt;&lt;code&gt;events.lookerml&lt;/code&gt;&lt;/a&gt; model that categorises whether a specific event has occurred on a services page or not:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;  &lt;span class='c1'&gt;# Snowplow-website specific dimension&lt;/span&gt;
  &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;dimension&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;occurred_on_services_page&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;yesno&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;sql&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;${page_urlpath} LIKE &amp;#39;%services%&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can then create a metric that counts the number of events that occur on services pages, further down the &lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/looker-analytics/lookml/events.lookml'&gt;&lt;code&gt;events.lookerml&lt;/code&gt;&lt;/a&gt; definition:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;  &lt;span class='c1'&gt;# Snowplow-website specific metric&lt;/span&gt;
  &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;measure&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;events_on_services_page_count&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;count&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;filters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
      &lt;span class='l-Scalar-Plain'&gt;occurred_on_services_page&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;yes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Both the above dimension and metric will now be available to include in any report produced in the Explorer. For example, we can now compare the number of events that occurred on services page by marketing campaign, landing page or over time.&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/events-on-services-page-by-day.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/events-on-services-page-by-day.JPG' title='Plotting the number of events on services pages by day' /&gt;&lt;/a&gt;
&lt;p&gt;We can define additional derived metrics (e.g. average events on a service page per visitor / session) or dimensions (e.g. classify visitors by whether or not they have visited the services pages at all) by simply extending the metadata model. The Looker metadata model is flexible enough to extend with your business, as you become more sophisticated in your use of data.&lt;/p&gt;
&lt;a name='drill-up-and-down'&gt;&lt;h2&gt;3. Drill-up and drill-down to visitor-level and event-level data&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;To illustrate this, let&amp;#8217;s start by comparing visit and engagement levels by refer medium for the last month (i.e. a session-level analysis):&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/visit-and-engagement-levels-by-refer-medium.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/visit-and-engagement-levels-by-refer-medium.JPG' title='Visit numbers and engagement levels by referer medium' /&gt;&lt;/a&gt;
&lt;p&gt;We can see visitors referered from other websites appear to engage more deeply, on average. We can explore that further, to see if it is true across e.g. all landing pages, by clicking on the &lt;strong&gt;Landing Page Count&lt;/strong&gt; (which is &amp;#8220;7&amp;#8221; and circled above):&lt;/p&gt;

&lt;p&gt;This opens another view, which lets us compare events per visit and bounce rates by the seven different landing pages that users refered to our websites from other websites were driven to. It looks like users refered from external websites to our recipe on market basket analysis engaged particularly deeply with our website:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/visit-and-engagement-levels-by-landing-page.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/visit-and-engagement-levels-by-landing-page.JPG' title='Visit numbers and engagement levels by landing page' /&gt;&lt;/a&gt;
&lt;p&gt;We can explore this further by clicking on the visit count to see the actual visits. For example, if we click on the &amp;#8220;17&amp;#8221; visits to the market basket analysis (circled above)&amp;#8230;&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/visits-to-market-basket-analaysis-recipe.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/visits-to-market-basket-analaysis-recipe.JPG' title='Exploring visits that land on teh market basket analysis recipe post' /&gt;&lt;/a&gt;
&lt;p&gt;&amp;#8230;we are shown an actual list of the 17 visits, including the cookie ID and the time each visitor spent on the website. (Note that all but the 3rd visitor were visiting our website for the first time).&lt;/p&gt;

&lt;p&gt;It looks like the 9th visitor on the list was on our website for a particularly long period of time - let&amp;#8217;s click on &amp;#8220;Event Stream&amp;#8221; (circled above) to find out what he / she actually did on the website:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/session-complete-event-stream.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/session-complete-event-stream.JPG' title='Drilling in to the individual event stream for a particular session' /&gt;&lt;/a&gt;
&lt;p&gt;We are now shown the &lt;em&gt;complete event stream&lt;/em&gt; for that user on that session. Incredibly, the visitor &lt;em&gt;only&lt;/em&gt; visited that recipe page (did not navigate to any other pages on our website).&lt;/p&gt;
&lt;a name='dashboards'&gt;&lt;h2&gt;4. Dashboards are a starting point for more involved analysis&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;It is straightforward in Looker to develop customized dashboards. The following is an example of one included in our &lt;a href='/blog/2014/01/08/snowplow-0.8.13-released-with-looker-support'&gt;Looker release&lt;/a&gt;:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/7-day-dashboard.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/7-day-dashboard.JPG' title='7 day dashboard built in Looker on top of Snowplow data' /&gt;&lt;/a&gt;
&lt;p&gt;Most BI tools offer great dashboarding facilities. What we like particularly about Looker&amp;#8217;s is that clicking on any of the graphs sends you straight into the Explorer, so you can then start slicing / dicing and drilling in as described in the sections above. For example, if you clicked on the data point circled above (representing the number of visits from search engines to the website on January 6th) brings up a list of all those different sessions. We can then click on the &lt;strong&gt;Event Stream&lt;/strong&gt; for any of those sessions to see what actually occurred.&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/session-drilldown.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/session-drilldown.JPG' title='Drilling down into an individual session' /&gt;&lt;/a&gt;&lt;a name='data-server'&gt;&lt;h2&gt;5. Access your data from &lt;i&gt;any&lt;/i&gt; application: Looker as a general purpose data server&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as enabling users to plot graphs directly in Looker, it is also possible to use Looker as a data server to make your data easily available to other applications to visualize.&lt;/p&gt;

&lt;p&gt;You can set Looker up to make specific slices of data available at designated URLs, in JSON, CSV or tab-delimited format, so that it can easily be ingested and refreshed from any application, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Web applications (e.g. built using &lt;a href='http://d3js.org/'&gt;D3.js&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;Analytics tools e.g. R, Python / Pandas&lt;/li&gt;

&lt;li&gt;Spreadsheets e.g. Google Docs and Excel&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Say for example the following cut of data was important to us (the number of visits and events per visitor by web page, for the last week):&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/visits-and-events-per-visitor-by-page-for-last-week.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/visits-and-events-per-visitor-by-page-for-last-week.JPG' title='Visits and events per visitor by page for last week' /&gt;&lt;/a&gt;
&lt;p&gt;We can use Looker to publish the data to a URL. We&amp;#8217;ve published the above view to the following URLs - check them out in your browser to see how easy it is to fetch the data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://snowplowanalytics.looker.com/looks/xyY6yZTg23RzrYpT2y9y4t5tggSMq7xz.txt'&gt;Tab-delimited data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://snowplowanalytics.looker.com/looks/xyY6yZTg23RzrYpT2y9y4t5tggSMq7xz.csv'&gt;CSV data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://snowplowanalytics.looker.com/looks/xyY6yZTg23RzrYpT2y9y4t5tggSMq7xz.json'&gt;JSON data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Open the data in Google Spreadsheets using &lt;code&gt;=ImportData(&amp;quot;https://snowplowanalytics.looker.com/looks/xyY6yZTg23RzrYpT2y9y4t5tggSMq7xz.csv&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see how the data looks in Google Spreadsheets below:&lt;/p&gt;
&lt;a href='/static/img/blog/2014/01/looker/google-spreadsheet.JPG'&gt;&lt;img src='/static/img/blog/2014/01/looker/google-spreadsheet.JPG' title='Snowplow data served live into Google Spreadsheet by Looker' /&gt;&lt;/a&gt;
&lt;p&gt;Because the data is being served live, it is always up-to-date. Pretty cool, huh?&lt;/p&gt;

&lt;h2 id='want_to_get_started_with_looker'&gt;Want to get started with Looker?&lt;/h2&gt;

&lt;p&gt;Then get in touch with the &lt;a href='http://looker.com/free-trial'&gt;team at Looker&lt;/a&gt; or the &lt;a href='http://snowplowanalytics.com/about/index.html'&gt;team at Snowplow&lt;/a&gt; to arrange a trial.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements"/>
    <title>Snowplow 0.8.12 released with a variety of improvements to the Scalding Enrichment process</title>
    <updated>2014-01-07T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the immediate availability of &lt;a href='https://github.com/snowplow/snowplow/releases/tag/0.8.12'&gt;Snowplow 0.8.12&lt;/a&gt;. We have quite a packed schedule of releases planned over the next few weeks - and we are kicking off with 0.8.12, which consists of various small improvements to our Scalding-based Enrichment process, plus some architectural re-work to prepare for the coming releases (in particular, &lt;a href='http://aws.amazon.com/kinesis/'&gt;Amazon Kinesis&lt;/a&gt; support).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements/#background'&gt;Background on this release&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements/#improvements'&gt;Scalding Enrichment improvements&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements/#rearchitecting'&gt;Re-architecting our Enrichment process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2014/01/07/snowplow-0.8.12-released-with-scalding-enrichment-improvements/#install'&gt;Installing this release&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='background'&gt;&lt;h2&gt;1. Background on this release&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;This release has two core objectives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;To make a set of small improvemnts to our Scalding-based Enrichment process&lt;/li&gt;

&lt;li&gt;To re-architect our Enrichment process to make it usable from Amazon Kinesis&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will detail both of these after the jump.&lt;/p&gt;
&lt;!--more--&gt;&lt;a name='improvements'&gt;&lt;h2&gt;2. Scalding Enrichment improvements&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The improvements made to our Scalding Enrichment process in this release are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have updated our user-agent parsing library to its latest version, thanks to Rob Kingston for the suggestion (&lt;a href='https://github.com/snowplow/snowplow/issues/416'&gt;#416&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;We have fixed an issue where Snowplow raw events without a page URI were automatically sent to the bad bucket. As a general purpose event analytics platform, raw events are no longer automatically expected to have an associated page URI. Thanks to Simon Rumble for this suggestion (&lt;a href='https://github.com/snowplow/snowplow/issues/399'&gt;#399&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;We have added missing validation for fields which the Tracker Protocol expects to be set to &amp;#8216;0&amp;#8217; or &amp;#8216;1&amp;#8217; (&lt;a href='https://github.com/snowplow/snowplow/issues/408'&gt;#408&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;We have added missing validation of the numeric fields in ecommerce transactions (&lt;a href='https://github.com/snowplow/snowplow/issues/400'&gt;#400&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;We have tweaked the code which parses CloudFront access logs to make it a little more permissive of missing fields if they are not required by Snowplow (&lt;a href='https://github.com/snowplow/snowplow/issues/410'&gt;#410&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally we have also upgraded some of the Enrichment process&amp;#8217;s underlying components: Scala to 2.10.3, Scalding to 0.8.11, SBT to 0.13.0 and sbt-assembly to 0.10.0.&lt;/p&gt;

&lt;p&gt;Finally, although not exactly an Enrichment improvement, we have now fixed a bug in &lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/cubes/cube-pages.sql'&gt;&lt;code&gt;cube-pages.sql&lt;/code&gt;&lt;/a&gt;, bumping this to 0.1.1. Many thanks to community member Matt Walker for this fix!&lt;/p&gt;
&lt;a name='rearchitecting'&gt;&lt;h3&gt;3. Re-architecting our Enrichment process&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Here at Snowplow we are hugely excited about the recent release of &lt;a href='http://aws.amazon.com/kinesis/'&gt;Amazon Kinesis&lt;/a&gt;, a fully managed service for continuous data processing. We plan to use Kinesis to enable real-time collection and processing of Snowplow event data: as well as enabling us to deliver real-time reporting via Amazon Redshift, this also opens up the possibility of building operational systems on top of the Snowplow event stream.&lt;/p&gt;

&lt;p&gt;As a first step, Brandon, one of our Snowplow winterns, is working on a new Snowplow collector which will collect raw Snowplow events and sink them onto a further Kinesis stream; for more details on this collector see &lt;a href='http://snowplowanalytics.com/blog/2013/12/20/introducing-our-snowplow-winterns/'&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next logical step is to create a &amp;#8220;Kinesis application&amp;#8221;, which reads raw events off one Kinesis stream, enriches them using our existing Scala Enrichment code, and then writes the enriched events back to another Kinesis stream for further processing or storage (e.g. drip feeding into Amazon Redshift).&lt;/p&gt;

&lt;p&gt;The only problem? Our existing Scala Enrichment code was tightly coupled to our Scalding/Cascading Scala project - making it hard to re-use it in a future Kinesis application. This 0.8.12 release fixes this with some &amp;#8216;software surgery&amp;#8217;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='rearchitect-img' src='/static/img/blog/2014/01/common-enrich-rearchitect.png' /&gt;&lt;/p&gt;

&lt;p&gt;Thus the new &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-common-enrich'&gt;Scala Common Enrich&lt;/a&gt; is a shared library for processing raw Snowplow events into validated and enriched Snowplow events. Common Enrich is designed to be used within a &amp;#8220;host&amp;#8221; enrichment process: initially our existing &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-hadoop-enrich'&gt;Scala Hadoop Enrich&lt;/a&gt; process, but it should be relatively straightforward to also embed this in a Kinesis application.&lt;/p&gt;

&lt;p&gt;If you are using the existing Scalding-based Enrichment process, the only difference you should notice is the new composite &lt;code&gt;v_etl&lt;/code&gt; for Snowplow events: &amp;#8220;hadoop-0.3.6-common-0.1.0&amp;#8221;.&lt;/p&gt;
&lt;a name='install'&gt;&lt;h3&gt;4. Installing this release&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Assuming you are using EmrEtlRunner, you simply need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest version of the Hadoop ETL:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.6&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that&amp;#8217;s it! As always, you can find more detail on the tickets in this release under the &lt;a href='https://github.com/snowplow/snowplow/releases/tag/0.8.12'&gt;Snowplow v0.8.12&lt;/a&gt; release in GitHub.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/12/20/introducing-our-snowplow-winterns</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/12/20/introducing-our-snowplow-winterns"/>
    <title>Introducing our Snowplow winterns</title>
    <updated>2013-12-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Just over two months ago we announced our winter internship program for open source hackers, &lt;a href='/blog/2013/10/07/announcing-out-winter-open-source-internship-program/'&gt;here on this blog&lt;/a&gt;. We had no idea what kind of response we would receive - it was our first attempt at designing an internship program, and we had never heard of a startup (even an open source company like ours) recruiting &lt;em&gt;remote&lt;/em&gt; interns. As it turned out, we were delighted by the response we received, and we decided to make offers to &lt;strong&gt;three&lt;/strong&gt; very talented &amp;#8220;winterns&amp;#8221; from around the world, rather than the one-to-two originally planned.&lt;/p&gt;

&lt;p&gt;This week saw all three of our winterns working hard on their respective Snowplow projects, and so we wanted to take this opportunity to introduce each of them to the community, as well as giving a little more background on the projects they are working on:&lt;/p&gt;

&lt;p&gt;&lt;img alt='winterns-montage' src='/static/img/blog/2013/12/winterns.png' /&gt;&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='brandon_amos_a_new_streambased_scala_collector_for_snowplow'&gt;Brandon Amos: a new stream-based Scala collector for Snowplow&lt;/h2&gt;

&lt;p&gt;Brandon Amos is one of our two remote winterns.&lt;/p&gt;

&lt;p&gt;Brandon is based on the East Coast of the US and is a third-year CS undergraduate at Virginia Tech. Outside of computer science, Brandon is an indie guitarist, classical pianist, and symphonic trumpeter. During the winternship, Brandon aims to learn more about Scala and associated libraries, such as Akka and Spray, so he can use them in his own work.&lt;/p&gt;

&lt;p&gt;Brandon is working on a new stream-based event collector for Snowplow, written in Scala. The original plan was for this new collector to collect events from Snowplow trackers over HTTP and then emit them onto a &lt;a href='https://kafka.apache.org/'&gt;Kafka&lt;/a&gt; queue. However, prior to Brandon starting, we were given early access to &lt;a href='http://aws.amazon.com/kinesis/'&gt;Amazon Kinesis&lt;/a&gt;, the new fully-managed stream processing service from AWS; we decided it makes more sense to focus on Kinesis as the first target for our new Scala collector.&lt;/p&gt;

&lt;p&gt;Brandon&amp;#8217;s new Scala collector is being built on top of &lt;a href='http://spray.io/'&gt;Spray&lt;/a&gt; (aka akka-http), a Scala/Akka toolkit for building REST/HTTP-based integration layers. As well as building the new collector, Brandon is working on a new &lt;a href='http://diwakergupta.github.io/thrift-missing-guide/'&gt;Thrift&lt;/a&gt;-based schema to store all raw Snowplow events in.&lt;/p&gt;

&lt;p&gt;You can follow Brandon&amp;#8217;s progress in the &lt;a href='https://github.com/snowplow/snowplow/tree/feature/scala-rt-coll'&gt;&lt;code&gt;feature/scala-rt-collector&lt;/code&gt;&lt;/a&gt; branch of the main Snowplow repository. Expect to hear a lot more from us soon about how we will be working with Amazon Kinesis at Snowplow - we are super-excited about the service!&lt;/p&gt;

&lt;h2 id='jiawen_zhou_a_new_forex_library_for_scala'&gt;Jiawen Zhou: a new forex library for Scala&lt;/h2&gt;

&lt;p&gt;Jiawen Zhou is working with us here at Snowplow HQ.&lt;/p&gt;

&lt;p&gt;Jiawen is a second-year MEng undergrad at Imperial in London; she likes basketball and tells us &lt;em&gt;&amp;#8220;I joined Snowplow because I wanted a chance to program in new languages which I am interested in. I hope I can learn from here and contribute at the end of my winternship.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Jiawen is working on a new high-performance Scala library for exchange rate lookups and currency conversions. The library will let users work with foreign exchange in an expressive high-level DSL - we are working with Finance professionals to get the details of this library right. Under the covers the library makes use of the excellent &lt;a href='https://openexchangerates.org/'&gt;Open Exchange Rates&lt;/a&gt; API, and leverages both &lt;a href='http://www.joda.org/joda-money/'&gt;Joda-Money&lt;/a&gt; and &lt;a href='http://www.joda.org/joda-time/'&gt;Joda-Time&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The ultimate plan is to integrate this library into the Snowplow Enrichment process, to allow financial transactions (e.g. ecommerce tracking) to be converted into a base currency for consistent reporting.&lt;/p&gt;

&lt;p&gt;You can follow Jiawen&amp;#8217;s progress in the new repository, &lt;a href='https://github.com/snowplow/scala-forex'&gt;&lt;code&gt;scala-forex&lt;/code&gt;&lt;/a&gt; (foreign exchange).&lt;/p&gt;

&lt;h2 id='anuj_more_a_new_python_tracker_for_snowplow'&gt;Anuj More: a new Python Tracker for Snowplow&lt;/h2&gt;

&lt;p&gt;Anuj More is the second of our remote winterns.&lt;/p&gt;

&lt;p&gt;Anuj is a recent IT graduate of Mumbai University, and is still based in the city. Anuj is ambidextrous, a huge &amp;#8220;A Bit of Fry and Laurie&amp;#8221; fan, and a Beatles lover. Through this winternship, Anuj is keen to find out how FOSS startups work, including the challenges on the business side of things. He also notes that &lt;em&gt;&amp;#8220;this will be my first professionally written Python project; so excited about that too.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Anuj is working on a Python event tracker for Snowplow. We are hugely excited about the potential for generating Snowplow events from Python - be it from Django/Flask/Bottle web applications, desktop applications, games or anything else. Anuj is taking our experiences from writing the previous trackers, implementing the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow Tracker Protocol&lt;/a&gt; and building out a nicely Pythonic API.&lt;/p&gt;

&lt;p&gt;The Python Tracker is built for Python 3, and makes use of the excellent &lt;a href='http://docs.python-requests.org/en/latest/'&gt;Requests&lt;/a&gt; library for HTTP GETs, and &lt;a href='http://andreacensi.github.io/contracts/'&gt;PyContracts&lt;/a&gt; for validating that the tracker is being implemented in client code correctly.&lt;/p&gt;

&lt;p&gt;Anuj&amp;#8217;s &lt;code&gt;snowplow-python-tracker&lt;/code&gt;, our fifth Snowplow tracker, will be open-sourced in early January.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data"/>
    <title>Introducing Looker - a fresh approach to Business Intelligence that works beautifully with Snowplow</title>
    <updated>2013-12-10T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;img alt='Looker-screenshot' src='/static/img/blog/2013/12/looker-screenshot.png' /&gt;&lt;/p&gt;

&lt;p&gt;In the last few weeks, we have been experimenting with using &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; as a front-end to analsye Snowplow data. We&amp;#8217;ve really liked what we&amp;#8217;ve seen: Looker works beautifully with Snowplow. Over the next few weeks, we&amp;#8217;ll share example analyses and visualizations of Snowplow data in Looker, and dive into Looker in more detail. In this post, we&amp;#8217;ll take a step back and walk through some context to explain why we are so excited about Looker.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data/#limitations'&gt;Understanding the limitations that arise when you use traditional Business Intelligence (BI) tools (like Tableau) to analyse Snowplow data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/12/10/introducing-looker-a-fresh-approach-to-bi-on-snowplow-data/#enter-looker'&gt;How Looker takes a fresh approach to BI, and why that overcomes the limitations Snowplow users have working with other solutions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='limitations'&gt;&lt;h2&gt;The limitations that arise when you use traditional Business Intelligence (BI) tools to analyse Snowplow data&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Our vision for Snowplow is to enable companies to capture and store granular, event-level data from their websites and applications, so that they can perform any type of analysis on that data, including joining that event data with other data sets.&lt;/p&gt;

&lt;p&gt;The majority of companies using Snowplow access their data from Amazon Redshift. Redshift is great: it makes it possible to run performant queries against massive data sets - and Snowplow data sets are often very large. It also makes it possible to plug-in one or more business intelligence tools to query and visualize the data, via its Postgres API. Perhaps not surprisingly, one of the quesitons we get asked most is what BI and reporting solutions to use to mine Snowplow data and deliver dashboards powered by that data. We have worked with a number clients to implement BI tools on top of Snowplow data, more often than not &lt;a href='http://tableausoftware.com'&gt;Tableau&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;BI tools are great for a host of reasons. By providing graphical user interfaces on top of the data, they make it possible for people to query the data without knowledge of SQL. They also make it possible to visualize the data, making it easier to identify underlying trends and drive value from the data.&lt;/p&gt;

&lt;p&gt;For companies running Snowplow, the benefits that a BI tool provide come at a cost: it is not possible to query the data as flexibily through the BI tool as it is executing SQL directly against the data in Redshift. To understand why, it helps to think about the architecture underlying traditional Business Intelligence tools:&lt;/p&gt;

&lt;p&gt;&lt;img alt='Diagram of BI tool architecture' src='/static/img/blog/2013/12/simplified-bi-architecture.png' /&gt;&lt;/p&gt;

&lt;p&gt;BI tools typically load data sets into their own internal analytics engine. The tool will then infer a collection of dimensions and metrics from that data, based on the underlying structure of the database it is connected to. Users of the BI tool can then go on to define additional dimensions and metrics on top of that data: for example by defining the metric &amp;#8216;uniques&amp;#8217; as a count of the number of distinct &amp;#8216;domain_userid&amp;#8217; values. It is then possible to use the complete set of dimensions and metrics to pivot the data (slice and dice differnet combinations of metrics and dimensions together) and visualize the different slices of data.&lt;/p&gt;

&lt;p&gt;There are two problems with using the above approach with Snowplow data in Amazon Redshift, however:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#volume'&gt;The volume of data in Redshift is simply too large to load into the BI tool&amp;#8217;s own data processing engine. (Which is generally optimized for in-memory computation.)&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#mapping'&gt;Deriving dimensions and metrics from the underlying Snowplow data is not trivial&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='volume'&gt;&lt;h3&gt;1.1 There is too much data in Redshift to load into the BI tool's own analytics engine&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;BI tools like Tableau have very fast in-memory analytics engines. This is important because when you are slicing and dicing different combinations of dimensions and metrics, you do not want to have to wait tens of minutes for the table or graph to update. Unfortunately, this approach does not work well with Snowplow data in Redshift, because the volume of underlying event-level data is too great to load in-memory.&lt;/p&gt;

&lt;p&gt;The primary workaround with Snowplow data is to reduce the volume of data loaded into the BI tool, by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Aggregating the data to a session or visitor level, prior to loading it into the BI tool. (Rather than keep it at the underlying event-level.) This effectively reduces the number of lines of data loaded in.)&lt;/li&gt;

&lt;li&gt;Limit the number of dimensions / metrics loaded in. (Effectivley limiting the number of columns loaded in.)&lt;/li&gt;

&lt;li&gt;Limiting the range of data loaded into the BI tool e.g. by time period, or only load data for specific user-segments or cohorts. (This also limits the number of lines of data loaded in.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The problem with all of the above approaches is that they limit the flexibility that the end-user of the BI tool has to query the data . Aggregating the data to a visit / session level prevents the user drilling in to specific visitors or sessions identified as part of the analysis. Limiting the number of columns loaded limits the number of dimensions and metrics that an end user can slice and dice the data by. And limiting the data loaded in by time period or user segment limits the number of segments or time periods that an analyst can explore data for.&lt;/p&gt;
&lt;a name='mapping'&gt;&lt;h3&gt;1.2 Deriving dimensions and metrics from the underlying Snowplow data is not trivial&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;The other feature of Snowplow data that BI tools typically struggle with is inferring many of the most useful dimensions and metrics from the underlying event data.&lt;/p&gt;

&lt;p&gt;Some dimensions and metrics are relatively straightforward to define for Snowplow data. For example, if we want to uniquely identify a session, we can do so by concatenating the &amp;#8216;domain_userid&amp;#8217; and &amp;#8216;domain_sessionidx&amp;#8217;. In Tableau, we can create a session ID dimension, by selecting &amp;#8216;Create Metric / Dimension&amp;#8217; and entering the following formula:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[domain_userid] + &amp;#39;-&amp;#39; +  STR([domain_sessionidx])&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, it is straightforward to define a &amp;#8216;Uniques&amp;#8217; metric as a count distinct on &amp;#8216;domain_userid&amp;#8217; values. Again, we select the &amp;#8216;Create Metric / Dimension&amp;#8217; option, and then enter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;COUNTD( [domain_userid] )&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The trouble is that there is a raft of other dimensions that are much harder to define: for example, what was the landing page of this particular session? That involves identifying the first event for a particular session (which is generally a page view), reading the different page parameters (&amp;#8216;page_urlhost&amp;#8217;, &amp;#8216;page_urlpath&amp;#8217;) for that event, and then blitting that value across all the other lines of data for that session. There are other dimensions that derive from the first event in a session, for example:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The page referer (including referer type)&lt;/li&gt;

&lt;li&gt;Any marketing campaign attributes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In all these cases, it is difficult to define the logic for identifying these dimensions in traditional BI tools. (It is impossible in Tableau.) There are other dimensions that relate to the visitor (rather than the session):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What date they first visited the website&lt;/li&gt;

&lt;li&gt;What visit they &amp;#8216;signed up&amp;#8217; (if relevant)&lt;/li&gt;

&lt;li&gt;Where that user was acquired from? (What referer? What marketing channel?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To fetch these dimension values, we need to identify the first event for that user across all their different user sessions. Again, this is very difficult within the BI tool, because there is no way to explain to the BI tool how to identify specific lines in the user&amp;#8217;s event stream, read dimension values from that event and then apply them to all the other events in the same session.&lt;/p&gt;

&lt;p&gt;Further, there are other dimensions that relate to the &lt;em&gt;last&lt;/em&gt; event in a user&amp;#8217;s session: namely, their exit page. The same difficulty arises.&lt;/p&gt;

&lt;p&gt;Finally, there are dimensions that relate to more than one line of data from a user&amp;#8217;s event stream: for example, session duration is the difference between the timestamp on the first and last event for a particular session. Again, the same difficulty arises.&lt;/p&gt;

&lt;p&gt;In all the above examples, it is &lt;strong&gt;not&lt;/strong&gt; possible to derive the above fields in Tableau directly on the data, because we have no way in Tableau of defining dimensions related to specific lines in a data set. As a result, companies that use Tableau or other standard BI tools on top of Snowplow have to compute them using SQL &lt;em&gt;before&lt;/em&gt; loading the data into Tableau. That is effectively what we have done in the different cubes that we created in the 0.8.12 release. You can see the underlying SQL &lt;a href='https://github.com/snowplow/snowplow/tree/master/5-analytics/redshift-analytics/cubes'&gt;here&lt;/a&gt;. We&amp;#8217;ve produced a guide to generating OLAP-compatible data from the &amp;#8216;atomic.events&amp;#8217; data in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;Analytics Cookbook&lt;/a&gt;. It is not trivial.&lt;/p&gt;

&lt;p&gt;As a result, companies that have implemented e.g. Tableau on top of Snowplow, typically have one or more members of a data team, internally, that are experts at using the SQL to generate different slices of the data that are suitable for loading into Tableau. Consumers of the data who are not versed in SQL query the data using Tableau (either through dashboards powered by Tableau Server or directly via Tableau desktop). If they find that they cannot generate the particular analysis they want out of the different cubes available, they can then ask a member of the data team to generate them a new cube from the event-level data. It works, but it is not ideal.&lt;/p&gt;

&lt;h2 id='enter_looker_a_fresh_approach_to_bi_that_is_much_better_suited_to_analysing_snowplow_data'&gt;Enter Looker: a fresh approach to BI, that is much better suited to analysing Snowplow data&lt;/h2&gt;

&lt;p&gt;&lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; is architected differently to traditional BI tools. There are a number of features that make Looker unique, but two are especially pertinent for users who want to use a BI tool to analyse their Snowplow data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#processing-engine'&gt;Looker does not load data into its own data processing engine. Instead, it uses the underlying database to perform the computations.&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#meta-data-layer'&gt;Looker has a lightweight meta-data model that makes it easy to derive a comprehensive set of dimensions and metrics on top of the underlying data.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='processing-engine'&gt;&lt;h3&gt;1. Looker uses the underlying database to crunch the data&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;If you have your data in an database like Amazon Redshift that is optmized for running analytics queries across large data sets, it makes to run your queries on the data in Amazon Redshift. Looker does that: every time you slice / dice combinations of metrics and dimensions in the Looker Explorer, plot a graph or draw a dashboard, Looker translates the actions in the user interface into SQL and runs that SQL on Redshift. Rather then spend time developing their own in-memory data processing architecture, the Looker team have concentrated instead on generating highly performant SQL. As a result, when you&amp;#8217;re exploring Snowplow data in Looker, you are exploring the complete data set.&lt;/p&gt;
&lt;a name='meta-data-layer'&gt;&lt;h3&gt;2. Looker has a light-weight meta-data model, that makes it easy to derive dimensions and metrics from the underlying Snowplow data&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Looker boasts a very expressive metadata model. You can create a model that understands different entities - for example: visitors, sessions and events. Events can be derived directly from the &amp;#8216;atomic.events&amp;#8217; table. In contrast, sessions and visitors are derived from aggregations on that data. The model is rich enough that you can express links between different entities: visitor A has visited the website on three separate occasions: Looker will let you drill into each of those three sessions and view the underlying event stream for each of them. (You can have as many entities as you like in your model: we typically include geographic data, referers, devices, browsers and event types in the models we&amp;#8217;ve built using Looker.)&lt;/p&gt;

&lt;p&gt;The &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; data model is not only expressive, but it&amp;#8217;s very lightweight: it consists simply of YAML definitions of dimensions, metrics and views on the data. That makes it easy to quickly put together rich models. It also makes it easy to extend the model to incorporate data from other sources: making it straightforward to use &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; to query Snowplow data joined with other data sets, including transactional data, customer data (e.g. from CRM systems) and ad server / marketing data, for example. The combination of computing on the data directly in Redshift and enabling users to define rich metadata models means &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; is an especially powerful analytics tool for exploring and dashboarding Snowplow data in Amazon Redshift. In the coming weeks, we plan to publish some of the models we&amp;#8217;ve built for Snowplow data in &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; to make it easy for Snowplow users who want to experiment with &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; get off to a flying start. This is the first in a series of blog posts on analysing Snowplow data with &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt;: in future posts we will drill into the &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; meta-data model in particular in more detail. Get in touch with either us or the Looker team, if you&amp;#8217;d like to trial &lt;a href='http://looker.com'&gt;Looker&lt;/a&gt; on top of your Snowplow data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/12/04/snowplow-at-the-graduate-data-science-initiative</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/12/04/snowplow-at-the-graduate-data-science-initiative"/>
    <title>The first Graduate Data Science Initiative event in London</title>
    <updated>2013-12-04T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night, I was very privilage to speak at the first meeting of the &lt;a href='http://www.meetup.com/Graduate-Data-Science-Initiative/'&gt;Graduate Data Science Initiative meetup&lt;/a&gt;, alongside Martin Goodson from Qubit and Eddie Bells from Lyst.&lt;/p&gt;

&lt;p&gt;The event was for graduates interested in careers in Data Science careers. It&amp;#8217;s a great initiative and we were very happy to support it.&lt;/p&gt;

&lt;p&gt;I gave the first talk on how data scientists and big data technolgies, including Snowplow, are fundamentally changing the way that people think about use web analytics data specifically, and digital event data more generally. I went on to describe why we believe event data is such an interesting data set to work with. We&amp;#8217;ll see if any of the participants are more excited about digital event data as a result. I&amp;#8217;ve embedded my slides below:&lt;/p&gt;
&lt;div id='pres'&gt;&lt;iframe frameborder='0' height='400' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/28874690' width='476'&gt; &lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;Eddie Bells gave a very interesting talk about the data infrastructure at &lt;a href='http://www.lyst.com/'&gt;Lyst&lt;/a&gt;, particularly around scraping and structuring (including classifying) data.&lt;/p&gt;

&lt;p&gt;Martin Goodson followed up with a fascinating talk highlighting the enormous amount of money wasted in online retail because of a lack of sound statistical approach to techniques like A/B and multivariant testing. I hope to post links to both Martin&amp;#8217;s and Eddie&amp;#8217;s slides in due course.&lt;/p&gt;

&lt;p&gt;Many thanks to Fatos and Amir for organising the event. We hope it continues to flourish, and look forward to continuing to support this and similar initiatives to bring more talented graduates into data science.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/11/20/loading-json-data-into-redshift</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/11/20/loading-json-data-into-redshift"/>
    <title>Loading JSON data into Redshift - the challenges of quering JSON data, and how Snowplow can be used to meet those challenges</title>
    <updated>2013-11-20T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Very many of our Professional Services projects involve forking the Snowplow codebase so that specific clients can use it to load their event data, stored as JSONs, into Amazon Redshift, so that they can use BI tools to create dashboards and mine that data.&lt;/p&gt;

&lt;p&gt;&lt;img alt='container-ship-image' src='/static/img/blog/2013/11/container-ship.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve been surprised quite how many companies have gone down the road of using JSONs to store their event data. In this blog post, we look at:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='/blog/2013/11/20/loading-json-data-into-redshift/#why'&gt;Why logging event data as JSONs has become so popular&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/11/20/loading-json-data-into-redshift/#weaknesses'&gt;The limitations of this approach&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/11/20/loading-json-data-into-redshift/#solution'&gt;Using the Snowplow tech stack to load JSON data into Redshift&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='why'&gt;Why has logging event data as JSONs has become so popular?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are four reasons logging event data as JSONs has become as popular as it is:&lt;/p&gt;

&lt;h3 id='1_easy_to_implement'&gt;1. Easy to implement&lt;/h3&gt;

&lt;p&gt;Representing an event as a JSON is an extremely simple, easy-to-understand approach. Let&amp;#8217;s look at an example, a video play event represented as a JSON:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;event_name&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;play_video&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;properties&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1384855393&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;viewer_id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;19A34Bdt1190&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;video_id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;234101234&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Another skateboarding dog&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;author_id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;asdf987023s&amp;quot;&lt;/span&gt;
    &lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The JSON is very easy to read and comprehend. Crucially for application developers, it is straightforward to compose.&lt;/p&gt;

&lt;p&gt;Note that there are many ways we could choose to represent a video play event with a JSON. We might want e.g. to use nesting, to capture a richer data set both about the video itself, and about the user who watched it:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;event_name&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;play_video&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;properties&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;timestamp&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1384855393&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;viewer&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;19A34Bdt1190&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;age&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;28&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;gender&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;male&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;member&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;
        &lt;span class='p'&gt;},&lt;/span&gt;
        &lt;span class='nt'&gt;&amp;quot;video&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;234101234&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;Another skateboarding dog&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='nt'&gt;&amp;quot;producer&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
                &lt;span class='nt'&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;asdf987023s&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='nt'&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;michaeldouglasboy&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='nt'&gt;&amp;quot;joined&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2012-09-29&amp;quot;&lt;/span&gt;
            &lt;span class='p'&gt;}&lt;/span&gt;
        &lt;span class='p'&gt;}&lt;/span&gt;
    &lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the above example, we&amp;#8217;ve used nesting to group related fields (fields related to the viewer, fields related to the video, and fields related to the producer of the video). Again, even with all the additional data, the JSON is easy for the human eye to parse, and the nesting provides a tidy way of structuring our data.&lt;/p&gt;

&lt;h3 id='2_flexible'&gt;2. Flexible&lt;/h3&gt;

&lt;p&gt;JSONs are flexible. If one day, an application developer decides she wants to add a new field to the &amp;#8220;play_video&amp;#8221; JSON, there&amp;#8217;s nothing stopping her!&lt;/p&gt;

&lt;p&gt;Typically, application developers can create new JSONs to represent new event types over time, and update the structure of JSONs for existing event types over time (adding or dropping fields), as they see fit. If the analytics system is simply logging the JSONs, there&amp;#8217;s no need to update any downstream analytics infrastructure in light of changes to the JSON schema.&lt;/p&gt;

&lt;h3 id='3_wellsupported'&gt;3. Well-supported&lt;/h3&gt;

&lt;p&gt;Lots of analytics applications store event data as JSONs, including &lt;a href='https://www.kissmetrics.com/'&gt;Kissmetrics&lt;/a&gt;, &lt;a href='https://mixpanel.com/'&gt;Mixpanel&lt;/a&gt;, &lt;a href='https://keen.io/'&gt;KeenIO&lt;/a&gt; and &lt;a href='http://www.swrve.com/'&gt;Swrve&lt;/a&gt;. At Snowplow, we&amp;#8217;re in the process of building out support for &lt;a href='https://github.com/snowplow/snowplow/wiki/Developer-FAQ#wiki-unstructtimeline'&gt;unstructured events&lt;/a&gt;, where events are represented as JSONs.&lt;/p&gt;

&lt;h3 id='4_seemingly_easy_to_analyze_using_hive_and_json_serde'&gt;4. &lt;em&gt;Seemingly&lt;/em&gt; easy to analyze using Hive and JSON Serde&lt;/h3&gt;

&lt;p&gt;JSONs, stored as flat files in S3 or HDFS &lt;em&gt;should&lt;/em&gt; be easy to analyze using Hive and the &lt;a href='https://github.com/rcongiu/Hive-JSON-Serde'&gt;JSON serde&lt;/a&gt;. (We&amp;#8217;ve blogged an example of this &lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'&gt;here&lt;/a&gt;.) You &amp;#8216;simply&amp;#8217; specify the schema as part of your Hive table definitions, implicit in the JSON. for each event type, and then you can query the data using SQL statements, as if it was a relational database. Easy peasy!&lt;/p&gt;
&lt;h2&gt;&lt;a name='weaknesses'&gt;The limitations of this approach&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Unfortunately, querying the JSON data is not as easy as it first appears:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is not always obvious, to the analyst creating the Hive table definitions, what the schema / structure for each event type should be. The analyst can visually inspect some of the data to get some idea, but it is impossible to tell:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Whether they have spotted all the possible fields that might appear. (E.g. there may be some that are sparsely populated, but very important when they are filled in.)&lt;/li&gt;

&lt;li&gt;Which fields are compulsory, and which are optional?&lt;/li&gt;

&lt;li&gt;Remember that application developers have been free to keep modifying and updating event schemas over time, with no requirement to document or sense-check any of these updates. The analyst suffers, as a consequence, as she has to work out how that schema has evolved, in order to interrogate the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;When exploding nested data into separate tables, it is can be hard to identify on what key that data should be joined to the parent table. A nice feature of JSONs is their ability to nest data, but unless we can explode that nested data out, querying it is not going to be easy.&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;It can be hard (if not impossible) for analysts and application developers to spot &amp;#8220;errors&amp;#8221; in the JSON at the point the event data is generated and captured. It is very easy for mistakes to creep in: JSONs are very fragile (a missing comma or inverted comma will break a JSON.) There&amp;#8217;s also no way to check the type of individual field.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When Amazon Redshift was launched earlier this year, many companies wanted to load their event data into Redshift, to enable faster querying than was possible in Apache Hive, and also so that they could use BI and analytics tools to create dashboards, visaulize and mine the data. Unfortunately, loading JSON data into Redshift is even harder:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Redshift tables have traditional schemas where each field has a fixed type. To make loading data into Redshift reliable, you really want to enforce the strong types on variables all the way through the data pipeline, from data collection. However, JSONs do not support strong typing or schemas.&lt;/li&gt;

&lt;li&gt;Any input line that does not conform to the Redshift schema fails to load. Many companies are then stuck between two unappealing approaches: junk data that doesn&amp;#8217;t fit the schema (which may be a significant subset of the data), or only load a very small subset of the fields that have been reliably collected across the different event types. (Thereby relaxing the requirements on input data to successfully load, but again, effectively loosing a lot of the richness in the raw JSON data set.)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='solution'&gt;Using the Snowplow tech stack to load JSON data into Redshift&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Snowplow stack can be forked so that Snowplow transforms JSON data and loads it into Redshift. We&amp;#8217;ve found this is a much better approach then building an ETL pipeline using e.g. Apache Hive and the JSON serde, because Snowplow has a lot of validation capabilities. We&amp;#8217;ll discuss this in a second - first, let&amp;#8217;s overview the process for adapting Snowplow to load your custom JSONs into Redshift:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We develop an event dictionary for the client, which catalogs all the different events in their application and the fields that are captured with each of those events.&lt;/li&gt;

&lt;li&gt;We use that event dictionary to define table definitions in Redshift where the data will be loaded. (So we have a mapping of the event dictionary to the output tables.)&lt;/li&gt;

&lt;li&gt;We work with the client to map the contents of their event JSONs to the dictionary. (So we have a mapping of the input JSONs to the event dictionary.)&lt;/li&gt;

&lt;li&gt;We then modify the Snowplow stack to unpick the JSONs (as per the JSON -&amp;gt; dictionary mapping) and write the data back to S3 in a format suitable for loading directly into Redshift (as per the dictionary -&amp;gt; Redshift table definitions mapping)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As mentioned above, the key to making this work is to use Snowplow&amp;#8217;s rich validation capabilities. We use these to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Check that the input data conforms to the schemas specified&lt;/li&gt;

&lt;li&gt;Output any data that does not conform to the schema to a &amp;#8220;bad buckeet&amp;#8221;. This means that the &amp;#8220;good data&amp;#8221; will successfully load into Redshift, while we don&amp;#8217;t lose any &amp;#8220;bad data&amp;#8221;. We can now easily spot errors as they arise (by ensuring that the ETL process is run every few hours) and deal with them immediately. It also means that the &amp;#8220;bad&amp;#8221; data can be inspected, updated, reprocessed, and then loaded into Redshift, which is much preferable to simpy dropping it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Going forwards, we plan to build out the validation capability, so that as well as simply checking if incoming JSONs adhere to the schema, Snowplow will also spot &amp;#8220;orphaned data&amp;#8221; (i.e. name / value pairs in the JSON that are not accommodated in the schema) so that the schema can be updated to incorporate this data, and we save it from being lost.&lt;/p&gt;

&lt;h2 id='interested_in_talking_to_the_snowplow_team_about_loading_your_json_data_into_redshift'&gt;Interested in talking to the Snowplow team about loading your JSON data into Redshift?&lt;/h2&gt;

&lt;p&gt;View our info on the &lt;a href='/services/pipelines.html'&gt;Professional Services pages&lt;/a&gt;, or &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; to discuss your requirements.&lt;/p&gt;

&lt;h2 id='references'&gt;References&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;re not the only people who think storing event data as JSONs is not a great. For more opinions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='http://nathanmarz.com/blog/thrift-graphs-strong-flexible-schemas-on-hadoop.html'&gt;Nathan Marz&lt;/a&gt; wrote a blog post back in 2010, highlighting their weaknesses, as part of presenting an alternative, graph-based model.&lt;/li&gt;

&lt;li&gt;The &lt;a href='http://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf'&gt;Unified Logging Infrastructure for Data Analytics at Twitter&lt;/a&gt; gives a very clear description of how this approach breaks down at scale. (See particularly section 3.1.)&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/11/19/quickstart-guide-to-using-sql-with-snowplow-data-published</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/11/19/quickstart-guide-to-using-sql-with-snowplow-data-published"/>
    <title>Quick start guide to learning SQL to query Snowplow data published</title>
    <updated>2013-11-19T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Whilst it is possible to use different BI tools to query Snowplow data with limited or no knowledge of SQL, to really get the full power of Snowplow you need to know some SQL.&lt;/p&gt;

&lt;p&gt;To help Snowplow users who are not familiar with SQL, or those who could do with a refreshing their knowledge, we&amp;#8217;ve put together a &lt;a href='/analytics/tools-and-techniques/beginners-guide-to-using-sql-to-query-snowplow-data.html'&gt;quick start guide&lt;/a&gt; on the &lt;a href='/analytics/tools-and-techniques/beginners-guide-to-using-sql-to-query-snowplow-data.html'&gt;Analytics Cookbook&lt;/a&gt;. The purpose of the guide is to get the reader up to speed with all the core SQL concepts, especially those that are relevant for slicing and dicing Snowplow data, as quickly as possible.&lt;/p&gt;

&lt;p&gt;We hope the guide is useful. Any feedback would be gratefully received. We&amp;#8217;d also love to point people at other resources for learning SQL: if there are any you can recommend, then please do add them via the comments section below.&lt;/p&gt;

&lt;p&gt;The quick start guide can be found &lt;a href='/analytics/tools-and-techniques/beginners-guide-to-using-sql-to-query-snowplow-data.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/11/11/round-up-and-thank-you-for-the-budapest-bi-conference-last-week</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/11/11/round-up-and-thank-you-for-the-budapest-bi-conference-last-week"/>
    <title>A round up of our trip to the Budapest BI Conference last week, and a thank you to the many people who made the trip so worthwhile</title>
    <updated>2013-11-11T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last week, Alex and I had the pleasure to attend the &lt;a href='http://budapestbiforum.com/'&gt;Budapest BI Forum&lt;/a&gt;. I learnt a great deal from the different people I got to meet, and got a chance to give a talk on what Snowplow is, where we&amp;#8217;re at today and how we plan to develop it going forwards.&lt;/p&gt;

&lt;p&gt;&lt;img alt='budapest' src='/static/img/blog/2013/11/budapest.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;To summarize a few of the things we learnt:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='1_the_python_toolset_for_data_analytics_is_developing_incredibly_rapidly'&gt;1. The Python toolset for data analytics is developing incredibly rapidly&lt;/h2&gt;

&lt;p&gt;We were fortunate to hear talks from three data scientists who are very active in the Python community: &lt;a href='https://twitter.com/almarklein'&gt;Almar Klein&lt;/a&gt; (who is part of the team developing &lt;a href='http://vispy.org/'&gt;VisPy&lt;/a&gt;, an advanced visualization library), Olivier Grisel (who is part of the team developing &lt;a href='http://scikit-learn.org/stable/'&gt;scikit-learn&lt;/a&gt;, a machine learning library) and &lt;a href='https://twitter.com/dyjh'&gt;Yves J. Hilpisch&lt;/a&gt; (from &lt;a href='http://www.continuum.io/'&gt;Continuum Analytics&lt;/a&gt;, who&amp;#8217;ve produced a raft of Python libraries incl. a just-in-time compiler). All three introduced us to compelling and fast-developing aspects of the Python data analytics ecosystem.&lt;/p&gt;

&lt;p&gt;Olivier&amp;#8217;s presentation can be accessed &lt;a href='https://speakerdeck.com/ogrisel/growing-randomized-trees-in-the-cloud-1'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Almar&amp;#8217;s presentation can be downloaded &lt;a href='https://github.com/vispy/static/raw/master/vispy-biforum-2013.pdf'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Yves&amp;#8217;s presentation can be browsed as a hosted iPython notebook &lt;a href='https://www.wakari.io/sharing/bundle/yves/CAE_Python_Next_Gen_Analytics'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also hope to add some Python tutorials to the &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Snowplow Analytics Cookbook&lt;/a&gt; in the future.&lt;/p&gt;

&lt;h2 id='2_bi_offerings_are_pushing_beyond_traditional_olap_into_predictive_analytics__modelling'&gt;2. BI offerings are pushing beyond traditional OLAP into predictive analytics / modelling&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;ve always used BI tools for slicing / dicing data, and R for modelling and predictive analytics. So it was interesting to learn that many of these tools are now incorporating modelling and predictive analytics capabilities via a GUI, including &lt;a href='http://rapidminer.com/'&gt;RapidMiner&lt;/a&gt;, &lt;a href='http://spotfire.tibco.com/'&gt;Spotfire&lt;/a&gt; and &lt;a href='http://www.knime.org/'&gt;Knime&lt;/a&gt; amongst others.&lt;/p&gt;

&lt;h2 id='3_media_companies_boast_some_of_the_most_sophisticated_big_data_analytics_platforms'&gt;3. Media Companies boast some of the most sophisticated big data analytics platforms&lt;/h2&gt;

&lt;p&gt;We know that in the UK, media companies including the Guardian and Channel 4 have implemented internally some super-sophisticated data pipelines and analytics engines. It was great to learn (though not surprising) that European media companies have also implemented equally sophisticated analytics infrastructure. In particular, we thoroughly enjoyed hearing about the Hadoop, R and Jython stack built at &lt;a href='http://www.sanoma.com/'&gt;Sanoma Media&lt;/a&gt; from &lt;a href='https://twitter.com/skieft'&gt;Sander Kieft&lt;/a&gt; and &lt;a href='https://twitter.com/Voogeltje'&gt;Jelmer Voogel&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Again, I hope to post a link to their slides shortly.&lt;/p&gt;

&lt;h2 id='4_the_budapest_tech_scene_is_buzzing'&gt;4. The Budapest tech scene is buzzing&lt;/h2&gt;

&lt;p&gt;Budapest is home to some very exciting, and rapidly growing tech companies. It was great to meet &lt;a href='http://budapestbiforum.com/program/innovative-bi-day/balazs-szakacs-the-bi-journey-of-ustream/'&gt;Balzs Szakcs&lt;/a&gt; from &lt;a href='http://www.ustream.tv/'&gt;Ustream&lt;/a&gt;, who presented on the development to date of the analytics stack at UStream, as well as &lt;a href='hu.linkedin.com/in/zoltanctoth/'&gt;Zoltn Csaba Tth&lt;/a&gt; from &lt;a href='http://prezi.com/'&gt;Prezi&lt;/a&gt; and of course Snowplow community member &lt;a href='https://twitter.com/rgabo'&gt;Gabor Ratky&lt;/a&gt; from &lt;a href='http://secretsaucepartners.com/'&gt;Secret Sauce Partners&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Again, I hope to post a link to Balzs&amp;#8217;s slides in due course.&lt;/p&gt;

&lt;h2 id='thank_you'&gt;Thank you&lt;/h2&gt;

&lt;p&gt;Big thanks to the many people who made the conference possible and enjoyable, especially &lt;a href='https://twitter.com/BenceArato'&gt;Bence Arat&lt;/a&gt;, who organised it and invited us to speak. We look forward to returning next year!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/28/yali-and-alex-introduce-snowplow-to-code-n</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/28/yali-and-alex-introduce-snowplow-to-code-n"/>
    <title>Our video introduction of Snowplow to code_n</title>
    <updated>2013-10-28T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We were very flattered to be invited by the team at &lt;a href='http://www.code-n.org/#about/main'&gt;code_n&lt;/a&gt; to enter their competition to identify &amp;#8220;outstanding young companies and promote their groundbreaking business models&amp;#8221;. This year&amp;#8217;s competition is focused on data, and has the motto &lt;em&gt;Driving the Data Revolution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As part of our application process, we put together a short video introducing Snowplow. You can watch the video below.&lt;/p&gt;
&lt;iframe allowfullscreen='1' frameborder='0' height='315' src='//www.youtube.com/embed/p49quiYRsgE' width='420'&gt; &lt;/iframe&gt;
&lt;p&gt;We look forward to finding out if our application has been successful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/28/call-for-data-this-winter</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/28/call-for-data-this-winter"/>
    <title>Call for data! Support us develop experimental analyses. Have us help you answer your toughest business questions.</title>
    <updated>2013-10-28T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;This winter we are recruiting &lt;a href='http://snowplowanalytics.com/blog/2013/10/07/announcing-our-winter-open-source-internship-program/'&gt;interns&lt;/a&gt; to join the Snowplow team to work on discrete projects.&lt;/p&gt;

&lt;p&gt;A number of the candidates we have interviewed have expressed an interest in working with us to develop new analytics approaches on Snowplow data. In particular, we&amp;#8217;ve had a lot of interest in piloting machine learning approaches to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Segmenting audience by behaviour&lt;/li&gt;

&lt;li&gt;Leveraging libraries for content / product recommendation (e.g. PredictionIO, Mahout, Weka)&lt;/li&gt;

&lt;li&gt;Developing and testing new approaches to attribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are very keen, however, that any of the above projects be executed on a real data set, for a real Snowplow user, to answer real business questions.&lt;/p&gt;

&lt;p&gt;&lt;img alt='call-for-data' src='/static/img/blog/2013/10/call.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;To that end, we are now making an offer to the Snowplow community: do you have a Snowplow data set and one or more business questions that you think would benefit from anyone of the above approaches? Would you be willing to &amp;#8220;share&amp;#8221; your data?&lt;/p&gt;

&lt;p&gt;We will want to blog about the approach and the results of our winterns&amp;#8217; work with the wider community, so we recognize that this may rule out a number of Snowplow users. Still, we are happy to work with whoever signs up to publish the work in a way that doesn&amp;#8217;t give away commercial secrets. And for companies that are willing to let us loose on their data, you will hopefully get some highly valuable insights at zero cost.&lt;/p&gt;

&lt;p&gt;Interested? Email us at &lt;a href='mailto:data@snowplowanalytics.com'&gt;data@snowplowanalytics.com&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/23/snowplow-team-in-budapest-to-speak-at-open-analytics-conference"/>
    <title>Join the Snowplow team in Budapest the first week of November</title>
    <updated>2013-10-23T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are thrilled to be going to Budapest this November, where I&amp;#8217;ve kindly been invited to speak at the &lt;a href='http://budapestbiforum.com/'&gt;Budapest BI Forum&lt;/a&gt; on Snowplow, on a day dedicated to &lt;a href='http://budapestbiforum.com/program/open-analytics-day/'&gt;Open Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img alt='budapest-pic' src='/static/img/blog/2013/10/budapest.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Budapest is home to a thriving community of tech-savvy companies. We are very keen to meet as many of them as possible whilst we&amp;#8217;re in Budapest - so if you&amp;#8217;re based in Budapest (or visiting for the conference), and would like to sit down with us, then &lt;a href='/about/index.html'&gt;give us a shout&lt;/a&gt;! We&amp;#8217;ll be in Budapest between November 5th and the 8th.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements"/>
    <title>Snowplow 0.8.11 released - supports all Cloudfront log file formats and host of small improvements for power users</title>
    <updated>2013-10-22T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re very pleased to announce the release of Snowplow 0.8.11. This releases includes two different sets of updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Critical update&lt;/strong&gt;: support for Amazon&amp;#8217;s new Cloudfront log file format (rolled out by Amazon during 21st October 2013)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Nice-to-have&lt;/strong&gt; additions - the most significant of which is &lt;strong&gt;IP anonymization&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;#8217;ll discuss the updates one at a time, before covering how to &lt;a href='#upgrade'&gt;upgrade to the latest version&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#critical'&gt;Critical upgrade: support for Amazon&amp;#8217;s new CloudFront log file format&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#ip'&gt;IP address anonymization&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#other'&gt;Other updates&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/22/snowplow-0.8.11-released-supports-all-cloudfront-file-formats-and-other-improvements/#upgrade'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we dive into the detail, thanks to community members &lt;a href='https://github.com/kingo55'&gt;Rob Kingson&lt;/a&gt; and &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; for contributing to this release.&lt;/p&gt;
&lt;a name='critical'&gt;&lt;h2&gt;1. Critical upgrade: support for Amazon's new CloudFront log file format&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Since August, Amazon has made a number of changes to their CloudFront log file format, the most recent of which was pushed live yesterday:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Cloudfront log file format&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Original format&lt;/td&gt;&lt;td style='text-align: left;'&gt;The original CloudFront log file format, around which Snowplow was originally developed.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;12 Sep 2012 - 17 Aug 2013 format&lt;/td&gt;&lt;td style='text-align: left;'&gt;The original format with three new fields appended.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;August 17 unnanounced change&lt;/td&gt;&lt;td style='text-align: left;'&gt;Surprise change around the URI encoding of fields. See &lt;a href='https://groups.google.com/forum/#!topic/snowplow-user/HWeSkiiXbdQ'&gt;the Google Group&lt;/a&gt; for details.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;September 14 resolution&lt;/td&gt;&lt;td style='text-align: left;'&gt;A new approach to URI encoding, different to the previous two. See &lt;a href='https://forums.aws.amazon.com/message.jspa?messageID=491582'&gt;this forum thread&lt;/a&gt; for details.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;October 21 update&lt;/td&gt;&lt;td style='text-align: left;'&gt;Amazon updated the latest log file format with three new fields. See &lt;a href='https://forums.aws.amazon.com/ann.jspa?annID=2174&amp;amp;ref_=pe_411040_33444690_11#'&gt;this post&lt;/a&gt; for details.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--more--&gt;
&lt;p&gt;The latest version of Snowplow supports &lt;em&gt;all&lt;/em&gt; the different versions of the file format listed above, including the new format that was rolled out yesterday. It is important to note that the October 21st CloudFront file format is &lt;strong&gt;not&lt;/strong&gt; supported by previous Snowplow versions: as a result, we&amp;#8217;d expect existing Snowplow users using the CloudFront collector to see a significant number of lines in their bad rows bucket in S3 with the following format:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2013-10-21\t18:49:02\tCDG51\t474\t80.10.159.93\tGET\td3dxd9302qh2q4.cloudfront.net\t/i\t200\thttp://www.mychoicepad.com/explore/\tMozilla/5.0%2520(iPad;%2520CPU%2520OS%25207_0_2%2520like%2520Mac%2520OS%2520X)%2520AppleWebKit/537.51.1%2520(KHTML,%2520like%2520Gecko)%2520Mobile/11A501\te=pp&amp;amp;page=Explore%2520MyChoicePad%2520-%2520the%2520educational%2520Makaton%2520iPad%2520app&amp;amp;;pp_mix=0&amp;amp;pp_max=67&amp;amp;pp_miy=1224&amp;amp;pp_may=1547&amp;amp;dtm=1382381340983&amp;amp;tid=031666&amp;amp;vp=980x1203&amp;amp;ds=980x2390&amp;amp;vid=1&amp;amp;duid=00ecea075c77a900&amp;amp;p=web&amp;amp;tv=js-0.11.2&amp;amp;fp=1956663502&amp;amp;aid=mychoicepadweb&amp;amp;lang=fr-fr&amp;amp;cs=UTF-8&amp;amp;tz=Europe%252FBerlin&amp;amp;refr=http%253A%252F%252Fwww.mychoicepad.com%252Fmychoicepad%252F&amp;amp;f_pdf=0&amp;amp;f_qt=1&amp;amp;f_realp=0&amp;amp;f_wma=0&amp;amp;f_dir=0&amp;amp;f_fla=0&amp;amp;f_java=0&amp;amp;f_gears=0&amp;amp;f_ag=0&amp;amp;res=768x1024&amp;amp;cd=32&amp;amp;cookie=1&amp;amp;url=http%253A%252F%252Fwww.mychoicepad.com%252Fexplore%252F\t-\tHit\tMBBAElwDr_1_43BnXPZxQwl_PWv0x90I2uu2qXzYOxV5HSsFUAgXaw==\td3dxd9302qh2q4.cloudfront.net\thttp\t823&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;Line does not match CloudFront header or data row formats&amp;quot;&lt;/span&gt;
    &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you have &lt;a href='#upgrade'&gt;upgraded your Snowplow installation to the latest version&lt;/a&gt;, you will need to reprocess those bad rows. Instructions on how to do so are given &lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'&gt;in this blog post&lt;/a&gt;.&lt;/p&gt;
&lt;a name='ip'&gt;&lt;h2&gt;2. IP address anonymization&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as the critical update, there are a number of &lt;em&gt;nice-to-have&lt;/em&gt; features bundled in this release. Chief amongst them is IP anonymization. The enrichment process can now be configured to mask IP addresses, so that privacy-conscious Snowplow users can prevent IP addresses being visible to analysts.&lt;/p&gt;

&lt;p&gt;Snowplow administrators can setup IP masking via the EmrEtlRunner config file. Instructions on how to do this can be found in the &lt;a href='#upgrade'&gt;section on upgrading&lt;/a&gt; below.&lt;/p&gt;
&lt;a name='other'&gt;&lt;h2&gt;3. Other updates&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Under the hood, there are a large number of updates we&amp;#8217;ve made to make Snowplow more robust, performant and support developments on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product%20roadmap'&gt;product roadmap&lt;/a&gt;. You can view the &lt;a href='https://github.com/snowplow/snowplow/blob/feature/improve-etl/CHANGELOG'&gt;complete changelog here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The most important of these updates is an update to the StorageLoader to make loading into PostgreSQL more robust, by fixing an issue where Postgres was accidentally escaping tabs in the file format, breaking the load. Many thanks to community member &lt;a href='https://github.com/kingo55'&gt;Rob Kingston&lt;/a&gt; for contributing this update.&lt;/p&gt;

&lt;p&gt;There are also some additional command-line options for our two Ruby apps which should make the Snowplow Enrichment process more flexible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Run EmrEtlRunner with &lt;code&gt;--debug&lt;/code&gt; to make Elastic MapReduce&amp;#8217;s job debugging available&lt;/li&gt;

&lt;li&gt;Run StorageLoader with &lt;code&gt;--include vacuum&lt;/code&gt; if you want to include a &lt;code&gt;VACUUM&lt;/code&gt; step after your table load&lt;/li&gt;

&lt;li&gt;Run StorageLoader with &lt;code&gt;--skip analyze&lt;/code&gt; if you &lt;strong&gt;don&amp;#8217;t&lt;/strong&gt; need to run a table &lt;code&gt;ANALYZE&lt;/code&gt; step after your table load&lt;/li&gt;

&lt;li&gt;Run StorageLoader with &lt;code&gt;--include compupdate&lt;/code&gt; if you want to (re-)generate the compression encodings on your table&amp;#8217;s fields. This setting uses the new &lt;code&gt;:comprows:&lt;/code&gt; parameter in the &lt;code&gt;config.yml&lt;/code&gt; file - see &lt;a href='#storage-loader'&gt;section 4.2&lt;/a&gt; below for details&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, there are a set of &amp;#8220;under the hood&amp;#8221; stability and performance and improvements in this release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For the definitive list of updates in this release, please see the &lt;a href='https://github.com/snowplow/snowplow/releases/tag/0.8.11'&gt;v0.8.11 Release Notes&lt;/a&gt; on GitHub.&lt;/strong&gt;&lt;/p&gt;
&lt;a name='upgrade'&gt;&lt;h2&gt;4. Upgrading&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Upgrading is a three step process:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;1 &lt;a href='#emr-etl-runner'&gt;Update EmrEtlRunner&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;2 &lt;a href='#storage-loader'&gt;Update StorageLoader&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;3 &lt;a href='#reprocess'&gt;Reprocess any &amp;#8220;bad rows&amp;#8221;&lt;/a&gt; collected from before the upgrade was completed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;#8217;s take these in term:&lt;/p&gt;
&lt;a name='emr-etl-runner'&gt;&lt;h3&gt;4.1 Update the EmrEtlRunner&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;You need to update EmrEtlRunner to the latest code (0.8.11 release) on Github:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;git clone git://github.com/snowplow/snowplow.git
&lt;span class='nv'&gt;$ &lt;/span&gt;git checkout 0.8.11
&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd &lt;/span&gt;snowplow/3-enrich/emr-etl-runner
&lt;span class='nv'&gt;$ &lt;/span&gt;bundle install --deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You also need to update the &lt;code&gt;config.yml&lt;/code&gt; file for EmrEtlRunner to use the latest version of the Hadoop ETL (0.3.5):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In addition, you need to add a new &amp;#8220;enrichments&amp;#8221; section in the &lt;code&gt;config.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:enrichments&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:anon_ip&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:enabled&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;false&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:anon_octets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;1&lt;/span&gt; &lt;span class='c1'&gt;# Or 2, 3 or 4. 0 is same as enabled: false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To enable IP enrichment, you need to set &lt;code&gt;anon_ip.enabled&lt;/code&gt; to true, and specify the level of anonymization with &lt;code&gt;anon_ip.anon_octets&lt;/code&gt; field. If, for example, my IP address is &amp;#8216;37.157.33.178&amp;#8217;, then setting it to different values between 0 and 4 would anonymize my IP address as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;code&gt;anon_ip.anon_octets&lt;/code&gt; value&lt;/th&gt;&lt;th&gt;IP address displayed in Snowplow&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;0&lt;/td&gt;&lt;td style='text-align: left;'&gt;37.157.33.178&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;1&lt;/td&gt;&lt;td style='text-align: left;'&gt;37.157.33.x&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2&lt;/td&gt;&lt;td style='text-align: left;'&gt;37.157.x.x&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;3&lt;/td&gt;&lt;td style='text-align: left;'&gt;37.x.x.x&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;4&lt;/td&gt;&lt;td style='text-align: left;'&gt;x.x.x.x&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;To see a complete example of the EmrEtlRunner &lt;code&gt;config.yml&lt;/code&gt; file, see the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'&gt;Github repo&lt;/a&gt;.&lt;/p&gt;
&lt;a name='storage-loader'&gt;&lt;h3&gt;4.2 Update the StorageLoader&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;You need to upgrade your StorageLoader installation to the latest code (0.8.11) on Github:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;git clone git://github.com/snowplow/snowplow.git
&lt;span class='nv'&gt;$ &lt;/span&gt;git checkout 0.8.11
&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd &lt;/span&gt;snowplow/4-storage/storage-loader
&lt;span class='nv'&gt;$ &lt;/span&gt;bundle install --deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The StorageLoader &lt;code&gt;config.yml&lt;/code&gt; file includes a new &lt;code&gt;:comprows:&lt;/code&gt; option for Redshift users. This determines the number of rows that Amazon analyzes in order to determine the best compression encoding format to use for each of the fields in your Redshift event table. Note that this is &lt;strong&gt;only&lt;/strong&gt; used if the &lt;code&gt;--include compupdate&lt;/code&gt; option is specified when running the StorageLoader. For more information on Amazon&amp;#8217;s &lt;code&gt;comprows&lt;/code&gt; functionality, see the &lt;a href='http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html'&gt;Redshift documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An example &lt;code&gt;config.yml&lt;/code&gt; for StorageLoader for Redshift users, including the new setting, can be found &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/config/redshift.yml.sample'&gt;on Github&lt;/a&gt;.&lt;/p&gt;
&lt;a name='reprocess'&gt;&lt;h3&gt;4.3 Reprocess any &quot;bad rows&quot; generated between October 21st and your upgrade&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;As described above, if you have been using the CloudFront collector, you will have a number of rows of data in your &amp;#8220;bad bucket&amp;#8221; on S3 generated after the new CloudFront log file format was rolled out on October 21st, because these data rows were not supported by the old version of the Snowplow.&lt;/p&gt;

&lt;p&gt;You need to reprocess these rows so they are not missing from your final data set. For detailed instructions on how to do this, see &lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'&gt;our guide to reprocessing bad data in Snowplow&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/22/cohort-analysis-with-using-new-sql-recipes-and-chartio"/>
    <title>Using the new SQL views to perform cohort analysis with ChartIO</title>
    <updated>2013-10-22T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;We wanted to follow-up our recent &lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/'&gt;launch of Snowplow 0.8.10&lt;/a&gt;, with inbuilt SQL recipes and cubes, with a few posts demonstrating how you can use those views to quickly perform analytics on your Snowplow data. This is the first of those posts.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt='final-cohort-analysis-summary' src='/static/img/blog/2013/10/cohort-analysis/final-cohort-analysis.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;In this post, we&amp;#8217;ll cover how to perform a cohort analysis using &lt;a href='http://chartio.com/'&gt;ChartIO&lt;/a&gt; and Snowplow.&lt;/p&gt;

&lt;h2 id='recap_what_is_cohort_analysis'&gt;Recap: what is Cohort Analysis&lt;/h2&gt;

&lt;p&gt;We have described cohort analysis at length in the &lt;a href='/analytics/customer-analytics/cohort-analysis.html'&gt;Analyst Cookbook&lt;/a&gt;. To sum up, a cohort analysis is a longitudal study, that compares the behaviour or characteristics of groups of people over a long period of time. It therefore encompasses a broad range of analyses, because you can vary:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;how you group people into cohorts (cohort definition), and&lt;/li&gt;

&lt;li&gt;the characteristic that you&amp;#8217;re comparing between cohort over time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In digital media people generally use the phrase &amp;#8216;cohort analysis&amp;#8217; to refer to measurements of retention rates for different cohorts, where cohorts are defined by &lt;em&gt;when&lt;/em&gt; a user was acquired. In that way, SaaS companies, for example, can compare how well they retained customers acquired in October vs those acquired in September vs those acquired in August, and measure in a robust way whether they are getting better at retaining users over time. (This is key to SaaS business model being viable.)&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;We&amp;#8217;ve included 10 different cohort analyses as standard with Snowplow, all of which compare retention rates between different cohorts, but which vary in how they define cohorts and what unit of time the comparison is performed over:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Cohort definition&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Unit of time comparison is performed over&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Paid channel acquired&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Paid channel acquired&lt;/td&gt;&lt;td style='text-align: left;'&gt;Weeks&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Referer acquired&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Referer acquired&lt;/td&gt;&lt;td style='text-align: left;'&gt;Weeks&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Month first touch website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Week first touch website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Weeks&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Month first sign in to website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Week first sign in to website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Month first transact on website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Months&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Week first transact on website&lt;/td&gt;&lt;td style='text-align: left;'&gt;Weeks&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Other metrics you might want to compare between cohorts include &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;average revenue per user&lt;/a&gt; and average &lt;a href='/analytics/customer-analytics/user-engagement.html'&gt;engagement by user&lt;/a&gt;. (Engagement can be defined in many different ways.) For details in how to perform cohort analyses with these measures, see the &lt;a href='/analytics/customer-analytics/cohort-analysis.html'&gt;detailed recipe in the Analysts Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post we&amp;#8217;re going to plot retention by month, based on the month that a user first touched the website. We&amp;#8217;re going to perform this analysis for the Snowplow website itself.&lt;/p&gt;

&lt;h2 id='performing_the_analysis'&gt;Performing the analysis&lt;/h2&gt;

&lt;p&gt;Before we dive into ChartIO and create our plot, let&amp;#8217;s first have a look at the raw data in &lt;a href='http://www.navicat.com/'&gt;Navicat&lt;/a&gt;, by opening up our view directly. Go into Navicat (or your SQL UI of choice) and open up &lt;code&gt;customer_recipes.cohort_retention_by_month_first_touch&lt;/code&gt; view:&lt;/p&gt;
&lt;p&gt;&lt;a href='/static/img/blog/2013/10/cohort-analysis/1.JPG'&gt;&lt;img src='/static/img/blog/2013/10/cohort-analysis/1.JPG' title='cohort data viewed directly in Navicat' /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(You can click on the picture above to &lt;a href='/static/img/blog/2013/10/cohort-analysis/1.JPG'&gt;zoom in&lt;/a&gt; on it.) Let&amp;#8217;s take a good look at the actual structure of the data: this will make it much clearer how to successfully plot the data in ChartIO:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each line of data gives the number of uniques, for that a particular cohort, that were active each month. (This number is given in the &lt;code&gt;uniques&lt;/code&gt; column.)&lt;/li&gt;

&lt;li&gt;Each line also gives a second metric: the &lt;code&gt;fraction_retained&lt;/code&gt;. This is the number of uniques that were active that month, divided by the total number of uniques in that cohort.&lt;/li&gt;

&lt;li&gt;We have three dimensions for each line of data: the first, called &lt;code&gt;cohort&lt;/code&gt;, is the month that the user first touched the website.&lt;/li&gt;

&lt;li&gt;The second dimension is &lt;code&gt;month_actual&lt;/code&gt; - this is the month that the measure was taken. So we see in our example, that we have 9 records for the February 2013 cohort, one for February, one for March, one for April etc. until October. For those 9 records, the first dimension is always equal to February 2013 - i.e. the month that these users first came to our website. But the &lt;code&gt;month_actual&lt;/code&gt; field increments.&lt;/li&gt;

&lt;li&gt;As well as giving the actual month that each measure was taken, we also get a &lt;code&gt;month_rank&lt;/code&gt; which equals 1 for the first measurement for the cohort, 2 for the second etc. This will make it easy to compare the retention rates after e.g. 3 months between cohorts that signed up in January and February, for example: note that we&amp;#8217;d want to compare the March numbers for our January cohort with our April numbers for our February cohort. Rather than work out the number of months between the cohort definition and the actual month, we can simply compare figures where &lt;code&gt;month_rank&lt;/code&gt; = 3 for our comparison.&lt;/li&gt;

&lt;li&gt;Finally, note that our &lt;code&gt;fraction_retained&lt;/code&gt; figure is always 1 (i.e. 100%) for the first month&amp;#8217;s reading for each cohort. That is because, by definition, every member of the cohort is active on the first month that they touch the website. (Activity is defined as visiting the website at least once.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now - to perform our analysis, we want to plot a line graph: one per cohort, where we&amp;#8217;re plotting month rank (on the x-axis) against fraction retained (on the y-axis). We expect a graph as follows, with retention by cohort decreasing over time for each of cohorts:&lt;/p&gt;

&lt;p&gt;&lt;img alt='cohort-mockup' src='/static/img/blog/2013/10/cohort-analysis/cohort-analysis-mockup.png' /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s plot our cohort analysis! Log into ChartIO, go into a dashboard where you want to create your graph and click the &lt;strong&gt;Add Chart&lt;/strong&gt; button. (We&amp;#8217;re going to assume you&amp;#8217;ve already setup a connection in ChartIO to your Snowplow data in Redshift, and in particular, the &lt;code&gt;recipes_customer&lt;/code&gt; schema. If you haven&amp;#8217;t, instructions can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data'&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;&lt;img alt='add-chart' src='/static/img/blog/2013/10/cohort-analysis/3.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Select the data source on the left which connects to the &lt;code&gt;recipes_customer&lt;/code&gt; schema. A long list of all the different views available in that schema will be shown below. Select the &lt;strong&gt;Cohort Dfn by Month First Touch&lt;/strong&gt; view. The different measures and dimensions will be shown:&lt;/p&gt;

&lt;p&gt;&lt;img alt='select-view' src='/static/img/blog/2013/10/cohort-analysis/4.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s create our plot. We want to plot &lt;strong&gt;Fraction Retained&lt;/strong&gt; as our metric, so drag that from the &lt;strong&gt;Data Sources&lt;/strong&gt; column to the &lt;strong&gt;Measures&lt;/strong&gt; pane:&lt;/p&gt;

&lt;p&gt;&lt;img alt='fraction-retained' src='/static/img/blog/2013/10/cohort-analysis/5.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Now we want to plot this over &lt;strong&gt;Month Rank&lt;/strong&gt;, so let&amp;#8217;s drag &lt;strong&gt;Month Rank&lt;/strong&gt; from the &lt;strong&gt;Data Sources&lt;/strong&gt; column to the &lt;strong&gt;Dimensions&lt;/strong&gt; pane:&lt;/p&gt;

&lt;p&gt;&lt;img alt='month-rank' src='/static/img/blog/2013/10/cohort-analysis/6.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Lastly we want to compare the fraction retained over month rank between &lt;strong&gt;Cohort&lt;/strong&gt;, so drag &lt;strong&gt;Cohort&lt;/strong&gt; from the &lt;strong&gt;Data Sources&lt;/strong&gt; column to the dimensions pane as pane as well. ChartIO, on seeing that this is a date field, assumes you want to plot it by week. We want to plot it by month, so click on it in the dimensions pane to reveal a dropdown, and set &lt;strong&gt;Time bucket&lt;/strong&gt; to &lt;strong&gt;Month&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='cohort' src='/static/img/blog/2013/10/cohort-analysis/7.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Now click the &lt;strong&gt;Chart Query&lt;/strong&gt; button. A table with the different values (a subset of those we saw in Navicat) will appear above. Click on the line graph icon, to its right, to plot a line chart and bingo! The cohort analysis is complete:&lt;/p&gt;

&lt;p&gt;&lt;img alt='complete-cohort-analysis' src='/static/img/blog/2013/10/cohort-analysis/8.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;We can see that retention &lt;em&gt;has not&lt;/em&gt;, sadly for us, improved over time. It does appear that the users acquired in August, however, were retained better than those acquired in other months. It would be interesting to understand why: to do this, we&amp;#8217;d need to look at how each user was acquired (what drove them to our website), and whether some channels are better at driving &amp;#8220;sticky&amp;#8221; users than others, and if those channels accounted for a bigger share of user acquisition in August than the other months.&lt;/p&gt;

&lt;p&gt;We plan to post more guides to using the recipes directly in ChartIO and other analytics tools, and as building blocks for developing your own, bespoke analysis, over the next few months. Stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript"/>
    <title>Scripting Hadoop, Part One - Adventures with Scala, Rhino and JavaScript</title>
    <updated>2013-10-21T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As we have got to know the Snowplow community better, it has become clear that many members have very specific event processing requirements including:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Custom trackers and collector logging formats&lt;/li&gt;

&lt;li&gt;Custom event models&lt;/li&gt;

&lt;li&gt;Custom business logic that impacts on the way their event data is processed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To date, we have relied on three main techniques to help Snowplow users meet these requirements:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Adding additional configuration options into the core Enrichment process (e.g. IP address anonymization, coming in 0.8.11)&lt;/li&gt;

&lt;li&gt;Working with users on bespoke re-writes of the Snowplow Enrichment process (mostly forks of the Scalding ETL job)&lt;/li&gt;

&lt;li&gt;Helping users to implement additional processing steps downstream of the current Enrichment/Storage processes (e.g. building reporting cubes in Hive or Redshift)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of these approaches has its strengths and weaknesses, and we will certainly continue to develop and improve all three. But we also want to explore if there is a &amp;#8220;middle ground&amp;#8221; between configuration options and fully bespoke code: can we somehow make the Snowplow Enrichment process user-scriptable?&lt;/p&gt;

&lt;p&gt;If possible, the following approach would make an attractive middle ground:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Pass one or more user-authored scripts into our Scalding ETL at runtime&lt;/li&gt;

&lt;li&gt;The user-authored script(s) are executed against each row of event data&lt;/li&gt;

&lt;li&gt;These scripts can be written in a popular and easy-to-learn scripting language&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Two technologies stood out as promising: Java&amp;#8217;s &lt;a href='http://docs.oracle.com/javase/6/docs/technotes/guides/scripting/programmer_guide/#jsengine'&gt;ScriptEngine&lt;/a&gt; and Mozilla&amp;#8217;s &lt;a href='https://developer.mozilla.org/en/docs/Rhino'&gt;Rhino&lt;/a&gt;. ScriptEngine is a technology bundled with J2SE 6+ which allows dynamic languages to be evaluated at runtime from Java; Rhino is an implementation of JavaScript written in Java and available to any JVM app through ScriptEngine.&lt;/p&gt;

&lt;p&gt;The first step to test if this approach is viable, was to test out Rhino and Scala&amp;#8217;s inter-operation to see what was possible. In the rest of this blog post, we will reproduce that investigation as an interactive REPL (readevalprint loop) session. To follow along, you will need to have SBT and Scala installed&amp;#8230;&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;First we clone our &lt;a href='https://github.com/snowplow/scalding-example-project'&gt;Scalding Example Project&lt;/a&gt;, available on GitHub. This gives us a Scala environment which we know successfully can run Scalding on Hadoop (including Elastic MapReduce), giving us some confidence that whatever scripting works in this environment will ultimately work fine on EMR too.&lt;/p&gt;

&lt;p&gt;So let&amp;#8217;s get started:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git@github.com:snowplow/scalding-example-project.git
$ cd scalding-example-project
$ sbt
scalding-example-project &amp;gt; console
scala&amp;gt; &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great, now we&amp;#8217;re in the Scala console within SBT, and we have access to all of the libraries loaded as part of the scalding-example-project should we need them.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s create a JavaScript-powered ScriptEngine in Scala:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;factory&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;javax&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;ScriptEngineManager&lt;/span&gt;
&lt;span class='n'&gt;factory&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;javax.script.ScriptEngineManager&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;javax&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;ScriptEngineManager&lt;/span&gt;&lt;span class='k'&gt;@&lt;/span&gt;&lt;span class='mi'&gt;381&lt;/span&gt;&lt;span class='n'&gt;ebaf3&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;factory&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;getEngineByName&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;JavaScript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;javax.script.ScriptEngine&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;com&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;sun&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;javascript&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;RhinoScriptEngine&lt;/span&gt;&lt;span class='k'&gt;@&lt;/span&gt;&lt;span class='mi'&gt;75&lt;/span&gt;&lt;span class='n'&gt;ee0563&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;print(&amp;#39;Hello, World\\n&amp;#39;)&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='nc'&gt;Hello&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='nc'&gt;World&lt;/span&gt;
&lt;span class='n'&gt;res4&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Excellent - we&amp;#8217;ve now got JavaScript executing from Scala! For more information, check out the Javadoc for &lt;a href='http://download.java.net/jdk6/archive/b104/docs/api/javax/script/ScriptEngineManager.html'&gt;ScriptEngineManager&lt;/a&gt; and &lt;a href='http://download.java.net/jdk6/archive/b104/docs/api/javax/script/ScriptEngine.html'&gt;ScriptEngine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a next step, let&amp;#8217;s focus on the boundaries and get data flowing from Scala to JavaScript and back out again. Let&amp;#8217;s pass in a variable - I&amp;#8217;m going to prepend dollar signs to all of my Scala-sourced JavaScript variables to make their origin clear:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;put&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$filter&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;yeah&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;isFiltered&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;($filter === \&amp;quot;yeah\&amp;quot;) ? true : false;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;isFiltered&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Great, that worked, although the return type java.lang.Object is obviously a little blunt. Now let&amp;#8217;s see what happens if we write some invalid JavaScript:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;isFiltered&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;undefined.splice()&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;javax&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;ScriptException&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;sun.org.mozilla.javascript.internal.EcmaError:&lt;/span&gt;
&lt;span class='nc'&gt;TypeError&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Cannot&lt;/span&gt; &lt;span class='kt'&gt;call&lt;/span&gt; &lt;span class='kt'&gt;method&lt;/span&gt; &lt;span class='err'&gt;&amp;quot;&lt;/span&gt;&lt;span class='kt'&gt;splice&lt;/span&gt;&lt;span class='err'&gt;&amp;quot;&lt;/span&gt; &lt;span class='kt'&gt;of&lt;/span&gt; &lt;span class='kt'&gt;undefined&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;&amp;lt;Unknown&lt;/span&gt; &lt;span class='kt'&gt;source&amp;gt;&lt;/span&gt;&lt;span class='k'&gt;#&lt;/span&gt;&lt;span class='err'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='kt'&gt;in&lt;/span&gt;
&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nc'&gt;Unknown&lt;/span&gt; &lt;span class='n'&gt;source&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;at&lt;/span&gt; &lt;span class='n'&gt;line&lt;/span&gt; &lt;span class='n'&gt;number&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay - this ScriptException is very similar to what you would see evaluating the same code in the Firefox or Chrome JavaScript consoles, so that&amp;#8217;s reassuring.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s try another failure scenario - where our JavaScript accidentally returns a Number when we are expecting a Boolean:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;isFiltered&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;($filter === \&amp;quot;yeah\&amp;quot;) ? 1 : 0;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;isFiltered&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mf'&gt;1.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The return type definitely looks problematic, although the problem won&amp;#8217;t manifest itself until we try to cast it into a Boolean. So let&amp;#8217;s put together an example with some type safety:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;PartialFunction._&lt;/span&gt;
&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;PartialFunction._&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt;
	&lt;span class='n'&gt;condOpt&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Any&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Boolean&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt; &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;script:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;($filter === \&amp;quot;yeah\&amp;quot;) ? true : false;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res30&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Some&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;($filter === \&amp;quot;yeah\&amp;quot;) ? 1 : 0;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res30&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Perfect! We have wrapped our JavaScript in some sensible Scala types. For more information on the &lt;code&gt;condOpt&lt;/code&gt; &amp;#8220;magic&amp;#8221;, check out &lt;a href='http://stackoverflow.com/a/9828815/255627'&gt;this Stack Overflow answer&lt;/a&gt; to &lt;em&gt;&amp;#8220;How to cast java.lang.Object to a specific type in Scala?&amp;#8221;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s try something a little more ambitious now. Can we mutate a POJO (&amp;#8220;plain old Java object&amp;#8221;) from inside JavaScript? Only one way to find out:&lt;/p&gt;

&lt;p&gt;&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;MyPojo&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; &lt;span class='nd'&gt;@scala&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;reflect&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;BeanProperty&lt;/span&gt; &lt;span class='k'&gt;var&lt;/span&gt; &lt;span class='n'&gt;myVar&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;heart scala&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='n'&gt;defined&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;MyPojo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;myPojo&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;MyPojo&lt;/span&gt;
&lt;span class='n'&gt;myPojo&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;MyPojo&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;MyPojo&lt;/span&gt;&lt;span class='k'&gt;@&lt;/span&gt;&lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='n'&gt;bbf1be2&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;put&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$myPojo&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;myPojo&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$myPojo.myVar = \&amp;quot;heart js\&amp;quot;;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;javax&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;script&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;ScriptException&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;sun.org.mozilla.javascript.internal.EvaluatorException:&lt;/span&gt;
&lt;span class='nc'&gt;Java&lt;/span&gt; &lt;span class='n'&gt;method&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;myVar&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;cannot&lt;/span&gt; &lt;span class='n'&gt;be&lt;/span&gt; &lt;span class='n'&gt;assigned&lt;/span&gt; &lt;span class='n'&gt;to&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt; &lt;span class='o'&gt;(&amp;lt;&lt;/span&gt;&lt;span class='nc'&gt;Unknown&lt;/span&gt; &lt;span class='n'&gt;source&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;&lt;span class='k'&gt;#&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='n'&gt;in&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nc'&gt;Unknown&lt;/span&gt; &lt;span class='n'&gt;source&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='n'&gt;at&lt;/span&gt; &lt;span class='n'&gt;line&lt;/span&gt; &lt;span class='n'&gt;number&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Oh dear! It looks like Java and Scala&amp;#8217;s getters and setters sugar doesn&amp;#8217;t translate well into JavaScript. So let&amp;#8217;s try the actual setter method, and then print using the getter:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$myPojo.setMyVar(\&amp;quot;heart js\&amp;quot;)&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res10&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;print($myPojo.myVar() + \&amp;quot;\\n\&amp;quot;)&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;heart&lt;/span&gt; &lt;span class='n'&gt;js&lt;/span&gt;
&lt;span class='n'&gt;res20&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay great - the mutation seems to be working. And note that trailing semi-colons are optional, just as they are in &amp;#8220;real&amp;#8221; JavaScript. Now let&amp;#8217;s try and get our POJO back out into our Scala context:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;myPojoRedux&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;get&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$myPojo&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;match&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
	&lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;p&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;MyPojo&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;p&lt;/span&gt;
	&lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='k'&gt;_&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;throw&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;ClassCastException&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='n'&gt;myPojoRedux&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;MyPojo&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;MyPojo&lt;/span&gt;&lt;span class='k'&gt;@&lt;/span&gt;&lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='n'&gt;bbf1be2&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;myPojoRedux&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;myVar&lt;/span&gt;
&lt;span class='n'&gt;res26&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;heart&lt;/span&gt; &lt;span class='n'&gt;js&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done! So we have made some progress: we have mutated a POJO inside of JavaScript using the de-sugared setter and getter forms.&lt;/p&gt;

&lt;p&gt;Okay, what&amp;#8217;s the situation with Scala case classes? Obviously we won&amp;#8217;t try to mutate them inside of JavaScript, but it would be great if we can at least see their contents:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;MyCaseClass&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;myVal&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;defined&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;MyCaseClass&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;myCC&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;MyCaseClass&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;can&amp;#39;t touch this&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;myCaseClass&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;MyCaseClass&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;MyCaseClass&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;can&lt;/span&gt;&lt;span class='-Symbol'&gt;&amp;#39;t&lt;/span&gt; &lt;span class='n'&gt;touch&lt;/span&gt; &lt;span class='k'&gt;this&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;put&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$myCaseClass&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;myCC&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;print($myCaseClass.myVal() + \&amp;quot;\\n\&amp;quot;)&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;can&lt;/span&gt;&lt;span class='-Symbol'&gt;&amp;#39;t&lt;/span&gt; &lt;span class='n'&gt;touch&lt;/span&gt; &lt;span class='k'&gt;this&lt;/span&gt;
&lt;span class='n'&gt;res35&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.Object&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Great! We can see inside Scala case classes without any particular fuss.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re almost done for our first blog post - of course, we haven&amp;#8217;t touched Hadoop yet, but we have a much better understanding of how we can script Scala programs (and so hopefully Scalding jobs) using JavaScript.&lt;/p&gt;

&lt;p&gt;Before we go, let&amp;#8217;s try to generalize our &lt;code&gt;evalAsBoolean()&lt;/code&gt; method above into something a little bit more reusable. How about a method with a signature like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='cm'&gt;/**&lt;/span&gt;
&lt;span class='cm'&gt; * Evaluate some JavaScript into a Some(Boolean),&lt;/span&gt;
&lt;span class='cm'&gt; * returning None if this evaluation failed.&lt;/span&gt;
&lt;span class='cm'&gt; *&lt;/span&gt;
&lt;span class='cm'&gt; * @param js The JavaScript to evaluate&lt;/span&gt;
&lt;span class='cm'&gt; * @param vars A Map of variables to pass into&lt;/span&gt;
&lt;span class='cm'&gt; * the JavaScript&lt;/span&gt;
&lt;span class='cm'&gt; * @return An Option-boxed Boolean&lt;/span&gt;
&lt;span class='cm'&gt; */&lt;/span&gt;
&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;, &lt;span class='kt'&gt;Object&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Hopefully the function arguments and return value are fairly clear, so let&amp;#8217;s proceed to the whole function definition:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;javax.script.ScriptEngineManager&lt;/span&gt;
&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;PartialFunction._&lt;/span&gt;

&lt;span class='cm'&gt;/**&lt;/span&gt;
&lt;span class='cm'&gt; * Evaluate some JavaScript into a Some(Boolean),&lt;/span&gt;
&lt;span class='cm'&gt; * returning None if this evaluation failed.&lt;/span&gt;
&lt;span class='cm'&gt; *&lt;/span&gt;
&lt;span class='cm'&gt; * @param js The JavaScript to evaluate&lt;/span&gt;
&lt;span class='cm'&gt; * @param vars A Map of variables to pass into&lt;/span&gt;
&lt;span class='cm'&gt; * the JavaScript&lt;/span&gt;
&lt;span class='cm'&gt; * @return An Option-boxed Boolean&lt;/span&gt;
&lt;span class='cm'&gt; */&lt;/span&gt;
&lt;span class='k'&gt;def&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;, &lt;span class='kt'&gt;Object&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
	
    &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;factory&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nc'&gt;ScriptEngineManager&lt;/span&gt;
    &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='n'&gt;factory&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;getEngineByName&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;JavaScript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

    &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;prependDollar&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;v&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;v&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;startsWith&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;$&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='n'&gt;v&lt;/span&gt; &lt;span class='k'&gt;else&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;$%s&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;format&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;v&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
    &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;((&lt;/span&gt;&lt;span class='n'&gt;k&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;v&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;vars&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;put&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;prependDollar&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;k&lt;/span&gt;&lt;span class='o'&gt;),&lt;/span&gt; &lt;span class='n'&gt;v&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

    &lt;span class='k'&gt;try&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='n'&gt;condOpt&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;engine&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;eval&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Any&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Boolean&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;b&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;catch&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='n'&gt;se&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;javax.script.ScriptException&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;None&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Paste that into the Scala console in SBT and you should see:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;js:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;vars:&lt;/span&gt; &lt;span class='kt'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;,&lt;span class='kt'&gt;java.lang.Object&lt;/span&gt;&lt;span class='o'&gt;])&lt;/span&gt;&lt;span class='nc'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;#8217;s try this out - first with a script which should evaluate to true:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;vars1&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;, &lt;span class='kt'&gt;Object&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;
&lt;span class='s'&gt;&amp;quot;one&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;java&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;lang&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;Integer&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;),&lt;/span&gt;
&lt;span class='s'&gt;&amp;quot;$two&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;java&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;lang&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='nc'&gt;Integer&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;vars1&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;scala.collection.immutable.Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;,&lt;span class='kt'&gt;java.lang.Object&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;one&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='nc'&gt;$two&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;js1&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;($one + $two) === 3;&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;js1&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='nc'&gt;$one&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='nc'&gt;$two&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;===&lt;/span&gt; &lt;span class='mi'&gt;3&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js1&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars1&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res1&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Some&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now a false value, involving checking a property inside of a case class:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;case&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ALang&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;aLang&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;String&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;defined&lt;/span&gt; &lt;span class='k'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ALang&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;vars2&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;, &lt;span class='kt'&gt;Object&lt;/span&gt;&lt;span class='o'&gt;](&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;lang&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;ALang&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;dart&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;
&lt;span class='n'&gt;vars2&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;scala.collection.immutable.Map&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;String&lt;/span&gt;,&lt;span class='kt'&gt;java.lang.Object&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Map&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;lang&lt;/span&gt; &lt;span class='o'&gt;-&amp;gt;&lt;/span&gt; &lt;span class='nc'&gt;ALang&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dart&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;js2&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;$lang.aLang() === \&amp;quot;js\&amp;quot;;&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;js2&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;$lang&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;aLang&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt; &lt;span class='o'&gt;===&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;js&amp;quot;&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js2&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res2&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;Some&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That&amp;#8217;s working a treat. Now let&amp;#8217;s try evaluating an invalid piece of JavaScript:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;js3&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;$doesNotExist.arg()&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;js3&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;$doesNotExist&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;arg&lt;/span&gt;&lt;span class='o'&gt;()&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js3&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res3&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Good, and finally a valid piece of JavaScript but one which returns a String when we want a Boolean:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;js4&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;\&amp;quot;I heart \&amp;quot; + $lang.aLang();&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;js4&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;java.lang.String&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;I heart &amp;quot;&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='nc'&gt;$lang&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;aLang&lt;/span&gt;&lt;span class='o'&gt;();&lt;/span&gt;

&lt;span class='n'&gt;scala&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='n'&gt;evalAsBoolean&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;js4&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='n'&gt;vars2&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;res4&lt;/span&gt;&lt;span class='k'&gt;:&lt;/span&gt; &lt;span class='kt'&gt;Option&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='kt'&gt;Boolean&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='nc'&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Great! Those are all behaving as expected. We&amp;#8217;re going to pause here, but we&amp;#8217;ve already made some good progress understanding how JavaScript can be invoked at runtime from a Scala environment.&lt;/p&gt;

&lt;p&gt;In the next post, we will take these learnings and start to apply them within a Scalding environment, with the aim of getting some basic user-defined JavaScript executing on Elastic MapReduce. Stay tuned for the next installment!&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re interested in adapting Snowplow&amp;#8217;s technology to meet your custom event processing needs, and would like to discuss your requirements with the Snowplow team, then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes"/>
    <title>Snowplow 0.8.10 released with analytics cubes and recipes 'baked in'</title>
    <updated>2013-10-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the release of Snowplow 0.8.10. In this release, we have taken many of the SQL recipes we have covered in the &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analysts Cookbook&lt;/a&gt; and &amp;#8216;baked them&amp;#8217; into Snowplow by providing them as views that can be added directly to your Snowplow data in Amazon Redshift or PostgreSQL.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#background'&gt;Background on this release&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#schema'&gt;Reorganizing the Snowplow database&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#recipe-use'&gt;Seeing a recipe in action: charting the number of uniques over time&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#visitor-cube'&gt;Seeing a cube in action: interrogating the visitors cube in Tableau&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#setup'&gt;Installing this release&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/10/18/snowplow-0.8.10-released-with-analytics-recipes-and-cubes/#next-steps'&gt;Next steps: where to go from here&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='background'&gt;&lt;h2&gt;1. Background on this release&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;One of the things we&amp;#8217;ve learnt from many new Snowplow users, is that they want to get up and running analyzing Snowplow data as fast as possible: often by putting a familiar business intelligence tool directly on top of Snowplow data, to start exploring and visualizing that data.&lt;/p&gt;

&lt;p&gt;For those users, a frustration with Snowplow is that each analysis typically starts with having to write a SQL query on the Snowplow data, to transform it into a format suitable for analysing in a BI / OLAP tool. Whilst we believe it is a strength that Snowplow gives you the flexibility to design and structure a wide range of different analyses, we recognise that for new users in particular, it would be nicer if they could dive straight into the data in their BI tool of choice.&lt;/p&gt;

&lt;p&gt;This release aims to bridge that gap: we are providing a range of recipes and cubes as SQL views into the atomic Snowplow data; all of these are suitable for being loaded into BI tools like Excel, ChartIO and Tableau, directly.&lt;/p&gt;
&lt;!--more--&gt;&lt;a name='schema'&gt;&lt;h2&gt;2. Reorganizing the Snowplow database&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;With this release we have reorganized Snowplow data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;events&lt;/code&gt; table is now located in the &lt;code&gt;atomic&lt;/code&gt; schema. (This is new for Redshift users, not for Postgres users.)&lt;/li&gt;

&lt;li&gt;There are three new schemas that contain &amp;#8220;cubes&amp;#8221; - i.e. views of the Snowplow data that can be consumed directly in a pivot / OLAP / BI tool e.g. PowerPivot or Tableau. These are &lt;code&gt;cubes_pages&lt;/code&gt;, &lt;code&gt;cubes_visits&lt;/code&gt; and &lt;code&gt;cubes_transactions&lt;/code&gt;.&lt;/li&gt;

&lt;li&gt;There are three new schemas that contain &amp;#8220;recipes&amp;#8221; - views of the Snowplow data that can be visualized directly in any graphics package. These are &lt;code&gt;recipes_basic&lt;/code&gt;, &lt;code&gt;recipes_customer&lt;/code&gt; and &lt;code&gt;recipes_catalog&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='recipe-use'&gt;&lt;h2&gt;3. Seeing a recipe in action: charting the number of uniques over time&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Let&amp;#8217;s start by showing how easy the views make it to start plotting Snowplow data in ChartIO. Log into ChartIO and create a new connection to your database, just as before, but this time set the &lt;strong&gt;Schema name&lt;/strong&gt; field to be &amp;#8216;recipes_basic&amp;#8217; rather than &amp;#8216;atomic&amp;#8217;. (This is necessary, because ChartIO requires a different connection for each schema on your database, rather than a single connection per database.)&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-setup' src='/static/img/blog/2013/10/chartio-connection.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now go back into ChartIO and select to create a new graph. Select your new data source: a long list of different &amp;#8220;recipes&amp;#8221; should be displayed. Select the &lt;strong&gt;Uniques And Visits By Day&lt;/strong&gt; from the list - this will reveal the dimensions and metrics returned in that view:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-setup-2' src='/static/img/blog/2013/10/chartio-2.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now you can simply drag the relevant metrics and dimensions over from the list into the design pane. Let&amp;#8217;s simply plot uniques by date: drag the &lt;strong&gt;Date&lt;/strong&gt; dimension to the dimensions pane, and the &lt;strong&gt;Uniques&lt;/strong&gt; measure over to the &lt;strong&gt;Measures&lt;/strong&gt; pane:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-setup-select-dimensions' src='/static/img/blog/2013/10/chartio-select-dimensions.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now click &lt;strong&gt;Chart Query&lt;/strong&gt; to draw the graph. Simple visualization of Snowplow data with no SQL required!&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-setup-3' src='/static/img/blog/2013/10/chartio-final-graph.png' /&gt;&lt;/p&gt;
&lt;a name='visitor-cube'&gt;&lt;h2&gt;4. Seeing a cube in action: interrogating the visitors cube in Tableau&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We&amp;#8217;ve included a number of &amp;#8220;cube views&amp;#8221; in the new release. These can be opened directly into your pivoting tool of choice.&lt;/p&gt;

&lt;p&gt;For this example, we&amp;#8217;re going to open the &lt;code&gt;cubes_visits.referer_entries_and_exits&lt;/code&gt; view directly into Tableau.&lt;/p&gt;

&lt;p&gt;Open up Tableau, select to create a new database connection, enter your database details. Select the &lt;code&gt;referer_entries_and_exits&lt;/code&gt; view to connect to.&lt;/p&gt;

&lt;p&gt;&lt;img alt='talbeau-setup-1' src='/static/img/blog/2013/10/tableau-connection.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Note: if you are connecting to the views in Redshift, you will need to add the new schemas to your &lt;code&gt;search_path&lt;/code&gt; before they are visible in Tableau. You can, however, access them directly by selecting &lt;strong&gt;Custom SQL&lt;/strong&gt; and entering &lt;code&gt;SELECT * FROM cubes_visits.referer_entries_and_exits&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Tableau will ask whether you want to import all the data, or connect live. If you have a lot of data, we recommend connecting live.&lt;/p&gt;

&lt;p&gt;&lt;img alt='tableau-setup-2' src='/static/img/blog/2013/10/tableau-1.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;You can now drag and drop any of the dimensions and any of the metrics listed. For example, we can drag in &lt;code&gt;entry_page_path&lt;/code&gt;, &lt;code&gt;visit_start_ts&lt;/code&gt; and &lt;code&gt;visit_duration&lt;/code&gt; to see how average visit lengths have changed per landing page over time:&lt;/p&gt;

&lt;p&gt;&lt;img alt='tableau-setup-3' src='/static/img/blog/2013/10/tableau-visualization.JPG' /&gt;&lt;/p&gt;
&lt;a name='setup'&gt;&lt;h2&gt;5. Installing this release&lt;/h2&gt;&lt;/a&gt;
&lt;h3 id='51_redshift_users'&gt;5.1 Redshift users&lt;/h3&gt;

&lt;p&gt;If you&amp;#8217;re using Redshift, you will need to migrate your Snowplow events table from the &lt;code&gt;public&lt;/code&gt; schema to the &lt;code&gt;atomic&lt;/code&gt; schema. This can be done using &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.2.1_to_0.2.2.sql'&gt;this migration script&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You will then you need to update your &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/config/redshift.yml.sample'&gt;StorageLoader config file&lt;/a&gt; to ensure that from now on, all new data is loaded into the &lt;code&gt;atomic.events&lt;/code&gt; table, rather than the &lt;code&gt;public.events&lt;/code&gt; table. You do this by updating the file so that the &lt;code&gt;:table:&lt;/code&gt; key is set to &amp;#8216;atomic.events&amp;#8217; rather than just &amp;#8216;events&amp;#8217;:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:targets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;:name&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;     &lt;span class='s'&gt;&amp;quot;Snowplow&lt;/span&gt;&lt;span class='nv'&gt; &lt;/span&gt;&lt;span class='s'&gt;PostgreSQL&amp;quot;&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;     &lt;span class='l-Scalar-Plain'&gt;postgres&lt;/span&gt; 
    &lt;span class='l-Scalar-Plain'&gt;:host&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;     &lt;span class='l-Scalar-Plain'&gt;ec2-54-221-8-121.compute-1.amazonaws.com&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:database&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;snplow2&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:port&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;     &lt;span class='l-Scalar-Plain'&gt;5432&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:table&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;    &lt;span class='l-Scalar-Plain'&gt;atomic.events&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you need to create the new schemas for the different views, and define each view. The following six scripts need to be run:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/recipes/recipes-basic.sql'&gt;recipes-basic.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/recipes/recipes-catalog.sql'&gt;recipes-catalog.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/recipes/recipes-customers.sql'&gt;recipes-customers&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/cubes/cube-visits.sql'&gt;cube-visits.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/cubes/cube-transactions.sql'&gt;cube-transactions.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/redshift-analytics/cubes/cube-pages.sql'&gt;cube-pages.sql&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These can be run directly using the &lt;code&gt;psql&lt;/code&gt; command line tool, as described &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-the-prebuilt-views-in-Redshift-and-PostgreSQL'&gt;in the setup guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, you will want to add the new schemas to your &lt;code&gt;search_path&lt;/code&gt;. This is necessary for the views in these schemas to show up in tools like Tableau and SQL Workbench. An explanation of how to update the search path is given &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-redshift#wiki-search_path'&gt;here, in the setup guide&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='52_postgresql_users'&gt;5.2 PostgreSQL users&lt;/h3&gt;

&lt;p&gt;If you are using PostgreSQL, your events data should already be in the &lt;code&gt;atomic.events&lt;/code&gt; schema.&lt;/p&gt;

&lt;p&gt;You need to do is updated your events table definition, as per &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/postgres-storage/sql/migrate_0.1.0_to_0.1.1.sql'&gt;this migration script&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Afterwards, you can create the new schemas and views, by running the following scripts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/recipes/recipes-basic.sql'&gt;recipes-basic.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/recipes/recipes-catalog.sql'&gt;recipes-catalog.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/recipes/recipes-customers.sql'&gt;recipes-customers&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/cubes/cube-visits.sql'&gt;cube-visits.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/cubes/cube-transactions.sql'&gt;cube-transactions.sql&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/blob/master/5-analytics/postgres-analytics/cubes/cube-pages.sql'&gt;cube-pages.sql&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These can be run directly using the &lt;code&gt;psql&lt;/code&gt; command line tool, as described &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-the-prebuilt-views-in-Redshift-and-PostgreSQL'&gt;in the setup guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name='next-steps'&gt;&lt;h2&gt;6. Next steps: where to go from here&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We&amp;#8217;ll be covering how to use the recipes and cubes in more detail in forthcoming blog posts, and of course adding new recipes to the &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analysts Cookbook&lt;/a&gt;. In the meantime, we recommend that curious users start experimenting with the different views, and refer to the underlying SQL to understand how they&amp;#8217;re created, and indeed how they can tweak those statements to deliver the data formatted as they need.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/07/announcing-our-winter-open-source-internship-program</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/07/announcing-our-winter-open-source-internship-program"/>
    <title>Announcing our winter open source internship program</title>
    <updated>2013-10-07T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Applications for the new Snowplow Analytics open source internship program are now open! At Snowplow we are passionate about enterprise-strength open-source technology, and we are hugely excited to be offering paid internships for open source hackers this winter.&lt;/p&gt;

&lt;p&gt;Snowplow Analytics is looking for one or two open source interns this winter (December through February), for 3-6 week paid internships. Our &amp;#8220;winterns&amp;#8221; will work directly on and contribute to data engineering projects within the &lt;a href='https://github.com/snowplow'&gt;Snowplow open source stack&lt;/a&gt;. We have lots of ideas for cool projects to do around Snowplow - and if you have any suggestions of your own, we would love to hear them!&lt;/p&gt;

&lt;p&gt;&lt;img alt='billion-dollar-brain' src='/static/img/blog/2013/10/billion-dollar-brain.png' /&gt;&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;A Snowplow wintern loves coding, enjoys experimenting with new technologies and is happiest working &amp;#8220;in the open&amp;#8221; on community/team projects. In terms of programming languages: most of our stack is in Scala, Ruby, JavaScript and Clojure, but we also have code written in R, Java, C, SQL and Lua - in fact, whatever languages you like working in, we can probably find something interesting for you to do. We should stress that almost all of our work revolves around data engineering - you won&amp;#8217;t be involved in front-end/web development during your winternship, unless you want to of course!&lt;/p&gt;

&lt;p&gt;We won&amp;#8217;t filter based on your life stage, professional qualifications or (prior) employment status - you just have to be available to work full time (Monday-Friday, 8 hours a day) for between 3 and 6 weeks this winter. We will consider candidates who can work from our London office, and additionally remote candidates who are UTC +/- 5 hours maximum. We need to restrict the timezones like this to maintain some quality shared office hours between our interns and the London office - we&amp;#8217;ll be seeing you on HipChat!&lt;/p&gt;

&lt;p&gt;These are paid internships - pay is negotiable and will be based on location and travel costs.&lt;/p&gt;

&lt;p&gt;Interested? Please email &lt;a href='mailto:wintern@snowplowanalytics.com'&gt;wintern@snowplowanalytics.com&lt;/a&gt;, and tell us about a piece of software you are proud to have written. Here are some do&amp;#8217;s for your application:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do tell us a little bit about yourself - what you are passionate about, what you are looking for in an internship&lt;/li&gt;

&lt;li&gt;Do share any public profiles or contributions you have on open-source, libre or community projects&lt;/li&gt;

&lt;li&gt;Do share links to publically available code written by you (perhaps something on GitHub or Bitbucket?)&lt;/li&gt;

&lt;li&gt;Do suggest specific projects/initiatives/features that you would like to work on in your internship (it&amp;#8217;s okay if you don&amp;#8217;t know!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some don&amp;#8217;ts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Please don&amp;#8217;t share any proprietary or commercial software with us&lt;/li&gt;

&lt;li&gt;Please don&amp;#8217;t attach any code to your email (but it&amp;#8217;s okay to tell us about code that you &lt;em&gt;could&lt;/em&gt; share later on)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And that&amp;#8217;s it! We look forward to receiving your applications to &lt;a href='mailto:wintern@snowplowanalytics.com'&gt;wintern@snowplowanalytics.com&lt;/a&gt;, and we guarantee that everybody who applies will get a personal response from the founders.&lt;/p&gt;

&lt;p&gt;And finally, if you&amp;#8217;re interested in what we&amp;#8217;re doing at Snowplow but interning this winter isn&amp;#8217;t for you, do set your diary to check back with us in March 2014, when we should be announcing our summer internship program.&lt;/p&gt;

&lt;p&gt;Keep plowing!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/10/01/snowplow-passes-500-stars</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/10/01/snowplow-passes-500-stars"/>
    <title>Snowplow passes 500 stars on GitHub</title>
    <updated>2013-10-01T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As of yesterday, the &lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow repository on GitHub&lt;/a&gt; now has over 500 stars! We&amp;#8217;re hugely excited to reach this milestone, having picked up &lt;strong&gt;300 new watchers&lt;/strong&gt; since our &lt;a href='/blog/2013/01/20/snowplow-hits-202-stars'&gt;last milestone&lt;/a&gt; in January.&lt;/p&gt;

&lt;p&gt;Many thanks to everyone in the Snowplow community and on GitHub for their continuing support and interest!&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a quick round-up of the most popular open source analytics projects on GitHub:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/mnutt/hummingbird'&gt;Hummingbird&lt;/a&gt; (real-time web analytics) - 2,299 stars&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/piwik/piwik'&gt;Piwik&lt;/a&gt; (web analytics) - 1,290 stars&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/Countly/countly-server'&gt;Countly&lt;/a&gt; (mobile analytics) - 798 stars&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow&lt;/a&gt; - 501 stars&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we&amp;#8217;re in the challenger position but definitely nipping at the heels of the alternatives!&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;We thought it would be fun to share an updated &lt;a href='http://jrvis.com/red-dwarf/?user=snowplow&amp;amp;repo=snowplow'&gt;Red Dwarf heatmap&lt;/a&gt; showing where our 501 GitHub stars are located across the world:&lt;/p&gt;

&lt;p&gt;&lt;img alt='heatmap' src='/static/img/blog/2013/10/snowplow-stars-at-501.png' /&gt;&lt;/p&gt;

&lt;p&gt;Viewed alongside &lt;a href='/blog/2013/01/20/snowplow-hits-202-stars'&gt;January&amp;#8217;s heatmap&lt;/a&gt;, it&amp;#8217;s really encouraging to see Snowplow picking up more interest in South Asia, Eastern Europe and the West Coast of the US!&lt;/p&gt;

&lt;p&gt;In other news, GitHub recently replaced its list of the most-watched Scala projects with a list of &lt;a href='https://github.com/trending?l=scala'&gt;Trending Scala repositories&lt;/a&gt;. We were delighted to hit the top of this list in mid-September, pipping Scala stalwarts including the Play Framework and Scala itself:&lt;/p&gt;

&lt;p&gt;&lt;img alt='trending' src='/static/img/blog/2013/10/snowplow-trending-16-sep-2013.png' /&gt;&lt;/p&gt;

&lt;p&gt;All of this is hugely encouraging for the future. So many thanks for your continued support and look out for some big new features coming to Snowplow soon!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/09/30/book-review-instant-hive-essentials-how-to</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/09/30/book-review-instant-hive-essentials-how-to"/>
    <title>Book review - Apache Hive Essentials How-to</title>
    <updated>2013-09-30T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Although it is no longer part of the core Snowplow stack, Apache Hive is the gateway drug that got us started on Hadoop. As some of our &lt;a href='http://snowplowanalytics.com/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data/'&gt;recent&lt;/a&gt; &lt;a href='http://snowplowanalytics.com/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/'&gt;blog posts&lt;/a&gt; testify, Hive is still very much a part of our big data toolkit, and this will continue as we use it to roll out new features. (E.g. for analyzing custom unstructured events.) I suspect that many Hadoopers started out with Hive, before experimenting with the myriad other tools to crunch data using Hadoop.&lt;/p&gt;

&lt;p&gt;We were therefore delighted to be invited to review &lt;a href='http://www.packtpub.com/apache-hive-essentials-how-to/book?utm_source=blog&amp;amp;utm_medium=link&amp;amp;utm_campaign=bookmention'&gt;Apache Hive Essentials How-to&lt;/a&gt;, a new guide to Hive written by Darren Lee from &lt;a href='http://www.bizo.com/home'&gt;Bizo&lt;/a&gt;.&lt;/p&gt;
&lt;a href='http://www.packtpub.com/apache-hive-essentials-how-to/book?utm_source=blog&amp;amp;utm_medium=link&amp;amp;utm_campaign=bookmention'&gt;
	&lt;img src='/static/img/blog/2013/09/instant-apache-hive-essentials.png' title='Hive how to guide' /&gt;
&lt;/a&gt;
&lt;p&gt;For me, there are two totally different categories of technical book that I enjoy in completely different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Books that help me use tools more effectively: so I do more in less time.&lt;/li&gt;

&lt;li&gt;Books that change the way I see the tools I use. These books step back from the practicalities of using the particular tools they cover to situate them in their proper context, and compare their usage with other tools. My favorite example in this cateogry is &lt;a href='http://pragprog.com/book/btlang/seven-languages-in-seven-weeks'&gt;Seven Languages in Seven Weeks&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Often, technical books fail because they try and accomplish &lt;em&gt;both&lt;/em&gt; of the above.&lt;/p&gt;

&lt;p&gt;Fortunately, that is &lt;strong&gt;not&lt;/strong&gt; true of the &lt;a href='http://www.packtpub.com/apache-hive-essentials-how-to/book?utm_source=blog&amp;amp;utm_medium=link&amp;amp;utm_campaign=bookmention'&gt;Apache Hive Essentials How-to&lt;/a&gt;. This is an uncompromisingly practical, focused book, that makes essential reading for anyone working with Hive.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='why_is_it_such_a_good_book'&gt;Why is it such a good book?&lt;/h2&gt;

&lt;p&gt;The best thing about Hive is that it makes Hadoop accessible to anyone familiar with SQL. The tricky thing about Hive, is that getting the most out of it means using those features that take you out of SQL, in particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Writing custom SerDes&lt;/li&gt;

&lt;li&gt;Writing user defined functions, including table defining functions&lt;/li&gt;

&lt;li&gt;Streaming with Python / Ruby&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Darren Lee does a great job of jumping straight into some of the more useful, trickier aspects of Hive (particularly optimizing joins) before covering the topics listed above. His instructions for writing custom SerDes and user defined functions is succinct and easy to follow. His code is clear and well explained. His examples are well chosen. (I particularly like example of implementing simple linear regression as a user-defined aggregation function.)&lt;/p&gt;

&lt;p&gt;This book is short, sharp and to the point. I only wish I&amp;#8217;d read it 18 months ago.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/09/27/how-much-does-snowplow-cost-to-run</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/09/27/how-much-does-snowplow-cost-to-run"/>
    <title>How much does Snowplow cost to run, vs the competition?</title>
    <updated>2013-09-27T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the release of the &lt;a href='https://github.com/snowplow/snowplow-tco-model'&gt;Snowplow Total Cost of Ownership Model&lt;/a&gt;. This is a model we started developing &lt;a href='http://snowplowanalytics.com/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'&gt;back in July&lt;/a&gt;, to enable:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Snowplow users and prospective users to better forecast their Snowplow costs on &lt;a href='http://aws.amazon.com/'&gt;Amazon Web Services&lt;/a&gt; going forwards&lt;/li&gt;

&lt;li&gt;The Snowplow Development Team to monitor how the cost of running Snowplow evolves as we build out the platform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modelling the costs associated with running Snowplow has not been straightforward: in the next few weeks, we&amp;#8217;ll publish a series of blog posts exploring those challenges and how we tackled them. We&amp;#8217;ll also review why we chose to build the model in &lt;a href='http://cran.r-project.org/'&gt;R&lt;/a&gt; (rather than Excel), and explore some surprising aspects of what drives Snowplow costs on AWS, building on our &lt;a href='http://snowplowanalytics.com/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'&gt;last blog post&lt;/a&gt; on the subject.&lt;/p&gt;

&lt;p&gt;In the meantime, &lt;a href='https://github.com/snowplow/snowplow-tco-model'&gt;download our TCO Model&lt;/a&gt; and try it out yourself: it will let you model the cost of your particular Snowplow setup, and see how costs divide between the different AWS services.&lt;/p&gt;

&lt;p&gt;In the rest of this blog post, we&amp;#8217;ll focus on perhaps the most interesting output of the model: &lt;strong&gt;How expensive is it to run Snowplow vs our commercial competitors?&lt;/strong&gt; Let&amp;#8217;s start by comparing it with web analytics stalwarts Google Analytics and SiteCatalyst:&lt;/p&gt;

&lt;p&gt;&lt;img alt='snowplow-premium-price-comparison' src='/static/img/price-comparison/snowplow-google-analytics-omniture-sitecatalyst-price-comparison.png' /&gt;&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Snowplow cannot compete with the free version of &lt;a href='http://www.google.co.uk/intl/en_uk/analytics/index.html'&gt;Google Analytics&lt;/a&gt; directly on cost. Our model forecasts that even at low volumes, Snowplow ends up costing $1000 per year: this really reflects the cost of running a server to host a PostgreSQL database for your Snowplow data. (Our model assumes an m1.large EC2 instance being provisioned for this purpose - this accounts for 75% of the cost - there may be alternatives that bring that cost down significantly.)&lt;/p&gt;

&lt;p&gt;At higher volumes, though, the savings are substantial. Running Snowplow is &lt;strong&gt;at least an order of magnitude cheaper&lt;/strong&gt; than &lt;a href='http://www.google.co.uk/intl/en_uk/analytics/premium/index.html'&gt;GA Premium&lt;/a&gt; and &lt;a href='http://www.adobe.com/solutions/digital-analytics/marketing-reports-analytics.html'&gt;Adobe SiteCatalyst&lt;/a&gt; for businesses recording between 10k and 100k events per month. (At these volumes, we&amp;#8217;ve modelled the cost based on users employing Redshift, rather than PostgreSQL, to store their data.) Even at a billion events per month (where multiple Redshift nodes are required to store all your atomic data), the cost of running Snowplow is a little over half the cost of running Google Analytics Premium.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve then compared the cost of running Snowplow with the cost of some of the SaaS analytics startups (note in this diagram we&amp;#8217;re comparing monthly, rather than yearly costs):&lt;/p&gt;

&lt;p&gt;&lt;img alt='snowplow-saas-price-comparison' src='/static/img/price-comparison/snowplow-saas-price-comparison.png' /&gt;&lt;/p&gt;

&lt;p&gt;For anyone capturing 500K events or more, it is cheaper to run Snowplow, than &lt;a href='https://mixpanel.com/'&gt;Mixpanel&lt;/a&gt;, &lt;a href='https://www.kissmetrics.com/'&gt;Kissmetrics&lt;/a&gt; or &lt;a href='https://keen.io/'&gt;Keen IO&lt;/a&gt;. However, the saving increases with volume: at 20M events per month, Snowplow costs 25% less to run than Mixpanel, for example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sources&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The above pricing comparisons were put together with data from the following sources: if over time the prices change, then please let us know, so we can update them to keep them accurate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='http://www.neboagency.com/blog/google-analytics-vs-site-catalyst-2/'&gt;Google Analytics Premium pricing&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Omniture SiteCatalyst pricing based on a cross section of 20 different clients&lt;/li&gt;

&lt;li&gt;&lt;a href='https://mixpanel.com/pricing/'&gt;Mixpanel pricing page&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://www.kissmetrics.com/pricing'&gt;Kissmetrics pricing page&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://keen.io/'&gt;Keen IO pricing page&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;For Snowplow pricing, please refer to the &lt;a href='https://github.com/snowplow/snowplow-tco-model'&gt;Snowplow TCO model&lt;/a&gt; for details on both the underlying model assumptions and the model mechanics.&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole"/>
    <title>Reprocessing bad rows of Snowplow data using Hive, the JSON Serde and Qubole</title>
    <updated>2013-09-11T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the distinguishing features of the Snowplow data pipeline is the handling of &amp;#8220;bad&amp;#8221; data. Every row of incoming, raw data is validated. When a row fails validation, it is logged in a &amp;#8220;bad rows&amp;#8221; bucket on S3 alongside the error message that was generated by the failed validation. That means you can keep track of the number of rows that fail validation, and have the opportunity to update and then reprocess those bad rows. (This makes Snowplow different from traditional web analytics platforms, that simply ignore bad rows of data, and provide no insight into the volume of incoming data that ends up being ignored.)&lt;/p&gt;

&lt;p&gt;This functionality was crucial in spotting that, in mid-August, Amazon made an &lt;a href='/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change/'&gt;undocumented update the CloudFront collector file format&lt;/a&gt;. This resulted in a sudden spike in the number of &amp;#8220;bad rows&amp;#8221; generated by Snowplow, as the &lt;code&gt;cs-uri-query&lt;/code&gt; field format changed from the format the Enrichment process expected. (For details of the change, see &lt;a href='/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change/'&gt;this blog post&lt;/a&gt;, and the links in it.) Amazon has since rolled back the update, and we have since updated Snowplow to be able to process rows in both formats. However, Snowplow users will have three weeks of data with lines of data missing, that ideally need to be reprocessed using the updated Snowplow version.&lt;/p&gt;
&lt;img src='/static/img/blog/2013/09/black_sheep.jpg' title='black sheet - can you spot bad data?' width='300' /&gt;
&lt;p&gt;In this blog post, we will walk through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to use &lt;a href='http://hive.apache.org/'&gt;Apache Hive&lt;/a&gt;, &lt;a href='http://www.qubole.com/'&gt;Qubole&lt;/a&gt; and &lt;a href='https://github.com/rcongiu'&gt;Robert Congui&amp;#8217;s&lt;/a&gt; &lt;a href='https://github.com/rcongiu/Hive-JSON-Serde'&gt;JSON serde&lt;/a&gt; to monitor the number of bad rows generated over time&lt;/li&gt;

&lt;li&gt;How to use the same tools to reprocess the bad rows of data, so that they are added to your Snowplow data in Redshift / PostgreSQL&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The steps necessary to reprocess the data will be very similar to those required regardless of the reason that the reprocessing is necessary: as a result, this blog post should be useful for anyone interested in using the bad rows functionality to debug and improve the robustness of their event data collection. It should also be useful for anyone interested in using &lt;a href='http://hive.apache.org/'&gt;Hive&lt;/a&gt; and the &lt;a href='https://github.com/rcongiu/Hive-JSON-Serde'&gt;JSON serde&lt;/a&gt; to process JSON data in S3. (Bad row data is stored by Snowplow in JSON format.) We will use &lt;a href='http://www.qubole.com/'&gt;Qubole&lt;/a&gt;, our preferred platform for running Hive jobs on data in S3, which we previously introduced in &lt;a href='/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data/'&gt;this blog post&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#how-snowplow-handles-bad-rows'&gt;Understanding how Snowplow handles bad rows&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#processing-bad-rows-data-using-json-serde-hive-qubole'&gt;Processing the bad rows data using the JSON serde, Hive and Qubole&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#plot-bad-rows-over-time'&gt;Plotting the number of bad rows over time&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/09/11/reprocessing-bad-data-using-hive-the-json-serde-and-qubole/#processing-bad-rows'&gt;Reprocessing bad rows&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='how-snowplow-handles-bad-rows'&gt;&lt;h2&gt;1. Understanding how Snowplow handles bad rows&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The Snowplow enrichment process takes input lines of data, in the form of collector logs. It validates the format of the data in each of those lines. If the format is as expected, it performs the relevant enrichments on that data (e.g. referer parsing, geo-IP lookups), and writes the enriched data to the Out Bucket on S3, from where it can be loaded into Redshift / PostgreSQL. If the input line of data fails the validation, it gets written to the Bad Rows Bucket on S3.&lt;/p&gt;

&lt;p&gt;&lt;img alt='flow-chart' src='/static/img/blog/2013/09/snowplow-data-processing-bad-bucket-flow-chart-cropped.png' /&gt;&lt;/p&gt;

&lt;p&gt;The locations are specified in the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'&gt;EmrEtlRunner config file&lt;/a&gt;, an example of which can be found &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:s3&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:region&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:buckets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:assets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;s3://snowplow-hosted-assets&lt;/span&gt; &lt;span class='c1'&gt;# DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:log&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:in&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:processing&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE WITH SUB-FOLDER&lt;/span&gt; &lt;span class='c1'&gt;# e.g. s3://my-out-bucket/events&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out_bad_rows&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# e.g. s3://my-out-bucket/bad-rows&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out_errors&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Leave blank unless :continue_on_unexpected_error: set to true below&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:archive&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Each bad row is a JSON containing just two fields:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A field called &lt;code&gt;line&lt;/code&gt; (of type String), which is the &lt;em&gt;raw&lt;/em&gt; line of data from the collector log&lt;/li&gt;

&lt;li&gt;A field called &lt;code&gt;errors&lt;/code&gt; (an Array of Strings), which includes an error message for &lt;em&gt;every&lt;/em&gt; validation test the line failed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example row generated for the Snowplow website, caused by Amazon&amp;#8217;s CloudFront log file format update, is shown below (formatted to make it easier to read):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2013-08-19\t04:06:09\tHKG50\t826\t175.159.22.201\tGET\td3v6ndkyapxc2w.cloudfront.net\t/i\t200\thttp://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html\tMozilla/5.0%20(Macintosh;%20Intel%20Mac%20OS%20X%2010_6_8)%20AppleWebKit/534.57.2%20(KHTML,%20like%20Gecko)%20Version/5.1.7%20Safari/534.57.2\te=pv&amp;amp;page=Market%20basket%20analysis%20-%20identifying%20products%20and%20content%20that%20go%20well%20together%20-%20Snowplow%20Analytics&amp;amp;dtm=1376885168897&amp;amp;tid=479753&amp;amp;vp=1361x678&amp;amp;ds=1346x6578&amp;amp;vid=1&amp;amp;duid=24210ca58692c76e&amp;amp;p=web&amp;amp;tv=js-0.12.0&amp;amp;fp=421731260&amp;amp;aid=snowplowweb&amp;amp;lang=en-us&amp;amp;cs=UTF-8&amp;amp;tz=Asia%2FShanghai&amp;amp;refr=http%3A%2F%2Fwww.google.com%2Furl%3Fsa%3Dt%26rct%3Dj%26q%3Dmarket%2520basket%2520analysis%2520apriori%2520algorithm%26source%3Dweb%26cd%3D9%26sqi%3D2%26ved%3D0CGgQFjAI%26url%3Dhttp%253A%252F%252Fsnowplowanalytics.com%252Fanalytics%252Fcatalog-analytics%252Fmarket-basket-analysis-identifying-products-that-sell-well-together.html%26ei%3DnZkRUp_UF4qdiAem-YHwAg%26usg%3DAFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ%26sig2%3DFPZRScoJkUEg5G2qa8BoBA%26bvm%3Dbv.50768961%2Cd.aGc%26cad%3Drjt&amp;amp;f_pdf=1&amp;amp;f_qt=1&amp;amp;f_realp=0&amp;amp;f_wma=0&amp;amp;f_dir=0&amp;amp;f_fla=1&amp;amp;f_java=1&amp;amp;f_gears=0&amp;amp;f_ag=0&amp;amp;res=1440x900&amp;amp;cd=24&amp;amp;cookie=1&amp;amp;url=http%3A%2F%2Fsnowplowanalytics.com%2Fanalytics%2Fcatalog-analytics%2Fmarket-basket-analysis-identifying-products-that-sell-well-together.html\t-\tRefreshHit\tmEPXmPmaMHvqTD6ung3_IlOgVuNOLnliGz9mVYn29oyOPMDadhuQpQ==&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
    &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;Provided URI string [http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=market basket analysis apriori algorithm&amp;amp;source=web&amp;amp;cd=9&amp;amp;sqi=2&amp;amp;ved=0CGgQFjAI&amp;amp;url=http://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html&amp;amp;ei=nZkRUp_UF4qdiAem-YHwAg&amp;amp;usg=AFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ&amp;amp;sig2=FPZRScoJkUEg5G2qa8BoBA&amp;amp;bvm=bv.50768961,d.aGc&amp;amp;cad=rjt] violates RFC 2396: [Illegal character in query at index 45: http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=market basket analysis apriori algorithm&amp;amp;source=web&amp;amp;cd=9&amp;amp;sqi=2&amp;amp;ved=0CGgQFjAI&amp;amp;url=http://snowplowanalytics.com/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html&amp;amp;ei=nZkRUp_UF4qdiAem-YHwAg&amp;amp;usg=AFQjCNE8XEB-2ItaXcOC5i2T-jLvpv77uQ&amp;amp;sig2=FPZRScoJkUEg5G2qa8BoBA&amp;amp;bvm=bv.50768961,d.aGc&amp;amp;cad=rjt]&amp;quot;&lt;/span&gt;
    &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;a name='processing-bad-rows-data-using-json-serde-hive-qubole'&gt;&lt;h2&gt;2. Processing the bad rows data using the JSON serde, Hive and Qubole&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;There are a couple of ways to process JSON data in Hive. For this tutorial, we&amp;#8217;re going to use Roberto Congiu&amp;#8217;s &lt;a href='https://github.com/rcongiu/Hive-JSON-Serde'&gt;Hive-JSON-Serde&lt;/a&gt;. This is our preferred method of working with JSONs in Hive, where your complete data set is stored as a series of JSONs. (When you have a single JSON-formatted field in a regular Hive table, we recommend using the &lt;code&gt;get_json_object&lt;/code&gt; UDF to parse the JSON data.)&lt;/p&gt;

&lt;p&gt;The Hive-JSON-serde is available &lt;a href='https://github.com/rcongiu/Hive-JSON-Serde'&gt;on Github&lt;/a&gt; and can be built using Maven. If you prefer not to compile it for yourself, we have made a hosted version of the compiled JAR available &lt;a href='snowplow-hosted-assets.s3.amazonaws.com/third-party/rcongiu/json-serde-1.1.6-jar-with-dependencies.jar'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that we have placed the JSON serde in an S3 location that is accessible to us when we run Hive, we are in a position to fire up Qubole and start analyzing our bad rows data. Log into Qubole via the web UI to get started and open up the &lt;strong&gt;Composer&lt;/strong&gt; window. (If you have not tried Qubole yet, we recommend you &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-Qubole-to-analyze-Snowplow-data-using-Apache-Hive'&gt;read our guide to getting started with Qubole&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Now enter the following in the Qubole Composer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD JAR s3://snowplow-hosted-assets/third-party/rcongiu/json-serde-1.1.6-jar-with-dependencies.jar;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a short period Qubole should alert you that the JAR has been successfully uploaded:&lt;/p&gt;

&lt;p&gt;&lt;img alt='qubole-pic-1' src='/static/img/blog/2013/09/qubole-add-jar.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now we need to define a table so that Hive can query our bad row data in S3. Execute the following query in the Qubole Composer, making sure that you update the &lt;code&gt;LOCATION&lt;/code&gt; setting to point to the location in S3 where your bad rows are stored. (This can be worked out from your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file, as explained &lt;a href='#how-snowplow-handles-bad-rows'&gt;above&lt;/a&gt;).&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;CREATE&lt;/span&gt; &lt;span class='n'&gt;EXTERNAL&lt;/span&gt; &lt;span class='k'&gt;TABLE&lt;/span&gt; &lt;span class='ss'&gt;`bad_rows`&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='n'&gt;line&lt;/span&gt; &lt;span class='n'&gt;string&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;errors&lt;/span&gt; &lt;span class='n'&gt;array&lt;/span&gt;&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='n'&gt;string&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; 
&lt;span class='n'&gt;PARTITIONED&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;run&lt;/span&gt; &lt;span class='n'&gt;string&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;ROW&lt;/span&gt; &lt;span class='n'&gt;FORMAT&lt;/span&gt; &lt;span class='n'&gt;SERDE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;org.openx.data.jsonserde.JsonSerDe&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;STORED&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;TEXTFILE&lt;/span&gt;
&lt;span class='n'&gt;LOCATION&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s3n://snowplow-data/snplow/bad-rows/&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt='qubole-pic-2' src='/static/img/blog/2013/09/qubole-create-table.png' /&gt;&lt;/p&gt;

&lt;p&gt;Our table is partitioned by &lt;code&gt;run&lt;/code&gt; - each time the Snowplow enrichment process is run (in our case daily), any bad rows are saved in their own separate subfolder labelled &lt;code&gt;run=2013-xx-xx...&lt;/code&gt;. Let&amp;#8217;s recover those partitions, by executing the following:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;ALTER&lt;/span&gt; &lt;span class='k'&gt;TABLE&lt;/span&gt; &lt;span class='ss'&gt;`bad_rows`&lt;/span&gt; &lt;span class='n'&gt;RECOVER&lt;/span&gt; &lt;span class='n'&gt;PARTITIONS&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;a name='plot-bad-rows-over-time'&gt;&lt;h2&gt;3. Plotting the number of bad rows over time&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We run the Snowplow ETL once a day. As a result, each &amp;#8220;run&amp;#8221; represents one days worth of data. By counting the number of bad rows per run, we effectively calculate the number of bad rows of data generated per day. We can do that by executing the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt; 
&lt;span class='n'&gt;run&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='ss'&gt;`bad_rows`&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;run&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt='qubole-pic-3' src='/static/img/blog/2013/09/qubole-execute-count-query.png' /&gt;&lt;/p&gt;

&lt;p&gt;Execute that in Qubole, and then download your results. (By clicking the &lt;strong&gt;Download&lt;/strong&gt; link in the UI. If you open them in Excel, you should see something as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Run ID&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Number of bad rows&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-08-17-03-00-02&lt;/td&gt;&lt;td style='text-align: left;'&gt;6&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-08-18-03-00-03&lt;/td&gt;&lt;td style='text-align: left;'&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-08-19-03-00-03&lt;/td&gt;&lt;td style='text-align: left;'&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;(We have added the headers to the table above - these will not be downloaded)&lt;/p&gt;

&lt;p&gt;We can plot the data directly in Excel:&lt;/p&gt;

&lt;p&gt;&lt;img alt='excel-graph' src='/static/img/blog/2013/09/excel-graph-of-bad-rows-per-day.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Notice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have &lt;em&gt;no&lt;/em&gt; bad rows before August 17th, when Amazon updated their Cloudfront log format&lt;/li&gt;

&lt;li&gt;We then have bad rows every day since. (In our case, this varies between 2-25. This is on the Snowplow site, which attracts c.200 uniques per day.)&lt;/li&gt;
&lt;/ul&gt;
&lt;a name='processing-bad-rows'&gt;&lt;h2&gt;4. Reprocessing bad rows&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Using plots like the one above to spot emerging problems with your Snowplow data pipeline is one thing. When you&amp;#8217;ve identified the cause of the problem, and fixed it (as we have), you then need to reprocess those bad lines of data.&lt;/p&gt;

&lt;p&gt;Fortunately, this is pretty straightforward. We need to extract the bad lines out of the JSONs, and write them back into a new location in S3 in their raw form. We can then set the &lt;code&gt;IN&lt;/code&gt; bucket on the EmrEtlRunner to point to this new location, and run the updated Enrichment process on the data.&lt;/p&gt;

&lt;p&gt;To extract the raw lines of data out of the JSONs, we first create another external table in Hive, this time in the location where we will save the data to be reprocessed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;CREATE&lt;/span&gt; &lt;span class='n'&gt;EXTERNAL&lt;/span&gt; &lt;span class='k'&gt;TABLE&lt;/span&gt; &lt;span class='ss'&gt;`data_to_reprocess`&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='n'&gt;line&lt;/span&gt; &lt;span class='n'&gt;string&lt;/span&gt;  
&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;ROW&lt;/span&gt; &lt;span class='n'&gt;FORMAT&lt;/span&gt; &lt;span class='n'&gt;DELIMITED&lt;/span&gt;
&lt;span class='k'&gt;LINES&lt;/span&gt; &lt;span class='k'&gt;TERMINATED&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;STORED&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;TEXTFILE&lt;/span&gt;
&lt;span class='n'&gt;LOCATION&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s3n://qubole-analysis/data-to-reprocess/snplow/2013-09-11/&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&amp;#8217;ve created our table in the special bucket that we&amp;#8217;ve given Qubole unrestricted write access to&lt;/li&gt;

&lt;li&gt;We&amp;#8217;ve created a specific folder in that bucket for the new data, so it will be easy to find later&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we&amp;#8217;ve created our table, we need to insert into it the bad rows to reprocess:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;INSERT&lt;/span&gt; &lt;span class='k'&gt;INTO&lt;/span&gt; &lt;span class='k'&gt;TABLE&lt;/span&gt; &lt;span class='ss'&gt;`data_to_reprocess`&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt; &lt;span class='n'&gt;line&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='ss'&gt;`bad_rows`&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how we are &lt;strong&gt;only&lt;/strong&gt; writing the actual raw line of data into the new table (and ignoring everything else in the &lt;code&gt;bad_rows&lt;/code&gt; table, including both the &lt;code&gt;run&lt;/code&gt; and the actual error message itself).&lt;/p&gt;

&lt;p&gt;Bingo! When the query is complete, the data to reprocess is available in the new bucket we&amp;#8217;ve created:&lt;/p&gt;

&lt;p&gt;&lt;img alt='s3-pic' src='/static/img/blog/2013/09/file_with_lines_of_data_to_reprocess.png' /&gt;&lt;/p&gt;

&lt;p&gt;We now need to run the Snowplow Enrichment process on this new data set. We do that using EmrEtlRunner. Navigate to the server you run EmrEtlRunner from, and navigate to the directory it is installed in.&lt;/p&gt;

&lt;p&gt;Now, create a copy of your &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'&gt;EmrEtlRunner config.yml&lt;/a&gt; with a suitable name e.g. &lt;code&gt;config-process-bad-rows-2013-09-11.yml&lt;/code&gt; and update the In Bucket to point to the location of the the data to be reprocessed is (i.e. the location of the Hive &lt;code&gt;data_to_reprocess&lt;/code&gt; table). Don&amp;#8217;t forget as well to update (if you haven&amp;#8217;t already done so) the ETL to the latest version, which can handle the change in Amazon&amp;#8217;s CloudFront log file format:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.4&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now execute the following command at the command line:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;bundle &lt;span class='nb'&gt;exec &lt;/span&gt;bin/snowplow-emr-etl-runner --config config/config-process-bad-rows-2013-09-11.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure you update the path to point at the name of the config file you created in the previous step. This should kick off the Enrichment process in EMR. Once it has been completed, you can run the StorageLoader to load the newly processed data into Redshift / PostgreSQL as normal:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;&lt;span class='nb'&gt;cd&lt;/span&gt; ../../4-storage/storage-loader
&lt;span class='nv'&gt;$ &lt;/span&gt;bundle &lt;span class='nb'&gt;exec &lt;/span&gt;bin/snowplow-storage-loader --config config/config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done! The data that was previously excluded has now been added to your Snowplow database!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/09/05/snowplow-0.8.9-released-to-handle-cloudfront-log-file-format-change"/>
    <title>Snowplow 0.8.9 released to handle CloudFront log file format change</title>
    <updated>2013-09-05T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate availability of Snowplow 0.8.9. This release was necessitated by an unannounced change Amazon made to the CloudFront access log file format on 17th August, discussed in this &lt;a href='https://forums.aws.amazon.com/thread.jspa?messageID=484509&amp;amp;#484509'&gt;AWS Forum thread&lt;/a&gt; and this &lt;a href='https://groups.google.com/forum/#!topic/snowplow-user/HWeSkiiXbdQ'&gt;snowplow-user email thread&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Essentially, Amazon switched from URL-encoding all &amp;#8221;%&amp;#8221;&amp;#8221; signs found in the &lt;code&gt;cs-uri-query&lt;/code&gt; field, to only URL-encoding them if they were not already escaped, i.e. were not followed by &amp;#8220;25&amp;#8221; (&amp;#8220;%25&amp;#8221;). This unannounced change was in contradiction to the existing &lt;a href='http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat'&gt;CloudFront access log technical specification&lt;/a&gt;, which was not updated.&lt;/p&gt;

&lt;p&gt;Snowplow expects &amp;#8221;%&amp;#8221; signs to be double-encoded, and so decodes them twice. Unfortunately, if a URL contains a space (e.g. in a referer search term), then constructing a URI object from this double-decoded string will fail, the event will be rejected by Snowplow validation and be logged to the bad-rows bucket (not loaded into Redshift). Assuming you are using the CloudFront collector, if you check your bad-rows bucket for Snowplow ETL runs before and after August 17th, expect to see a large increase in the number of bad-rows after the 17th.&lt;/p&gt;

&lt;p&gt;On 5th September, Amazon decided to reverse this change, but this still leaves Snowplow users with three weeks&amp;#8217; worth of CloudFront logs in the wrong format. Therefore, we have released Snowplow 0.8.9, which adds support for both single- and double-encoded &amp;#8221;%&amp;#8221; signs in the &lt;code&gt;cs-uri-query&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;Assuming you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest version of the Hadoop ETL:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.4&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that&amp;#8217;s it! You may also want to re-run your Snowplow process against your CloudFront logs starting from the 17th August to recover the events wrongly identified as &amp;#8220;bad rows&amp;#8221;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/09/03/using-qubole-to-analyze-snowplow-web-data"/>
    <title>Using Qubole to crunch your Snowplow web data using Apache Hive</title>
    <updated>2013-09-03T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;ve just published a getting-started guide to using &lt;a href='http://www.qubole.com/#All'&gt;Qubole&lt;/a&gt;, a managed Big Data service, to query your Snowpow data. You can read the guide &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-Qubole-to-analyze-Snowplow-data-using-Apache-Hive'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img alt='qubole-logo' src='/static/img/blog/2013/09/qubole-logo.png' /&gt;&lt;/p&gt;

&lt;p&gt;Snowplow delivers event data to users in a number of different places:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Amazon Redshift or PostgreSQL, so you can analyze the data using traditional analytics and BI tools&lt;/li&gt;

&lt;li&gt;Amazon S3, so you can analyze that data using Hadoop-backed, big data tools e.g. &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt;, &lt;a href='http://hive.apache.org/'&gt;Hive&lt;/a&gt; and &lt;a href='http://pig.apache.org/'&gt;Pig&lt;/a&gt;, on EMR&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since we started offering support for Amazon Redshift and more recently PostgreSQL, our focus on the blog and in the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; has been on using traditional analytics tools e.g. &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt;, &lt;a href='http://cran.r-project.org/'&gt;R&lt;/a&gt; and &lt;a href='http://office.microsoft.com/en-gb/excel/'&gt;Excel&lt;/a&gt; to crunch the data. However, there are a host of reasons when you might want to crunch the data using one of the new generation of big data tools. Two give two examples:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You may want to join your Snowplow data with other data sets, and those data sets are not structured. (E.g. they are in JSON, or custom text file formats.)&lt;/li&gt;

&lt;li&gt;You want to use specific algorithms or libraries that have been built for big data tools e.g. Mahout recommendation or clustering algorithms.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;
&lt;p&gt;For situations when you want to use big data tools to crunch your Snowplow and other data in S3, an increasingly attractive alternative to doing that in EMR is to use &lt;a href='http://www.qubole.com/#All'&gt;Qubole&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img alt='qubole-ui' src='/static/img/blog/2013/09/qubole-ui.png' /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://www.qubole.com/#All'&gt;Qubole&lt;/a&gt; runs on top of Amazon Web Services. It interfaces directly with S3, in just the same way that EMR does. However, Qubole is a significantly more polished product than EMR. Data scientists can explore their data in S3, create tables and query those tables all via an easy-to-use web UI. You can test queries on samples of the data, easily run and monitor multiple queries in parallel and download the results of queries directly to your local computer, so you can quickly visualize it in Excel or R.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s not just the UI that makes using Qubole a lot nicer than EMR. Qubole handles the behind the scenes scaling of your cluster, so you don&amp;#8217;t have to. Queries that do not require map reduce jobs to produce outputs, e.g. reading data directly from S3, can be executed without spinning up clusters (and so return much faster). Clusters that are no longer used, are automatically shut down. (Am I the only person who&amp;#8217;s got in trouble for leaving a big cluster running over night?)&lt;/p&gt;

&lt;p&gt;Qubole is a particular nice service if you want to use Apache Hive. It was developed by the same engineers who originally built Apache Hive at Facebook. In the &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-Qubole-to-analyze-Snowplow-data-using-Apache-Hive'&gt;guide to getting started with Qubole&lt;/a&gt;, which we have just published, we walk Snowplow users through the process of running their first query Apache Hive query on Snowplow data with Qubole.&lt;/p&gt;

&lt;p&gt;We intend to follow-up the getting started guide with a set of recipes for using both &lt;a href='http://hive.apache.org/'&gt;Hive&lt;/a&gt; and &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; via Qubole. Stay tuned!&lt;/p&gt;

&lt;p&gt;As always, we welcome comments and feedback - especially on how you find the combination of Qubole&amp;#8217;s data crunching capability, on Snowplow&amp;#8217;s granular event-level data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar"/>
    <title>Towards universal event analytics - building an event grammar</title>
    <updated>2013-08-12T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As we outgrow our &amp;#8220;fat table&amp;#8221; structure for Snowplow events in Redshift, we have been spending more time thinking about how we can model digital events in Snowplow in the most universal, flexible and future-proof way possible.&lt;/p&gt;

&lt;p&gt;When we blogged about &lt;a href='http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;building out the Snowplow event model&lt;/a&gt; earlier this year, a comment left on that post by &lt;a href='https://twitter.com/mglcel'&gt;Loc Dias Da Silva&lt;/a&gt; made us realize that we were missing an even more fundamental point: defining a Snowplow event &lt;strong&gt;grammar&lt;/strong&gt; to underpin our Snowplow event dictionary. Here is part of Loc&amp;#8217;s excellent comment - although I would encourage you to read it in full &lt;a href='http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;on the blog post&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hi, we&amp;#8217;re also working on an event model for our global eventing platform but our events currently are more macro, inspired by RDF in a sense:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;An Actor(id/type) made and Action(verb, context) on another Object(id/type).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Each Actor, Action and Object can hold k/v properties.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The context itself, owned by the action, is a k/v dictionary.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So in designing his event grammar, Loc was influenced by the &lt;a href='http://en.wikipedia.org/wiki/Resource_Description_Framework'&gt;Resource Description Framework&lt;/a&gt;, the W3C specifications for modelling relationships to web resources.&lt;/p&gt;

&lt;p&gt;An event grammar inspired by RDF is certainly interesting, but I am using a much older, more sophisticated and more tested &amp;#8220;event grammar&amp;#8221; to write this sentence: the &lt;strong&gt;grammar of human language&lt;/strong&gt;. Why not start, then, from the core grammar underpinning English, Latin, Greek, German and other languages to see just how far this approach can take us in modelling events in the digital world?&lt;/p&gt;

&lt;p&gt;So, in the rest of this post we will:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#grammar'&gt;Introduce the components of our grammar&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#ecommerce'&gt;Model some ecommerce events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#videogame'&gt;Model some videogame events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#media'&gt;Model some digital media events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#learnings'&gt;Discuss what we have learnt&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/12/towards-universal-event-analytics-building-an-event-grammar#conc'&gt;Draw some conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='grammar'&gt; &lt;/a&gt;&lt;h2&gt;1. The components of our grammar&lt;/h2&gt;
&lt;p&gt;All of the human languages mentioned above (and many, many others) share the same fundamental building blocks in their grammars for describing an event with a verb in the &lt;em&gt;active voice&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='grammar' src='/static/img/blog/2013/08/event-grammar.png' /&gt;&lt;/p&gt;

&lt;p&gt;To go through these in turn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Subject&lt;/strong&gt;, or noun in the &lt;em&gt;nominative&lt;/em&gt; case. This is the entity which is carrying out the action: &amp;#8221;&lt;strong&gt;I&lt;/strong&gt; wrote a letter&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Verb&lt;/strong&gt;, this describes the action being done by the Subject: &amp;#8220;I &lt;strong&gt;wrote&lt;/strong&gt; a letter&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Direct Object&lt;/strong&gt;, or simply &lt;em&gt;Object&lt;/em&gt; or noun in the &lt;em&gt;accusative&lt;/em&gt; case. This is the entity to which the action is being done: &amp;#8220;I wrote &lt;strong&gt;a letter&lt;/strong&gt;&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Indirect Object&lt;/strong&gt;, or noun in the &lt;em&gt;dative&lt;/em&gt; case. A slightly more tricky concept: this is the entity indirectly affected by the action: &amp;#8220;I sent the letter &lt;em&gt;to&lt;/em&gt; &lt;strong&gt;Tom&lt;/strong&gt;&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Prepositional Object&lt;/strong&gt;. An object introduced by a preposition (in, for, of etc), but not the direct or indirect object: &amp;#8220;I put the letter &lt;em&gt;in&lt;/em&gt; &lt;strong&gt;an envelope&lt;/strong&gt;&amp;#8221;. In a language such as German, prepositional objects will be found in the &lt;em&gt;accusative&lt;/em&gt;, &lt;em&gt;dative&lt;/em&gt; or &lt;em&gt;genitive&lt;/em&gt; case depending on the preposition used&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;. Not a grammatical term, but we will use context to describe the phrases of time, manner, place and so on which provide additional information about the action being performed: &amp;#8220;I posted the letter &lt;strong&gt;on Tuesday from Boston&lt;/strong&gt;&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these grammatical building blocks defined, let&amp;#8217;s now put them through their paces modelling some digital events - starting with some online retail events:&lt;/p&gt;
&lt;a name='ecommerce'&gt; &lt;/a&gt;&lt;h2&gt;2. Modelling some ecommerce events&lt;/h2&gt;
&lt;p&gt;Here are some ecommerce events mapped to our grammatical model:&lt;/p&gt;

&lt;p&gt;&lt;img alt='ecomm1' src='/static/img/blog/2013/08/grammar-ecomm1.png' /&gt;&lt;/p&gt;

&lt;p&gt;In this event, a shopper (Subject) views (Verb) a t-shirt (Direct Object) while browsing an online store (Context).&lt;/p&gt;

&lt;p&gt;&lt;img alt='ecomm2' src='/static/img/blog/2013/08/grammar-ecomm2.png' /&gt;&lt;/p&gt;

&lt;p&gt;Here we introduce an Indirect Object which has been affected by the event: the shopper (Subject) adds (Verb) a t-shirt (Direct Object) to her shopping basket (Indirect Object). Again, this is while browsing the online store (Context).&lt;/p&gt;

&lt;p&gt;&lt;img alt='ecomm3' src='/static/img/blog/2013/08/grammar-ecomm3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Here we have an Object introduced by preposition: the shopper (Subject) pays (Verb) for his order (Prepositional Object). This is all within the checkout flow (Context).&lt;/p&gt;
&lt;a name='videogame'&gt; &lt;/a&gt;&lt;h2&gt;3. Modelling some videogame events&lt;/h2&gt;
&lt;p&gt;So far so good, but how well does this model work with events generated by a gaming session?&lt;/p&gt;

&lt;p&gt;&lt;img alt='videogame1' src='/static/img/blog/2013/08/grammar-videogame1.png' /&gt;&lt;/p&gt;

&lt;p&gt;In a gifting screen within the game (Context), the player (Subject) gifts (Verb) some gold (Direct Object) to another player (Indirect Object).&lt;/p&gt;

&lt;p&gt;&lt;img alt='videogame2' src='/static/img/blog/2013/08/grammar-videogame2.png' /&gt;&lt;/p&gt;

&lt;p&gt;During a two-player skirmish (Context), the first player (Subject) kills (Verb) the second player (Direct Object) using a nailgun (Prepositional Object). This illustrates how your end-users can be the Object of events, not just their Subjects.&lt;/p&gt;

&lt;p&gt;&lt;img alt='videogame3' src='/static/img/blog/2013/08/grammar-videogame3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Here we illustrate a reflexive verb: through grinding (Context), the player (Subject) levels herself up (Verb, reflexive). A reflexive Verb is one where the Subject and the Object are the same.&lt;/p&gt;
&lt;a name='media'&gt; &lt;/a&gt;&lt;h2&gt;4. Modelling some digital media events&lt;/h2&gt;
&lt;p&gt;This seems to be working well! Finally, let&amp;#8217;s map our new event grammar onto the world of digital media and publishing:&lt;/p&gt;

&lt;p&gt;&lt;img alt='media1' src='/static/img/blog/2013/08/grammar-media1.png' /&gt;&lt;/p&gt;

&lt;p&gt;While consuming media on your site (Context), a user (Subject) reads (Verb) an article (Direct Object).&lt;/p&gt;

&lt;p&gt;&lt;img alt='media2' src='/static/img/blog/2013/08/grammar-media2.png' /&gt;&lt;/p&gt;

&lt;p&gt;Wanting to share content socially (Context), a user (Subject) shares (Verb) a video (Direct Object) on Twitter (Prepositional Object). Also note that Twitter here is a proper noun (not a common noun).&lt;/p&gt;

&lt;p&gt;&lt;img alt='media3' src='/static/img/blog/2013/08/grammar-media3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Working from the moderation UI (Context), an administrator (Subject) bans (Verb) user #23 (Direct Object). This illustrates how an end-user can be the Object of an event, and how someone other than an end-user can be the Subject of the event.&lt;/p&gt;
&lt;a name='learnings'&gt; &lt;/a&gt;&lt;h2&gt;5. What have we learnt&lt;/h2&gt;
&lt;p&gt;As you can see, it is relatively straightforward to map any of the digital events above into these six &amp;#8220;slots&amp;#8221; of: Subject, Verb, Object, Indirect Object, Prepositional Object and Context. This is unsurprising: our core grammar has been unambiguously describing events in many different human languages across thousands of years.&lt;/p&gt;

&lt;p&gt;Going through the above exercise, several further things have become clear to us that we will want to factor into the Snowplow event grammar going forwards:&lt;/p&gt;

&lt;h3 id='implicit_subjects_are_a_mistake'&gt;Implicit Subjects are a mistake&lt;/h3&gt;

&lt;p&gt;Most web and event analytics systems make the mistake of making the Subject of the event implicit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(End user) adds product to basket
(Admin) bans user #23&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a mistake, because as we have seen above, expressing the Subject is a key component of our event grammar.&lt;/p&gt;

&lt;p&gt;Going further, it is particularly dangerous to assume that the Subject of every event is your end-user or customer, because we have seen cases where this is not the case.&lt;/p&gt;

&lt;h3 id='an_entity_can_be_subject_or_object_or_both_across_multiple_events'&gt;An entity can be Subject or Object or both across multiple events&lt;/h3&gt;

&lt;p&gt;As per these gaming examples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;User #1 gifts gold to user #2
User #2 kills user #3
User #2 levels up
Admin bans user #1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we can see from this, the same entities will be found as Subject, Direct Object, Indirect Object or Prepositional Object depending on the event.&lt;/p&gt;

&lt;p&gt;Most analytics systems miss the fact that an end-user (for example) is not merely the implicit Subject of multiple events, but is in fact an entity which is the Subject and the Object of different events.&lt;/p&gt;

&lt;h3 id='we_can_keep_our_verbs_really_simple'&gt;We can keep our Verbs really simple&lt;/h3&gt;

&lt;p&gt;All of the events above were modelled simply using verbs in the &lt;em&gt;active voice&lt;/em&gt;, not the &lt;em&gt;passive voice&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Active voice: &amp;#8220;I watch a video&amp;#8221;&lt;/li&gt;

&lt;li&gt;Passive voice: &amp;#8220;the video was watched by Alex&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We don&amp;#8217;t need to use passive voice for our event model, because we can always derive (if needed) a passive voice event from our active voice event.&lt;/p&gt;

&lt;p&gt;Going further, Verbs conjugate in lots of other ways (tense, person, mood etc) - but again we don&amp;#8217;t need to include any of this into our event model: all of this can be derived (if needed) from our event&amp;#8217;s Context.&lt;/p&gt;

&lt;h3 id='context_is_king'&gt;Context is king&lt;/h3&gt;

&lt;p&gt;Our idea of Context does not map cleanly onto a singular grammatical component, but it is just too useful to exclude. In fact, de facto we already have a rich web context for Snowplow events in our &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;Canonical event model&lt;/a&gt;, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When the event occurred&lt;/li&gt;

&lt;li&gt;Where (geographically) the event occurred&lt;/li&gt;

&lt;li&gt;Properties of the device on which the event occurred&lt;/li&gt;
&lt;/ul&gt;
&lt;a name='learnings'&gt; &lt;/a&gt;&lt;h2&gt;6. Conclusions&lt;/h2&gt;
&lt;p&gt;We hope this has been an interesting exploration of how we can potentially adapt and simplify the grammar of human languages to express a new grammar for digital events. We are really excited about the possibilities this opens up - initially around expressing such a grammar in our new &lt;a href='http://avro.apache.org/'&gt;Avro&lt;/a&gt; event model, and later hopefully in graph databases such as &lt;a href='http://www.neo4j.org/'&gt;Neo4J&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course, we have only just started to sketch out this new event model, and we hope that it will prompt a wider debate with the Snowplow and analytics communities. We are excited to evolve these ideas and build a model for universal event analytics with you, together - and we look forward to continuing the conversation on our &lt;a href='https://groups.google.com/d/forum/snowplow-user'&gt;snowplow-user mailing list&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And finally, many thanks again to &lt;a href='https://twitter.com/mglcel'&gt;Loc Dias Da Silva&lt;/a&gt; for sharing his original Actor-Action-Object idea on our blog!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support"/>
    <title>Snowplow 0.8.8 released with Postgres and Hive support</title>
    <updated>2013-08-05T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate release of Snowplow 0.8.8. This is a big release for us: it adds the ability to store your Snowplow events in the popular &lt;a href='http://www.postgresql.org/'&gt;PostgreSQL&lt;/a&gt; open-source database. This has been the most requested Snowplow feature all summer, so we are delighted to finally release it.&lt;/p&gt;

&lt;p&gt;And if you are already happily using Snowplow with Redshift, there are two other new features to check out:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We have added support for multiple storage targets to Snowplow&amp;#8217;s StorageLoader. This means that you can configure StorageLoader to load into three different Redshift databases, one PostgreSQL database and one Redshift - whatever&lt;/li&gt;

&lt;li&gt;We have brought back the ability to query your Snowplow events using &lt;a href='http://hive.apache.org/'&gt;HiveQL&lt;/a&gt;. Regardless of which storage target(s) you are using, you can now also run HiveQL queries against your Snowplow events stored in Amazon S3&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As well as these new features, we have made a large number of improvements across Snowplow:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have made some improvements to the Hadoop-based Enrichment process (bumped to version 0.3.3)&lt;/li&gt;

&lt;li&gt;We have simplified EmrEtlRunner and its configuration file format&lt;/li&gt;

&lt;li&gt;We have improved the performance of the Redshift loading code&lt;/li&gt;

&lt;li&gt;We have added a configuration option for setting &lt;code&gt;MAXERROR&lt;/code&gt; when loading into Redshift (see the &lt;a href='http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html'&gt;Redshift &lt;code&gt;COPY&lt;/code&gt; documentation&lt;/a&gt; for details)&lt;/li&gt;

&lt;li&gt;We have moved the Snowplow JavaScript Tracker into &lt;a href='https://github.com/snowplow/snowplow-javascript-tracker'&gt;its own repository&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We have removed the deprecated Hive ETL and Infobright folders from the repository&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After the fold, we will cover the options for upgrading and using the new functionality:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support#postgres'&gt;Loading events into Postgres&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support#hiveql'&gt;Querying events with HiveQL&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/08/05/snowplow-0.8.8-released-with-postgres-and-hive-support#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='upgrading'&gt;1. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;three components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Hadoop ETL, to version 0.3.3&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.4.0, and its configuration file&lt;/li&gt;

&lt;li&gt;StorageLoader, to version 0.1.0, and its configuration file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest version of the Hadoop ETL:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.3&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Read on for the rest of the changes you will need to make to &lt;code&gt;config.yml&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.8 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.8
$ cd snowplow/3-enrich/emr-etl-runner
$ bundle install --deployment&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, you need to update the format of your &lt;code&gt;config.yml&lt;/code&gt; - the format has been simplified significantly. The new format (&lt;a href='https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/config/config.yml.sample'&gt;as found on GitHub&lt;/a&gt;) looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:aws&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:access_key_id&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:secret_access_key&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:s3&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:region&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:buckets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:assets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;s3://snowplow-hosted-assets&lt;/span&gt; &lt;span class='c1'&gt;# DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:log&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:in&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:processing&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE WITH SUB-FOLDER&lt;/span&gt; &lt;span class='c1'&gt;# e.g. s3://my-out-bucket/events&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out_bad_rows&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;        &lt;span class='c1'&gt;# e.g. s3://my-out-bucket/bad-rows&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:out_errors&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Leave blank unless :continue_on_unexpected_error: set to true below&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:archive&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:emr&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='c1'&gt;# Can bump the below as EMR upgrades Hadoop&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;1.0.3&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:placement&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:ec2_key_name&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='c1'&gt;# Adjust your Hadoop cluster below&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:jobflow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:master_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:core_instance_count&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;2&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:core_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_count&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0&lt;/span&gt; &lt;span class='c1'&gt;# Increase to use spot instances&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_bid&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.015&lt;/span&gt; &lt;span class='c1'&gt;# In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:etl&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:job_name&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;SnowPlow ETL&lt;/span&gt; &lt;span class='c1'&gt;# Give your job a name&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.3&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:collector_format&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;cloudfront&lt;/span&gt; &lt;span class='c1'&gt;# Or &amp;#39;clj-tomcat&amp;#39; for the Clojure Collector&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:continue_on_unexpected_error&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;false&lt;/span&gt; &lt;span class='c1'&gt;# You can switch to &amp;#39;true&amp;#39; (and set :out_errors: above) if you really don&amp;#39;t want the ETL throwing exceptions&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the &lt;code&gt;:snowplow:&lt;/code&gt; section has been mostly removed, with &lt;code&gt;:hadoop_etl_version:&lt;/code&gt; moving into the &lt;code&gt;:etl:&lt;/code&gt; section.&lt;/p&gt;

&lt;h3 id='storageloader'&gt;StorageLoader&lt;/h3&gt;

&lt;p&gt;You need to upgrade your StorageLoader installation to the latest code (0.8.8 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.8
$ cd snowplow/4-storage/storage-loader
$ bundle install --deployment&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, you need to update the format of your &lt;code&gt;config.yml&lt;/code&gt; - the format has been updated to support multiple storage targets. The new format (&lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/config/redshift.yml.sample'&gt;as found on GitHub&lt;/a&gt;) looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:aws&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:access_key_id&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:secret_access_key&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:s3&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:region&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# S3 bucket region must be the same as Redshift cluster region&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:buckets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:in&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Must be s3:// not s3n:// for Redshift&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:archive&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:download&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:folder&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='c1'&gt;# Not required for Redshift&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:targets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;:name&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;My&lt;/span&gt;&lt;span class='nv'&gt; &lt;/span&gt;&lt;span class='s'&gt;Redshift&lt;/span&gt;&lt;span class='nv'&gt; &lt;/span&gt;&lt;span class='s'&gt;database&amp;quot;&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;redshift&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:host&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# The endpoint as shown in the Redshift console&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:database&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Name of database &lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:port&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;5439&lt;/span&gt; &lt;span class='c1'&gt;# Default Redshift port&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:table&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;events&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:username&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; 
    &lt;span class='l-Scalar-Plain'&gt;:password&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; 
    &lt;span class='l-Scalar-Plain'&gt;:maxerror&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;1&lt;/span&gt; &lt;span class='c1'&gt;# Stop loading on first error, or increase to permit more load errors&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the new &lt;code&gt;:maxerror:&lt;/code&gt; setting - see the &lt;a href='http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html'&gt;Redshift &lt;code&gt;COPY&lt;/code&gt; documentation&lt;/a&gt; for more on this.&lt;/p&gt;

&lt;p&gt;To add another Redshift storage target, just add another configuration block under &lt;code&gt;:targets:&lt;/code&gt;, starting with &lt;code&gt;- :name:&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To add a new Postgres storage target, read on&amp;#8230;&lt;/p&gt;
&lt;h2&gt;&lt;a name='postgres'&gt;2. Loading events into Postgres&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Loading events into Postgres is quite straightforward:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrade to the latest version of Snowplow &lt;a href='#upgrading'&gt;as described above&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;If you don&amp;#8217;t have one already, setup a Postgres database server&lt;/li&gt;

&lt;li&gt;Create a &lt;code&gt;snowplow&lt;/code&gt; database within Postgres&lt;/li&gt;

&lt;li&gt;Deploy the Snowplow schema and table into &lt;code&gt;snowplow&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;Configure StorageLoader to load into Postgres&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For help on steps 2-4, please see our new guide, &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-PostgreSQL'&gt;Setting up PostgreSQL&lt;/a&gt;. You can find the new PostgreSQL script on GitHub, &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/postgres-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For step 5, you should create a StorageLoader configuration file which looks like this (&lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/config/postgres.yml.sample'&gt;as found on GitHub&lt;/a&gt;):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:aws&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:access_key_id&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:secret_access_key&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:s3&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:region&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# S3 bucket region&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:buckets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:in&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:archive&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:download&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:folder&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Postgres-only config option. Where to store the downloaded files&lt;/span&gt;
&lt;span class='l-Scalar-Plain'&gt;:targets&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;:name&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;My&lt;/span&gt;&lt;span class='nv'&gt; &lt;/span&gt;&lt;span class='s'&gt;PostgreSQL&lt;/span&gt;&lt;span class='nv'&gt; &lt;/span&gt;&lt;span class='s'&gt;database&amp;quot;&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;postgres&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:host&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Hostname of database server&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:database&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; &lt;span class='c1'&gt;# Name of database &lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:port&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;5432&lt;/span&gt; &lt;span class='c1'&gt;# Default Postgres port&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:table&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;atomic.events&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:username&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; 
    &lt;span class='l-Scalar-Plain'&gt;:password&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;ADD HERE&lt;/span&gt; 
    &lt;span class='l-Scalar-Plain'&gt;:maxerror&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='c1'&gt;# Not required for Postgres&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure to set &lt;code&gt;:folder:&lt;/code&gt; to a local directory where you can download the Snowplow event files to, ready for loading into your local Postgres database server.&lt;/p&gt;
&lt;h2&gt;&lt;a name='hiveql'&gt;3. Querying events with HiveQL&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The new release makes it possible again to query your Snowplow events directly on Amazon S3 using &lt;a href='http://hive.apache.org/'&gt;HiveQL&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Steps are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrade to the latest version of Snowplow &lt;a href='#upgrading'&gt;as described above&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Wait for Snowplow to run at least once following the upgrade&lt;/li&gt;

&lt;li&gt;Follow the instructions in our updated guide, &lt;a href='https://github.com/snowplow/snowplow/wiki/Running-Hive-using-the-command-line-tools'&gt;Running Hive using the command line tools&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you want to run HiveQL queries across your historical event data (i.e. from before the upgrade), this is possible too. You will need to rename the timestamped folders in your event archive from the old format to the new format, by prepending &lt;code&gt;run=&lt;/code&gt;. So for example, change:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s3://my-snowplow-archive/events/2013-07-01-04-00-03&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s3://my-snowplow-archive/events/run=2013-07-01-04-00-03&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One this is done for all folders, all of your historic event files should be correctly partitioned ready for Hive to query.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For more details on this release, please check out the &lt;a href='https://github.com/snowplow/snowplow/releases/0.8.8'&gt;0.8.8 Release Notes&lt;/a&gt; on GitHub.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/19/snowplow-presentation-to-hadoop-user-group-london-aws-event</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/19/snowplow-presentation-to-hadoop-user-group-london-aws-event"/>
    <title>Snowplow presentation at the Hadoop User Group London AWS event</title>
    <updated>2013-07-19T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Yesterday at the &lt;a href='http://www.meetup.com/hadoop-users-group-uk/'&gt;Hadoop User Group&lt;/a&gt;, I was very fortunate to get the opportunity to speak about Snowplow at the event focused specifically on Amazon Web Services, and Redshift in particular.&lt;/p&gt;

&lt;p&gt;I hope the talk was interesting to the participants who attended. I described how we use Cloudfront and Elastic Beanstalk to get event data into AWS for processing by EMR, and how we push the output of our EMR-based enrichment process into Redshift for analysis. The slides I presented are below:&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/24416560' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/yalisassoon/snowplow-presentation-to-hug-uk' target='_blank' title='Snowplow presentation to hug uk'&gt;Snowplow presentation to hug uk&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/yalisassoon' target='_blank'&gt;yalisassoon&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;Many thanks to Dan Harvey for organising the event, Ian and Ianni from Amazon for their presentations, and Amazon for sponsoring the event!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model"/>
    <title>Help us build out the Snowplow Total Cost of Ownership Model</title>
    <updated>2013-07-10T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In a &lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'&gt;previous blog post&lt;/a&gt;, we described how we were in the process of building a &lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'&gt;Total Cost of Ownership model&lt;/a&gt; for Snowplow: something that would enable a Snowplow user, or prospective user, to accurately forecast their AWS bill going forwards based on their traffic levels.&lt;/p&gt;

&lt;p&gt;&lt;img alt='your-country-needs-you' src='/static/img/blog/2013/07/your-country-needs-you.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;To build that model, though, we need &lt;strong&gt;your help&lt;/strong&gt;. In order to ensure that our model is accurate and robust, we need to make sure that the relationships we believe exist between the number of events tracked, and the number and size of files generated, as detailed in the &lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/'&gt;last post&lt;/a&gt;, are correct, and that we have modelled them accurately. To that end, we are asking Snowplow users to help us by providing the following data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#events-per-day'&gt;The number of events tracked per day&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#runs-per-day'&gt;The number of times the enrichment process is run per day&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#log-files-per-day'&gt;The number of Cloudfront log files generated per day, and the volume of data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#emr-details'&gt;The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#output-back-to-s3'&gt;The number of files outputted back to S3, and the size of those files&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/#redshift-data-points'&gt;The total number of lines of data in Redshift, and the amount of Redshift capacity used&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will then share this data back, in an anonymized form, with the community, as part of the model.&lt;/p&gt;

&lt;p&gt;We recognise that that is a fair few data points! To thank Snowplow users for their trouble in providing them (as well as building a model for you), we will &lt;em&gt;also&lt;/em&gt; send each person that provides data a &lt;strong&gt;free Snowplow T-shirt&lt;/strong&gt; in their size.&lt;/p&gt;

&lt;p&gt;In the rest of this post, we provide simple instructions for pulling the relevant data from Amazon.&lt;/p&gt;
&lt;!--more--&gt;&lt;h3&gt;&lt;a name='events-per-day'&gt;1. Calculating the number of events tracked per day&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Simply execute the following SQL statement in Redshift&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;to_char&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;collector_tstamp&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;YYYY-MM-DD&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;&amp;quot;Day&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;&amp;quot;Number of events&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;collector_tstamp&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='err'&gt;{$&lt;/span&gt;&lt;span class='k'&gt;START&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='nb'&gt;DATE&lt;/span&gt;&lt;span class='err'&gt;}&lt;/span&gt;
&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;collector_tstamp&lt;/span&gt;&lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='err'&gt;{$&lt;/span&gt;&lt;span class='k'&gt;START&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='nb'&gt;DATE&lt;/span&gt;&lt;span class='err'&gt;}&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;&amp;quot;Day&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3&gt;&lt;a name='runs-per-day'&gt;2. Calculating the number of times the enrichment process is run per day&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Most Snowplow users run the enrichment process once per day.&lt;/p&gt;

&lt;p&gt;You can confirm how many times you run Snowplow by logging into the AWS S3 console and navigating to the bucket where you archive your Snowplow event files. (This is specified in the &lt;a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'&gt;StorageLoader config file&lt;/a&gt;.) Within the bucket you&amp;#8217;ll see a single folder generated for each enrichment &amp;#8216;run&amp;#8217;, labelled with the timestamp of the run. You&amp;#8217;ll be able to tell directly how many times the enrichment process is run - in the below case - it is once per day:&lt;/p&gt;

&lt;p&gt;&lt;img alt='aws-s3-screenshot' src='/static/img/blog/2013/07/number-of-runs-per-day.png' /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a name='log-files-per-day'&gt;3. Measuring The number of Cloudfront log files generated per day, adn the volume of data&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is most easily done using an S3 front end, as the AWS S3 console is a bit limited. We use &lt;a href='http://www.cloudberrylab.com/'&gt;Cloudberry&lt;/a&gt;. On Cloudberry, you can read the number of files generated per day, and their size, directly, by simply right clicking on the folder with the day&amp;#8217;s worth of log file archives and selecting properties:&lt;/p&gt;

&lt;p&gt;&lt;img alt='number-of-collector-logs-and-size' src='/static/img/blog/2013/07/number-of-collector-logs-and-size.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;In the above case we see there were 370 files generated on 2013-07-08, which occupied a total of 366.5KB.&lt;/p&gt;
&lt;h3&gt;&lt;a name='emr-details'&gt;4. The amount of time taken to enrich the data in EMR (and the size of cluster used to perform the enrichment)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;You can use the EMR command line tools to generate a JSON with details of each EMR job. In the below example, we pull a JSON for a specific job:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;./elastic-mapreduce --describe --jobflow j-Y9QNJI44PA0X
&lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;JobFlows&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
    &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='s2'&gt;&amp;quot;Instances&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;TerminationProtected&amp;quot;&lt;/span&gt;: &lt;span class='nb'&gt;false&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;MasterInstanceId&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;i-944414d9&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;HadoopVersion&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;1.0.3&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;NormalizedInstanceHours&amp;quot;&lt;/span&gt;: 2,
        &lt;span class='s2'&gt;&amp;quot;MasterPublicDnsName&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ec2-54-228-105-10.eu-west-1.compute.amazonaws.com&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;SlaveInstanceType&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;m1.small&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;MasterInstanceType&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;m1.small&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;InstanceGroups&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
          &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;ReadyDateTime&amp;quot;&lt;/span&gt;: 1372215923.0,
            &lt;span class='s2'&gt;&amp;quot;InstanceGroupId&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ig-2TGA68QGUOCUV&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;State&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ENDED&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;LastStateChangeReason&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Job flow terminated&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;InstanceType&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;m1.small&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;InstanceRequestCount&amp;quot;&lt;/span&gt;: 1,
            &lt;span class='s2'&gt;&amp;quot;InstanceRunningCount&amp;quot;&lt;/span&gt;: 0,
            &lt;span class='s2'&gt;&amp;quot;StartDateTime&amp;quot;&lt;/span&gt;: 1372215848.0,
            &lt;span class='s2'&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;BidPrice&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;Market&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ON_DEMAND&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;CreationDateTime&amp;quot;&lt;/span&gt;: 1372215689.0,
            &lt;span class='s2'&gt;&amp;quot;InstanceRole&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;MASTER&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;EndDateTime&amp;quot;&lt;/span&gt;: 1372216249.0
          &lt;span class='o'&gt;}&lt;/span&gt;,
          &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;ReadyDateTime&amp;quot;&lt;/span&gt;: 1372215929.0,
            &lt;span class='s2'&gt;&amp;quot;InstanceGroupId&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ig-2M2UW6B8LFWOG&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;State&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ENDED&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;LastStateChangeReason&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Job flow terminated&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;InstanceType&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;m1.small&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;InstanceRequestCount&amp;quot;&lt;/span&gt;: 1,
            &lt;span class='s2'&gt;&amp;quot;InstanceRunningCount&amp;quot;&lt;/span&gt;: 0,
            &lt;span class='s2'&gt;&amp;quot;StartDateTime&amp;quot;&lt;/span&gt;: 1372215929.0,
            &lt;span class='s2'&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;BidPrice&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;Market&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;ON_DEMAND&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;CreationDateTime&amp;quot;&lt;/span&gt;: 1372215689.0,
            &lt;span class='s2'&gt;&amp;quot;InstanceRole&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;CORE&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;EndDateTime&amp;quot;&lt;/span&gt;: 1372216249.0
          &lt;span class='o'&gt;}&lt;/span&gt;
        &lt;span class='o'&gt;]&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;InstanceCount&amp;quot;&lt;/span&gt;: 2,
        &lt;span class='s2'&gt;&amp;quot;KeepJobFlowAliveWhenNoSteps&amp;quot;&lt;/span&gt;: &lt;span class='nb'&gt;false&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;Placement&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
          &lt;span class='s2'&gt;&amp;quot;AvailabilityZone&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;eu-west-1a&amp;quot;&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;Ec2SubnetId&amp;quot;&lt;/span&gt;: null,
        &lt;span class='s2'&gt;&amp;quot;Ec2KeyName&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;etl-nasqueron&amp;quot;&lt;/span&gt;
      &lt;span class='o'&gt;}&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;JobFlowId&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;j-Y9QNJI44PA0X&amp;quot;&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;BootstrapActions&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[]&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;JobFlowRole&amp;quot;&lt;/span&gt;: null,
      &lt;span class='s2'&gt;&amp;quot;AmiVersion&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;2.3.6&amp;quot;&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;LogUri&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;s3n:\/\/snowplow-emr-logs\/pbz\/&amp;quot;&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;Steps&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
        &lt;span class='o'&gt;{&lt;/span&gt;
          &lt;span class='s2'&gt;&amp;quot;ExecutionStatusDetail&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;State&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;COMPLETED&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;LastStateChangeReason&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;StartDateTime&amp;quot;&lt;/span&gt;: 1372215928.0,
            &lt;span class='s2'&gt;&amp;quot;CreationDateTime&amp;quot;&lt;/span&gt;: 1372215689.0,
            &lt;span class='s2'&gt;&amp;quot;EndDateTime&amp;quot;&lt;/span&gt;: 1372216010.0
          &lt;span class='o'&gt;}&lt;/span&gt;,
          &lt;span class='s2'&gt;&amp;quot;StepConfig&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;HadoopJarStep&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
              &lt;span class='s2'&gt;&amp;quot;MainClass&amp;quot;&lt;/span&gt;: null,
              &lt;span class='s2'&gt;&amp;quot;Args&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
                &lt;span class='s2'&gt;&amp;quot;--src&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;s3n:\/\/snowplow-emr-processing\/pbz\/&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--dest&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;hdfs:\/\/\/local\/snowplow-logs&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--groupBy&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;.*\\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\\..*&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--targetSize&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;128&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--outputCodec&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;lzo&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--s3Endpoint&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;s3-eu-west-1.amazonaws.com&amp;quot;&lt;/span&gt;
              &lt;span class='o'&gt;]&lt;/span&gt;,
              &lt;span class='s2'&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[]&lt;/span&gt;,
              &lt;span class='s2'&gt;&amp;quot;Jar&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;\/home\/hadoop\/lib\/emr-s3distcp-1.0.jar&amp;quot;&lt;/span&gt;
            &lt;span class='o'&gt;}&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Elasticity Custom Jar Step&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;ActionOnFailure&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;TERMINATE_JOB_FLOW&amp;quot;&lt;/span&gt;
          &lt;span class='o'&gt;}&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;,
        &lt;span class='o'&gt;{&lt;/span&gt;
          &lt;span class='s2'&gt;&amp;quot;ExecutionStatusDetail&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;State&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;COMPLETED&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;LastStateChangeReason&amp;quot;&lt;/span&gt;: null,
            &lt;span class='s2'&gt;&amp;quot;StartDateTime&amp;quot;&lt;/span&gt;: 1372216010.0,
            &lt;span class='s2'&gt;&amp;quot;CreationDateTime&amp;quot;&lt;/span&gt;: 1372215689.0,
            &lt;span class='s2'&gt;&amp;quot;EndDateTime&amp;quot;&lt;/span&gt;: 1372216196.0
          &lt;span class='o'&gt;}&lt;/span&gt;,
          &lt;span class='s2'&gt;&amp;quot;StepConfig&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;HadoopJarStep&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
              &lt;span class='s2'&gt;&amp;quot;MainClass&amp;quot;&lt;/span&gt;: null,
              &lt;span class='s2'&gt;&amp;quot;Args&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[&lt;/span&gt;
                &lt;span class='s2'&gt;&amp;quot;com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--hdfs&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--input_folder&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;hdfs:\/\/\/local\/snowplow-logs&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--input_format&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;cloudfront&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--maxmind_file&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;http:\/\/snowplow-hosted-assets.s3.amazonaws.com\/third-party\/maxmind\/GeoLiteCity.dat&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--output_folder&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;s3n:\/\/snowplow-events-pbz\/events\/2013-06-26-04-00-03\/&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;--bad_rows_folder&amp;quot;&lt;/span&gt;,
                &lt;span class='s2'&gt;&amp;quot;2013-06-26-04-00-03\/&amp;quot;&lt;/span&gt;
              &lt;span class='o'&gt;]&lt;/span&gt;,
              &lt;span class='s2'&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[]&lt;/span&gt;,
              &lt;span class='s2'&gt;&amp;quot;Jar&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;s3:\/\/snowplow-hosted-assets\/3-enrich\/hadoop-etl\/snowplow-hadoop-etl-0.3.2.jar&amp;quot;&lt;/span&gt;
            &lt;span class='o'&gt;}&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Elasticity Custom Jar Step&amp;quot;&lt;/span&gt;,
            &lt;span class='s2'&gt;&amp;quot;ActionOnFailure&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;TERMINATE_JOB_FLOW&amp;quot;&lt;/span&gt;
          &lt;span class='o'&gt;}&lt;/span&gt;
        &lt;span class='o'&gt;}&lt;/span&gt;
      &lt;span class='o'&gt;]&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Snowplow Enrichment for pbz&amp;quot;&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;ExecutionStatusDetail&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;{&lt;/span&gt;
        &lt;span class='s2'&gt;&amp;quot;ReadyDateTime&amp;quot;&lt;/span&gt;: 1372215929.0,
        &lt;span class='s2'&gt;&amp;quot;State&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;COMPLETED&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;LastStateChangeReason&amp;quot;&lt;/span&gt;: &lt;span class='s2'&gt;&amp;quot;Steps completed&amp;quot;&lt;/span&gt;,
        &lt;span class='s2'&gt;&amp;quot;StartDateTime&amp;quot;&lt;/span&gt;: 1372215929.0,
        &lt;span class='s2'&gt;&amp;quot;CreationDateTime&amp;quot;&lt;/span&gt;: 1372215689.0,
        &lt;span class='s2'&gt;&amp;quot;EndDateTime&amp;quot;&lt;/span&gt;: 1372216249.0
      &lt;span class='o'&gt;}&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;SupportedProducts&amp;quot;&lt;/span&gt;: &lt;span class='o'&gt;[]&lt;/span&gt;,
      &lt;span class='s2'&gt;&amp;quot;VisibleToAllUsers&amp;quot;&lt;/span&gt;: &lt;span class='nb'&gt;false&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;]&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
alex@nasqueron  ~/Apps/emr-cli  
&lt;span class='nv'&gt;$ &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Rather than parse the JSON yourself, we&amp;#8217;re very happy for community members to simply save the JSON and email it to us, with the other data points. We can then extract the relevant data points from the JSON directly. (We&amp;#8217;ll use R and the RJSON package, and blog about how we do it.) You can either generate a JSON for a specific job (you will need to enter the job ID:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;./elastic-mapreduce --describe --jobflow &lt;span class='o'&gt;{&lt;/span&gt;&lt;span class='nv'&gt;$jobflow&lt;/span&gt;-id&lt;span class='o'&gt;}&lt;/span&gt; &amp;gt; emr-job-data.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or you can fetch the data for every job run in the last two days:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;./elastic-mapreduce --describe &amp;gt; emr-job-data.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or all the data for every job in the last fortnight:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;./elastic-mapreduce --describe all &amp;gt; emr-job-data.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3&gt;&lt;a name='output-back-to-s3'&gt;5. Measuring the number of files written back to S3, and their size&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We can use Cloudberry again. Simply identify a folder in the archive bucket specified in the &lt;a href='https://github.com/snowplow/snowplow/wiki/1-installing-the-storageloader#wiki-configuration'&gt;StorageLoader config&lt;/a&gt;, right click on it and select properties:&lt;/p&gt;

&lt;p&gt;&lt;img alt='number-of-snowplow-event-files-and-size' src='/static/img/blog/2013/07/number-of-snowplow-event-files-and-size.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;In the above example, 3 files were generated for a single run, with a total size of 981.4KB.&lt;/p&gt;
&lt;h3&gt;&lt;a name='redshift-data-points'&gt;6. The total number of lines of data in Redshift, and the amount of Redshift capacity used&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Measuring the amount of space occupied by your events in Redshift is very easy.&lt;/p&gt;

&lt;p&gt;First, measure the number of events by executing the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;select&lt;/span&gt; &lt;span class='k'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;from&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then to find out how much disk space that occupies in your Redshift cluster execute the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;select&lt;/span&gt; &lt;span class='k'&gt;owner&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='n'&gt;node&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;diskno&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;used&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;capacity&lt;/span&gt; 
&lt;span class='k'&gt;from&lt;/span&gt; &lt;span class='n'&gt;stv_partitions&lt;/span&gt; 
&lt;span class='k'&gt;order&lt;/span&gt; &lt;span class='k'&gt;by&lt;/span&gt; &lt;span class='mi'&gt;1&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;4&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The amount of used capacity (in MB) is given in the &amp;#8220;used&amp;#8221; column: it is 1,941MB in the below example. The total capacity is given at 1906184 i.e. 1.8TB: that is because we are running a single (2TB) node.&lt;/p&gt;

&lt;p&gt;&lt;img alt='redshift-example' src='/static/img/blog/2013/07/redshift-disk-space.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;For our purposes, we only need one of the lines of data to calculate the relationship between disk space on Redshift and number of events stored on Redshift, and use that to model Redshift costs.&lt;/p&gt;

&lt;h2 id='help_us_build_an_accurate_robust_model_that_we_all_can_use_to_forecast_snowplow_aws_costs'&gt;Help us build an accurate, robust model, that we all can use to forecast Snowplow AWS costs&lt;/h2&gt;

&lt;p&gt;We realize that you, our users, are busy people who have plenty to do aside from spending 20-30 minutes fetching data points related to your Snowplow installation, and sending them to us. We really hope, however, that many of you do, because:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A Total Cost of Ownership Model will be really useful for all of us!&lt;/li&gt;

&lt;li&gt;We&amp;#8217;ll send you a Snowplow T-shirt, by way of thanks&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you can pop the above data points (in whatever format is most convenient), and email them to me on &lt;code&gt;yali at snowplowanalytics dot com&lt;/code&gt;, along with your T-shirt size, we will send you through your T-shirts as soon as they are printed.&lt;/p&gt;

&lt;p&gt;So please help us help you, and keep plowing!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs"/>
    <title>Unpicking the Snowplow data pipeline and how it drives AWS costs</title>
    <updated>2013-07-09T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Back in March, &lt;a href='https://groups.google.com/forum/#!searchin/snowplow-user/cloudfront$20cost/snowplow-user/b_HPkt3nwzo/Ms-J54e8bUYJ'&gt;Robert Kingston suggested&lt;/a&gt; that we develop a Total Cost of Ownership model for Snowplow: something that would enable a user or prospective user to accurately estimate their Amazon Web Services monthly charges going forwards, and see how those costs vary with different traffic levels. We thought this was an excellent idea.&lt;/p&gt;

&lt;p&gt;Since Rob&amp;#8217;s suggestion, we&amp;#8217;ve made a number of important changes to the Snowplow platform that have changed the way Snowplow costs scale with the number of events served:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We replaced the Hive-based ETL with the &lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/'&gt;Scalding-based enrichment process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We dealt with the &lt;a href='/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;small files problem&lt;/a&gt;, dramatically reducing EMR costs&lt;/li&gt;

&lt;li&gt;We enabled support for &lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements/#task-instances'&gt;task and spot instances&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a result of the pending updates, we held off building the model. But now that they have been delivered, we have started putting together the model. In this post we&amp;#8217;ll walk through the Snowplow data pipeline in detail, and show how different parts of the pipeline drive different AWS costs. In a &lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/'&gt;follow-on post&lt;/a&gt;, we will ask Snowplow users to share with us data points to help us accurately model those costs.&lt;/p&gt;
&lt;h2&gt;&lt;a name='details'&gt;What drives AWS costs for Snowplow users?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It is worth distinguishing the different AWS services, and examining how each scales with volume of events per day, and over time. If we take a &lt;em&gt;typical&lt;/em&gt; Snowplow user (i.e. one running the &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector'&gt;Cloudfront collector&lt;/a&gt; rather than the &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector'&gt;Clojure collector&lt;/a&gt;), and storing their data on Redshift for analysis, rather than analyzing their data in S3 using EMR) then we need to account for:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/#cloudfront'&gt;Cloudfront costs&lt;/a&gt;,&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/#s3'&gt;S3 costs&lt;/a&gt;,&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/#emr'&gt;EMR costs&lt;/a&gt; and&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/09/understanding-how-different-parts-of-the-Snowplow-data-pipeline-drive-AWS-costs/#redshift'&gt;Redshift costs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h3&gt;&lt;a name='cloudfront'&gt;1. Cloudfront costs&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Snowplow uses Cloudfront to serve both &lt;code&gt;sp.js&lt;/code&gt;, the Snowplow Javascript, and the Snowplow pixel &lt;code&gt;i&lt;/code&gt;. Broadly, Cloudfront costs scale linearly with the volume of data served out of Cloudfront, with a couple of provisos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The cost per MB served goes down, as volumes rise (because Amazon tier their pricing)&lt;/li&gt;

&lt;li&gt;The exact cost varies by AWS account region, and the region the browser that loads the content served by Cloudfront is in (so the exact cost depends on the distribution of your visitors geographically)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modelling the costs is therefore reasonably straightforward. The &lt;code&gt;i&lt;/code&gt; pixel is served with every event tracked: so we can calculate the cost of serving &lt;code&gt;i&lt;/code&gt; based on the size of &lt;code&gt;i&lt;/code&gt; (it is 37 bytes), the number of events, and the geographic split of visitors by Amazon region. The costs scale almost linearly with the number of events tracked per day.&lt;/p&gt;

&lt;p&gt;Modelling the cost of serving &lt;code&gt;sp.js&lt;/code&gt; is a little trickier. As discussed in our blog post on &lt;a href='/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control/'&gt;browser caching&lt;/a&gt;, it is possible to set &lt;code&gt;sp.js&lt;/code&gt; so that it is only served once per unique visitor, rather than once per event. Because &lt;code&gt;sp.js&lt;/code&gt; is 37KB (so a lot larger than &lt;code&gt;i&lt;/code&gt;), this has a significant impact on your Cloudfront costs. From a modelling perspective, then, we should estimate costs, based on the number of unique visitors per month, and their geographic distribution by Amazon regions. The costs scale almost linearly with the number of unique visitors to the site / network.&lt;/p&gt;
&lt;h3&gt;&lt;a name='s3'&gt;2. S3 costs&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Snowplow uses S3 to store event data. Amazon charges for S3 based on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The volume of data stored in S3&lt;/li&gt;

&lt;li&gt;The number of requests for that data (in the form of &lt;code&gt;GET&lt;/code&gt; / &lt;code&gt;PUT&lt;/code&gt; / &lt;code&gt;COPY&lt;/code&gt; requests)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Modelling how the volume of data grows with increasing numbers of events, and over time (as the amount of data stored in S3 grows, because Snowplow users generally never throw data away) is straightforward: we calculate how large a &amp;#8216;typical&amp;#8217; row of Snowplow data is in the raw collector logs, and then in the enriched Snowplow event files. We then assume one row of each type for every event that has been tracked, sum to find the total required storage space, and then multiply by Amazon S3&amp;#8217;s cost per GB.&lt;/p&gt;

&lt;p&gt;What is more tricky is modelling the number of requests to S3 for the files. To understand why, we need to examine the Snowplow data pipeline, and in particular, the part of the pipeline that takes the raw data generated by the Snowplow collector, cleans that data, enriches it, and uploads the enriched data into Amazon Redshift for analysis.&lt;/p&gt;

&lt;p&gt;The first part of the data pipeline is orchestrated by &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-EmrEtlRunner'&gt;EmrEtlRunner&lt;/a&gt;. This performs the bulk of data processing work:&lt;/p&gt;

&lt;p&gt;&lt;img alt='emr-etl-runner' src='/static/img/blog/2013/07/emr-etl-runner-steps.png' /&gt;&lt;/p&gt;

&lt;p&gt;This encapsulates the bulk of the data processing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Raw collector log files that need to be processed are identified in the in-bucket, and moved to the processing bucket&lt;/li&gt;

&lt;li&gt;EmrEtlRunner then triggers the Enrichment process to run. This spins up an EMR cluster, loads the data in the processing bucket into HDFS, loads Scalding Enrichment process (as a JAR) and uses that JAR to process the raw logs uploaded into HDFS.&lt;/li&gt;

&lt;li&gt;The output of that Scalding Enrichment process is then written straight into S3. The EMR cluster is then shut down.&lt;/li&gt;

&lt;li&gt;Once the job has completed, the raw logs are moved from the processing bucket to the archive bucket.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For modelling AWS costs (and S3 costs in particular), we need to note that two &lt;code&gt;COPY&lt;/code&gt; requests are executed for each collector log file written to S3: one to move that data from the in-bucket to the processing-bucket, and then another one to move the same file from the processing-bucket to the archive bucket.&lt;/p&gt;

&lt;p&gt;In addition, one &lt;code&gt;GET&lt;/code&gt; request is executed per raw collector log file, when the data is read from S3 for the purposes of writing into HDFS.&lt;/p&gt;

&lt;p&gt;The second part of the data pipeline is orchestrated by the &lt;a href='https://github.com/snowplow/snowplow/wiki/1-Installing-the-StorageLoader'&gt;StorageLoader&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='storage-loader' src='/static/img/blog/2013/07/storage-loader-steps.png' /&gt;&lt;/p&gt;

&lt;p&gt;This is a much simpler stage of the data processing pipeline:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data from enriched event files generated by the Scalding process on EMR is read and written to Amazon Redshift&lt;/li&gt;

&lt;li&gt;The enriched event files are then moved from the in-bucket (which &lt;em&gt;was&lt;/em&gt; the archive bucket for EmrEtlRunner) to the archive bucket (for the StorageLoader)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, for modelling purposes, we note that a single &lt;code&gt;GET&lt;/code&gt; request is executed for each enriched event file (when the file is read for the purposes of copying the data into Redshift). and then a &lt;code&gt;COPY&lt;/code&gt; request is executed for that file to move it from the in to the out bucket.&lt;/p&gt;

&lt;p&gt;This means that we can accurately forecast costs based on the number of raw collector log files generated and the number of enriched event files generated. Unfortunately, modelling how this number scales with visitors / page views and events is &lt;em&gt;not&lt;/em&gt; straightforward because it is not clear how the number of collector log files and Snowplow event files scales with numbers of events tracked. This is one of the things &lt;a href='#help'&gt;we hope the community can help us with&lt;/a&gt;, by providing us with data points to help us unpick the relevant relationships.&lt;/p&gt;

&lt;p&gt;To articulate our challenge: we need to understand&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How the number of raw collector log files (for the Cloudfront collector) scales with number of events&lt;/li&gt;

&lt;li&gt;How the number of enriched Snowplow event files scale with the number of events&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have working hypotheses for what determines both numbers. We need to validate these with the Snowplow community, and quantify the relationships mathematically, based on data points shared by members of our community:&lt;/p&gt;

&lt;p&gt;We believe that Amazon Cloudfront generates one log file per server per edge location every hour. That means that Snowplow users who do not track large volumes of traffic, will generate a surprisingly large number of log files, each with very low volumes of data. (E.g. 1-5 rows.) As traffic levels climb, the number of log files will increase (more requests to more edge locations is bound to hit more Cloudfront servers), but that this tails off reasonably rapidly once there are enough visitors that most servers are hit at most edge locations every hour. (We guess that Amazon has &lt;em&gt;lots&lt;/em&gt; of servers at each edge location, so this tailing off might only happen at very large volumes.) We&amp;#8217;d therefore expect a graph like the one below, if we plotted numbers of log files vs events:&lt;/p&gt;

&lt;p&gt;&lt;img alt='line-graph' src='/static/img/blog/2013/07/line-graph.png' /&gt;&lt;/p&gt;

&lt;p&gt;In the case of forecasting the number of Snowplow Event files: this should be more straightforward. We believe that the Scalding Enrichment Process generates one output file for every input file that it receives. The Scalding Enrichment process does &lt;em&gt;not&lt;/em&gt; operate on the raw collector logs: to speed up data processing (and reduce costs), these are aggregated using S3DistCopy prior to being fed into the Enrichment Process. This &lt;em&gt;should&lt;/em&gt; aggregate the input log files so they are as close to 128MB each as possible. If one output file is produced for each input file, then they will have the same number of rows: it is likely that there is reasonably constant relationship between the size of the two, that mean they are of similar, but not identical, size. (Because they have roughly the same data, but in different formats with different levels of row-level enrichments.) If that is right, then the number of event files should scale almost linearly with the number of events recorded: almost because given the large maximum file size (roughly 128MBs) the graph would look more like a series of step functions, where a new step change occurs each time an additional 128MBs of events are recorded:&lt;/p&gt;

&lt;p&gt;&lt;img alt='step-function' src='/static/img/blog/2013/07/step-function.png' /&gt;&lt;/p&gt;

&lt;p&gt;As we &lt;a href='#help'&gt;explain below&lt;/a&gt;, we hope to plot the above graphs with data points volunteered by Snowplow users, to see if we are correct, and then use to drive the model.&lt;/p&gt;
&lt;h3&gt;&lt;a name='emr'&gt;3. EMR costs&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Snowplow uses EMR to run the Enrichment process on the raw logs created by the collector. Because the process is powered by Hadoop, it should scale linearly: double the number of lines of data to be processed, the time to process should double. Double the number of machines in the cluster, the processing time should be halved.&lt;/p&gt;

&lt;p&gt;Amazon charges for EMR based on the number of boxes in the cluster, the size of those boxes and the time the cluster is live for. Amazon rounds up the nearest hour: as a result, we would expect the cost of the daily job to be a step function.&lt;/p&gt;

&lt;p&gt;&lt;img alt='emr-costs' src='/static/img/blog/2013/07/emr-costs.png' /&gt;&lt;/p&gt;

&lt;p&gt;We need to work out how many lines of data we can expect each Amazon box to process in one hour. Getting a handle on these figures will also mean we can advise Snowplow users what size of cluster to spin up based on the number of events processed per run.&lt;/p&gt;
&lt;h3&gt;&lt;a name='redshift'&gt;1.4 Redshift costs&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The majority of Snowplow users store their events table in Amazon Redshift, and then plug analytics tools into Redshift to crunch that data. As a result, they run a Redshift cluster constantly.&lt;/p&gt;

&lt;p&gt;Amazon charges per Redshift node, where each node provides a 2TB of storage (for standard XL nodes) or 16TB of storage (for 8XL nodes). As a result, we can model Redshift costs simply as a function of the volume of data stored (itself just a function of the number of events tracked per day, and the number of days Snowplow has been running). As for EMR, we expect a step function (with a big increase in cost each time an additional 2TB node is required):&lt;/p&gt;

&lt;p&gt;&lt;img alt='redshift-costs' src='/static/img/blog/2013/07/redshift-costs.png' /&gt;&lt;/p&gt;

&lt;h2 id='wrapping_up'&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;One of the things we realised when we started the modelling exercise, was that we had not sufficiently documented the workings of the Snowplow data pipeline. This blog post is a first stab at addressing that shortfall. We hope it&amp;#8217;s useful for those interested in how Snowplow works, and will be using the diagrams etc. in the &lt;a href='https://github.com/snowplow/snowplow/wiki/Snowplow-technical-documentation'&gt;technical documentation&lt;/a&gt; section of the &lt;a href='https://github.com/snowplow/snowplow/wiki'&gt;Snowplow Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also hope this post is interesting for anyone with an interest in modelling technology costs (and AWS costs in particular).&lt;/p&gt;

&lt;p&gt;Lastly, we hope that this has piqued the interest of any Snowplow user who would value the Total Cost of Ownership model. As alluded to above, &lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/'&gt;you can help us build that model&lt;/a&gt;, by providing us with the data points we need to validate the above hypothesised relationships, and model them accurately. We detail how you can help, exactly, in a &lt;a href='/blog/2013/07/10/help-us-build-out-the-snowplow-total-cost-of-ownership-model/'&gt;follow on post&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/09/dotnet-support-added-to-referer-parser</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/09/dotnet-support-added-to-referer-parser"/>
    <title>.NET (C#) support added to referer-parser</title>
    <updated>2013-07-09T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the addition of &lt;a href='https://github.com/snowplow/referer-parser/tree/master/dotnet'&gt;.NET support (C#)&lt;/a&gt; to our standalone &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; library. Many thanks to &lt;a href='https://github.com/swijnands'&gt;Sepp Wijnands&lt;/a&gt; at &lt;a href='http://www.iperform.nl/'&gt;iPerform Software&lt;/a&gt; for contributing this latest port!&lt;/p&gt;

&lt;p&gt;To recap: referer-parser is a simple library for extracting seach marketing attribution data from referer &lt;em&gt;(sic)&lt;/em&gt; URLs. You supply referer-parser with a referer URL; it then tells you the medium, source and term (in the case of a search) for this referrer. The Scala implementation of referer-parser is a key part of the Snowplow enrichment process.&lt;/p&gt;

&lt;p&gt;As part of our commitment to modular, sustainable technical architectures, we made a decision some time ago to release the Referrer Parser as a standalone library, and have been very pleased to see community ports of the library first to &lt;a href='https://github.com/snowplow/referer-parser/tree/master/python'&gt;Python&lt;/a&gt; and now to &lt;a href='https://github.com/snowplow/referer-parser/tree/master/dotnet'&gt;Dot Net&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a taster for using the library from C Sharp:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='c#'&gt;&lt;span class='k'&gt;using&lt;/span&gt; &lt;span class='nn'&gt;RefererParser&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

&lt;span class='kt'&gt;string&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='p'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;span class='kt'&gt;string&lt;/span&gt; &lt;span class='n'&gt;pageUrl&lt;/span&gt;    &lt;span class='p'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http:/www.psychicbazaar.com/shop&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt; &lt;span class='c1'&gt;// Our current URL&lt;/span&gt;

&lt;span class='kt'&gt;var&lt;/span&gt; &lt;span class='n'&gt;referer&lt;/span&gt; &lt;span class='p'&gt;=&lt;/span&gt; &lt;span class='n'&gt;Parser&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;Parse&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Uri&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt; &lt;span class='n'&gt;pageUrl&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;

&lt;span class='n'&gt;Console&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;WriteLine&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;Medium&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt; &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Search&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;Console&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;WriteLine&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;Source&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt; &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;Console&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;WriteLine&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;Term&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt; &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the jump we will hear from author Sepp Wijnands and then provide some brief next steps for using the library from .NET.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='a_brief_interview_with_author_sepp_wijnands'&gt;A brief interview with author Sepp Wijnands&lt;/h2&gt;

&lt;p&gt;We asked &lt;a href='https://github.com/swijnands'&gt;Sepp&lt;/a&gt; to tell us a little bit about himself, iPerform Software and why he ported referer-parser to the Dot Net platform:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href='http://www.iperform.nl/'&gt;iPerform Software&lt;/a&gt; (website in Dutch) helps companies in the Benelux succeed with their integration and web application needs. Our focus lies on modern Line of Business web applications, and pride ourselves with the ability to integrate almost anything and everything.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;One of our customers had a feature request about getting (directly) notified when a very specific set of keywords is used when somebody enters the website via Google or Bing (or &amp;#8230;.) searches.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;While we could probably have used an existing third-party analytics platform to do the job, one of the great things about the referer-parser project is that the &lt;a href='https://github.com/snowplow/referer-parser/blob/master/referers.yml'&gt;referer database&lt;/a&gt; is very complete, and using the referer-parser library we can be in full control how and when notifications are send.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The referer-parser solution is currently deployed and fully up-and-running. And so far, the customer couldn&amp;#8217;t be happier!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So, thanks again for the fabulous referer-parser project!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id='installation'&gt;Installation&lt;/h2&gt;

&lt;p&gt;Interested in using the .NET port of referer-parser? A NuGet Package is available, under package id RefererParser.&lt;/p&gt;

&lt;h2 id='find_out_more'&gt;Find out more&lt;/h2&gt;

&lt;p&gt;For more information, please check out the &lt;a href='https://github.com/snowplow/referer-parser/blob/master/dotnet/README.md'&gt;project README&lt;/a&gt; for the .NET port of referer-parser.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/07/snowplow-0.8.7-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/07/snowplow-0.8.7-released"/>
    <title>Snowplow 0.8.7 released with JavaScript Tracker improvements</title>
    <updated>2013-07-07T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;After a brief summer intermission, we are pleased to announce the release of &lt;a href='https://github.com/snowplow/snowplow/releases/0.8.7'&gt;Snowplow &lt;strong&gt;0.8.7&lt;/strong&gt;&lt;/a&gt;. This is a small release, primarily consisting of bug fixes for the JavaScript Tracker, which is bumped to version 0.12.0.&lt;/p&gt;

&lt;p&gt;As well as some tweaks and improvements, this release fixes bugs which only occurred on older versions of Internet Explorer, and fixes a bug which prevented the &lt;code&gt;setCustomUrl()&lt;/code&gt; method from working properly. Many thanks to community member &lt;a href='https://github.com/mfu0'&gt;mfu0&lt;/a&gt; and Snowplow client &lt;a href='http://www.bigcommerce.com/'&gt;BigCommerce&lt;/a&gt; for bringing some of these issues to our attention.&lt;/p&gt;

&lt;p&gt;As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.12.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note that this release implements the latest version of the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow Tracker Protocol&lt;/a&gt;, whereby custom structured event fields now start with &lt;code&gt;se_&lt;/code&gt; rather than &lt;code&gt;ev_&lt;/code&gt;. This change is backwards compatible with all versions of the Hadoop-based ETL, but &lt;strong&gt;not&lt;/strong&gt; with the Hive-based ETL. If you are still using the Hive ETL, we &lt;strong&gt;strongly&lt;/strong&gt; recommend you upgrade to the Hadoop ETL.&lt;/p&gt;

&lt;p&gt;This is the first Snowplow release to make use of the new &lt;a href='https://github.com/blog/1547-release-your-software'&gt;GitHub Releases&lt;/a&gt; functionality - so do check out the &lt;a href='https://github.com/snowplow/snowplow/releases/0.8.7'&gt;0.8.7 Release Notes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we should mention that this is the last Snowplow release with the JavaScript Tracker as part of the main Snowplow repository. As the number of Snowplow trackers grows (it&amp;#8217;s now four and counting), it makes more sense for each tracker to have its own repository, bug tracker, CI configuration and so forth. Don&amp;#8217;t worry though - the JavaScript Tracker will remain &lt;em&gt;primus inter pares&lt;/em&gt; among the Snowplow trackers. We will make this split in the next Snowplow release.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released"/>
    <title>Snowplow Tracker for Lua event analytics released</title>
    <updated>2013-07-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the release of our &lt;a href='https://github.com/snowplow/snowplow-lua-tracker'&gt;SnowplowTracker for Lua event analytics&lt;/a&gt;. This is our fourth tracker to be released, following on from our &lt;a href='https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker'&gt;JavaScript&lt;/a&gt;, &lt;a href='https://github.com/snowplow/snowplow/tree/master/1-trackers/no-js-tracker'&gt;no-JavaScript (aka pixel)&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;Arduino&lt;/a&gt; Trackers.&lt;/p&gt;

&lt;p&gt;As a lightweight, easily-embeddable scripting language, &lt;a href='http://www.lua.org/'&gt;Lua&lt;/a&gt; is available in a huge number of different computing environments and platforms, from &lt;a href='http://www.wowwiki.com/Lua'&gt;World of Warcraft&lt;/a&gt; through &lt;a href='http://openresty.org/'&gt;OpenResty&lt;/a&gt; to &lt;a href='http://www.adobe.com/devnet/photoshoplightroom.html'&gt;Adobe Lightroom&lt;/a&gt;. And now, the Snowplow Lua Tracker lets you collect event data from these Lua-based applications, Lua web servers/frameworks, or from the Lua scripting layer within your games or apps - here&amp;#8217;s a taster:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='lua'&gt;&lt;span class='kd'&gt;local&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;snowplow&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;newTrackerForCf&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;d3rkrsqld9gmqf&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;setAppId&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;my-warcraft-addon&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='kd'&gt;local&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;msg&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;trackStructEvent&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;shop&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;add-to-basket&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;nil&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;armour-vi&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We are hugely excited about our new event tracker for Lua. The first analytics tracker ever released for the Lua language, SnowplowTracker continues Snowplow&amp;#8217;s push into the tracking and analysis of non-Web events. Moreover, as part of our commitment to &lt;a href='http://snowplowanalytics.com/blog/2013/04/10/snowplow-event-validation/'&gt;High-Fidelity Analytics&lt;/a&gt;, this tracker:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Is our first to include a full suite of unit and integration tests, built using the excellent &lt;a href='http://olivinelabs.com/busted/'&gt;Busted&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Uses contracts-style argument validation throughout, to prevent incorrectly-structured events from being sent to Snowplow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After the jump we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released#supported-events'&gt;Supported events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released#usage-example'&gt;Usage example&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/03/snowplow-tracker-for-lua-event-analytics-released#read-more'&gt;Finding out more&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='supported-events'&gt;1. Supported events&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Lua Tracker supports three types of event:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;trackStructEvent()&lt;/code&gt; - for tracking Google Analytics-style custom structured events with five fields (category, action, label, property, value)&lt;/li&gt;

&lt;li&gt;&lt;code&gt;trackUnstructEvent()&lt;/code&gt; - for tracking Mixpanel-style custom unstructured events, consisting of an event name plus event properties in a Lua table&lt;/li&gt;

&lt;li&gt;&lt;code&gt;trackScreenView()&lt;/code&gt; - for tracking views of individual screens within a game, app or similar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Please note that currently only &lt;code&gt;trackStructEvent()&lt;/code&gt; is supported within the Snowplow ETL and Redshift table definition; adding support for unstructured events and screen views is on our roadmap.&lt;/p&gt;
&lt;h2&gt;&lt;a name='usage example'&gt;2. Usage example&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Using the SnowplowTracker from Lua is really simple.&lt;/p&gt;

&lt;p&gt;First require the library and instantiate a new tracker:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='lua'&gt;&lt;span class='kd'&gt;local&lt;/span&gt; &lt;span class='n'&gt;snowplow&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nb'&gt;require&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;snowplow&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='kd'&gt;local&lt;/span&gt; &lt;span class='n'&gt;tracker&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;snowplow&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;newTrackerForUri&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;snplow.mydomain.com&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can adjust the default configuration if you like:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='lua'&gt;&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;encodeBase64&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='c1'&gt;-- Default is true&lt;/span&gt;
&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;platform&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;tv&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='c1'&gt;-- Default is &amp;quot;pc&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Add any additional information about the app, user or device that you know:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='lua'&gt;&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;setUserId&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;bob@example.com&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;setAppId&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;my-lightroom-plugin&amp;quot;&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;setScreenResolution&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='mi'&gt;1068&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;720&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you are ready to track your events:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='lua'&gt;&lt;span class='n'&gt;tracker&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='n'&gt;trackUnstructEvent&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;save-file&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='n'&gt;save_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;4321&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                                           &lt;span class='n'&gt;compression&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mi'&gt;98&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                                           &lt;span class='n'&gt;description&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;&lt;span class='s'&gt;Backup without layers&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                                           &lt;span class='n'&gt;read_only&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;
                                         &lt;span class='p'&gt;}&lt;/span&gt;
                          &lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that&amp;#8217;s it!&lt;/p&gt;
&lt;h2&gt;&lt;a name='usage example'&gt;3. Finding out more&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To read through the complete API for this Lua event tracker, please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Lua-Tracker'&gt;Lua Tracker page&lt;/a&gt; on the Snowplow wiki.&lt;/p&gt;

&lt;p&gt;For setting up the tracker in various Lua environments (including &lt;a href='http://luarocks.org/repositories/rocks/'&gt;LuaRocks&lt;/a&gt;), please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Lua-tracker-setup'&gt;Lua Tracker Setup page&lt;/a&gt; on the Snowplow wiki.&lt;/p&gt;

&lt;p&gt;To check out the code itself, please see our &lt;a href='https://github.com/snowplow/snowplow-lua-tracker'&gt;snowplow-lua-tracker&lt;/a&gt; repository in GitHub.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control"/>
    <title>Reduce your Cloudfront costs with cache control</title>
    <updated>2013-07-02T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the reasons Snowplow is very popular with very large publishers and online advertising networks is that the cost of using Snowplow to track user behavior across your website or network is significantly lower than with our commercial competitors, and that difference becomes more pronounced as the number of users and events you track per day increases.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve been very focused on reducing the cost of running Snowplow further. Most of our efforts have focused on EMR costs (see especially last month&amp;#8217;s &lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements/'&gt;0.8.6 release&lt;/a&gt;), as EMR costs tend to make up the bulk of Snowplow users AWS bills. However, for Snowplow uses tracking billions of events per month, Cloudfront costs also become significant.&lt;/p&gt;

&lt;p&gt;In this post, we explain how to use browser caching to significantly reduce the size of your Cloudfront bills.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control/#costs'&gt;Understanding Cloudfront costs and the potential savings&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/07/02/reduce-your-cloudfront-bills-with-cache-control/#update'&gt;How to update your files in S3 to get them to cache in your users&amp;#8217; browsers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='money' src='/static/img/blog/2013/07/breaking-bad-money.jpg' /&gt;&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='costs'&gt;Understanding Cloudfront costs and the potential savings&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow users rack up Cloudfront costs because both the Snowplow Javascript file &lt;code&gt;sp.js&lt;/code&gt; and the &lt;code&gt;i&lt;/code&gt; pixel are served from Cloudfront. A straightforward way of reducing your Cloudfront costs is to encourage the browsers of users who visit the websites you are tracking to cache &lt;code&gt;sp.js&lt;/code&gt;. Because this file does not change frequently (it only changes on specific Snowplow upgrades), there is no need for the browser to reload it with each new page view. By getting browsers to cache the file, so that it only loads once per browser (i.e. once per visitor) rather than once per page view, your Cloudfront fees should drop notably. Take for example, an example ad network serving that uses Snowplow to track ad impressions, and serves 30 billion ads per month on 10 billion page views, across a network of 150M uniques.&lt;/p&gt;

&lt;p&gt;If the ad network does not setup &lt;code&gt;sp.js&lt;/code&gt; so that it is cached on visitor&amp;#8217;s browsers, &lt;code&gt;sp.js&lt;/code&gt; will be reloaded with every page view. &lt;code&gt;sp.js&lt;/code&gt; is 32 kilobytes in size, and Cloudfront charges a minimum of $0.12 per Gigabyte served + $0.0075 per 10k requests. Therefore, the monthly cost of serving &lt;code&gt;sp.js&lt;/code&gt; will be:&lt;/p&gt;
&lt;p style='text-align:center'&gt;
	`32/1048576 xx (10xx10^9) xx $0.12 + (10xx10^9)/10000 xx $0.0075 = $44121`
&lt;/p&gt;
&lt;p&gt;In contrast, if &lt;code&gt;sp.js&lt;/code&gt; is only requested once per browser i.e. 150M times per month, instead of 10Bn, the cost will be&lt;/p&gt;
&lt;p style='text-align:center'&gt;
	`32/1048576 xx 150xx10^6 xx $0.12 + (150xx10^6)/10000 xx $0.0075 = $662`
&lt;/p&gt;
&lt;p&gt;That&amp;#8217;s a tidy saving of reduction in Cloudfront cost of 98.5%, from $44k to only $662 per month!&lt;/p&gt;
&lt;h2&gt;&lt;a name='update'&gt;How to update your files in S3 to get them to cache in y our users' browsers&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In order to get browsers to cache &lt;code&gt;sp.js&lt;/code&gt;, we need to set the &lt;code&gt;Cache control max-age&lt;/code&gt; property on the &lt;code&gt;sp.js&lt;/code&gt; object. This instructs browsers to keep a local copy of &lt;code&gt;sp.js&lt;/code&gt; cached, and defines the time period over which it should be cached.&lt;/p&gt;

&lt;p&gt;To set this property, we go into the AWS S3 console, identify &lt;code&gt;sp.js&lt;/code&gt;, right click on it and select &lt;strong&gt;Properties&lt;/strong&gt;. Click on the &lt;strong&gt;Metadata&lt;/strong&gt; drop down and then click &lt;strong&gt;Add more metadata&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='add-more-metadata' src='/static/img/blog/2013/07/properties-filled-in.png' /&gt;&lt;/p&gt;

&lt;p&gt;In the keys dropdown select &lt;strong&gt;Cache control&lt;/strong&gt;. In the values field enter &lt;code&gt;max-age=&lt;/code&gt; and then a value for the lifetime of &lt;code&gt;sp.js&lt;/code&gt;. In our case, we&amp;#8217;ve set it to 10 years i.e. 315,360,000 seconds.&lt;/p&gt;

&lt;p&gt;Save the changes. Note: if you are editing a file in S3 that is already in Cloudfront, you will need to &lt;a href='http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html#invalidating-objects-console'&gt;invalidate it&lt;/a&gt;, so that Cloudfront fetches the new version (with the &lt;code&gt;Cache control max-age&lt;/code&gt; property set) to the edge locations, for loading into your visitors browsers.&lt;/p&gt;

&lt;h3 id='a_note_distinguishing_between_caching_in_cloudfront_edge_locations_and_the_user_browser'&gt;A note distinguishing between caching in Cloudfront edge locations, and the user browser&lt;/h3&gt;

&lt;p&gt;The above property sets the length of time that &lt;code&gt;sp.js&lt;/code&gt; should be cached in the Cloudfront Edge location &lt;strong&gt;and&lt;/strong&gt; the user browser.&lt;/p&gt;

&lt;p&gt;For our purposes, that is fine. However, it is the fact that the object is cached in the user&amp;#8217;s browser (rather than the Cloudfront edge location) that results in the desired cost saving.&lt;/p&gt;

&lt;h3 id='a_note_about_versioning_the_cached_file'&gt;A note about versioning the cached file&lt;/h3&gt;

&lt;p&gt;In the event that you update &lt;code&gt;sp.js&lt;/code&gt; (e.g. because you have upgraded to a newer version of Snowplow), you will want your visitors to reload the new version of &lt;code&gt;sp.js&lt;/code&gt;. To ensure this happens, we recommend uploading a newer version of &lt;code&gt;sp.js&lt;/code&gt; to a new subfolder in your S3 bucket, and then updating your Snowplow tags to the updated URL. In our case, for example, we host version 0.11.1 of &lt;code&gt;sp.js&lt;/code&gt; on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And version 0.11.2 on:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.2/sp.js&lt;/code&gt;&lt;/pre&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity"/>
    <title>Is web analytics easy or hard? Distinguishing different types of complexity, and approaches for dealing with them</title>
    <updated>2013-06-28T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;i&gt;This post is a response to an excellent, but old, blog post by &lt;a href='http://www.gilliganondata.com/index.php/about/'&gt;Tim Wilson&lt;/a&gt; called &lt;a href='http://www.gilliganondata.com/index.php/2011/08/09/web-analytics-platforms-are-fundamentally-broken/'&gt;Web Analytics Platforms are Fundamentally Broken&lt;/a&gt;, authored back in August 2011. Tim made the case (that is still true today) that web analytics is hard, and part of that hardness is because web analytics platforms are fundamentally broken. After Tim published his post, a very interesting conversation ensued on &lt;a href='https://plus.google.com/109933174446684687846/posts/fCNTrop8HJz'&gt;Google+&lt;/a&gt;. Reading through it, I was struck by how many of the points raised are still relevant today, and how many of the participants touched on issues that were central to our thinking when when we conceived of Snowplow.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;One thing that I feel was sorely lacking from the discussion around &lt;i&gt;hardness&lt;/i&gt; in web analytics was an effort to distinguish different sources of complexity. In this post, I identify some of those sources, and outline strategies for dealing with them. At the end of the post, I consider the extent to which web analytics platforms can help overcome that complexity, and argue that Omniture Sitecatalyst actually creates additional complexity because of the way it has been architected technically. I end by concluding that &lt;strong&gt;web analytics will continue to be hard because there are no straightforward ways to address every source of complexity, but that tools like Sitecatalyst make it harder than it needs to be&lt;/strong&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt='rubiks-cube' src='/static/img/blog/2013/06/rubiks-cube.png' /&gt;&lt;/p&gt;

&lt;h2 id='what_is_it_that_makes_web_analytics_difficult'&gt;What is it that makes web analytics difficult?&lt;/h2&gt;

&lt;p&gt;The following all make web analytics hard.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity#data'&gt;Large, complex, high volume, high velocity data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity#metadata'&gt;Lots of contextual knowledge requried to understand the data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity#questions'&gt;The business questions that people want to use the data to answer are hard&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/28/is-web-analytics-easy-or-hard-distinguishing-different-types-of-complexity#tools'&gt;Poorly architected tools make the previous three complexities &lt;em&gt;harder&lt;/em&gt; to deal with&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Much of the discussion around Tim&amp;#8217;s original post was whether the complexity was the fault of web analytics platforms or not. As should be clear from the above, I believe that a certain amount of complexity is inherent in web analytics. However, Omniture&amp;#8217;s SiteCatalyst (around which much of the discussion about Tim&amp;#8217;s blog post focused) actually manages to make things worse.&lt;/p&gt;
&lt;!--more--&gt;&lt;h3&gt;&lt;a name='data'&gt;1. Large, complex, high volume, high velocity data&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Web data is complicated. And not just for the obvious reasons that there&amp;#8217;s a lot of it, and it is generated very quickly:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is not obvious what data you should be capturing, and what you shouldn&amp;#8217;t. You can push just about any sort of data into your tracking tags - so how do you decide what needs capturing, and what does not?&lt;/li&gt;

&lt;li&gt;The range of activities that a user can engage with on the modern web is enormous. People shop, bank, research, collaborate with one-another, create documents, give presentations, flirt, pay taxes and more online. How do we capture and structure data to convey the breadth and depth of these different activities, in such a way that we can analyze them later?&lt;/li&gt;

&lt;li&gt;Each time a user performs any of the activities mentioned above, they typically interact with different entities online: product listings, newspaper articles, bank statements, research reports, potential dates, colleagues, companies, government organiastions, charities. How do we capture and structure data to represent this vast array of entities, in such a way that we can analyze them later?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The key to handling this kind of complexity is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use a straightforward data model that is simple to relate to the events they represent, and structure data in a way that mirrors our natural way of thinking about those events. In the case of web event data, that means your data should be an event stream, where each event is represented by a specific line, or several lines, of data, that describe the event and the entities involved in that event&lt;/li&gt;

&lt;li&gt;Let domain experts and data analysts decide what data points need to be captured with each event, and for each entity. It is impossible to come up with an abstract set of rules for what data needs to be captured when an event occurs, but it is often straight forward (and intuitive) for a domain expert and data analyst to identify what should be captured with each event.&lt;/li&gt;

&lt;li&gt;Let domain experts and data analysts decide how to structure those data points.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a name='metadata'&gt;2. Lots of contextual knowledge required to understand the data&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Raw web event data is hard to make sense of: we need contextual knowledge to do so. To give two examples:&lt;/p&gt;

&lt;h4 id='2a_how_do_i_infer_attributes_about_the_objects_and_actions_i_am_interested_in_from_the_data_and_crucially_the_scope_of_those_attributes'&gt;2a. How do I infer attributes about the objects and actions I am interested in from the data, and crucially, the scope of those attributes?&lt;/h4&gt;

&lt;p&gt;Let&amp;#8217;s take the example of a user on a shopping site who buys a pair of running shoes. There are several things we &lt;em&gt;might&lt;/em&gt; infer from the data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The user runs&lt;/li&gt;

&lt;li&gt;The user has size 10 feet&lt;/li&gt;

&lt;li&gt;The user is a man&lt;/li&gt;

&lt;li&gt;The user lives in Dallas, Texas&lt;/li&gt;

&lt;li&gt;The users name&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Whether we should make the above inferences from the data, and whether we use those inferences in other analyses down the line, are decisions that can only been taken based on our broader understanding of the business and the ways users engage with it. That kind of contextual knowledge isn&amp;#8217;t stored in the data itself. These types of decisions look different depending on the type of entity we are dealing with (customer vs product vs article vs company etc.) and the type of decisions and reasoning we need to perform about that entity.&lt;/p&gt;

&lt;p&gt;What is interesting about the above is that these decisions are reasonably straightforward for a data analyst with the data in front of her to make: we all know, for example, that changes in a person&amp;#8217;s gender are unusual, and so if the user is a man today, he is likely to still be a man in 6 months time. But they are &lt;strong&gt;not&lt;/strong&gt; straightforward for a technology platform to make.&lt;/p&gt;

&lt;h4 id='2b_does_the_eventstream_look_like_this_because_a_the_user_wanted_to_perform_this_action_b_the_website_pushed_the_user_into_performing_this_series_of_actions_or_c_the_way_the_web_analytics_tool_captures_the_data'&gt;2b. Does the event-stream look like this because (a) the user wanted to perform this action, (b) the website pushed the user into performing this series of actions or (c) the way the web analytics tool captures the data?&lt;/h4&gt;

&lt;p&gt;Interpreting web event data is hard because it is a function of all the above. To analyse web data, you need to appreciate all three factors. Unfortunately, you cannot control for all of them. A/B testing means you can control for site design, to a limited extent, and comparing user actions between different sets of users enables you to control (a little bit) for user intention. Exercising that control, though, is very difficult. For the most part, web analysts are like astrophysicists, able to capture data, but limited in the experiments they can run to unpick the impact of different factors on that data.&lt;/p&gt;

&lt;p&gt;Once again, an intelligent analyst is best placed to unpick the impact of the three factors identified above - it is a pretty impossible task for a web analytics platform to perform, because the platform lacks that contextual knowledge.&lt;/p&gt;

&lt;p&gt;The key, then, to handling complexity related to the amount of domain knowledge that is required to generate meaning out of web data, is to give the analyst the freedom to address the above questions using all the domain expertise at her disposal, and trust that she uses that domain knowledge to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Properly identify people&lt;/li&gt;

&lt;li&gt;Correctly scope variables associated with different entities&lt;/li&gt;

&lt;li&gt;Unpick the impact of different factors on the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These challenges are much better met by a person with that contextual knowledge, than a web analytics program that lacks it. The web analytics program really needs to &lt;em&gt;get out of the way&lt;/em&gt; of the analyst, so she can address them directly.&lt;/p&gt;
&lt;h3&gt;&lt;a name='questions'&gt;3. The business questions that people want to use the data to answer are hard to answer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Web event data can be used to help answer a whole host of business questions. Some important questions include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What combinations of channels should I spend my marketing budget? How should I stagger spend to maximize return?&lt;/li&gt;

&lt;li&gt;What products should I source for my catalog? How does my catalog need to evolve in the next few years / months?&lt;/li&gt;

&lt;li&gt;How sensitive are my customers to price? What price should I sell this product for?&lt;/li&gt;

&lt;li&gt;Who are my most valuable customers? How can I spot them early?&lt;/li&gt;

&lt;li&gt;What are the events in a customer&amp;#8217;s lifetime that drive loyalty? What are the things we can do to encourage them?&lt;/li&gt;

&lt;li&gt;What are the moments in a customer&amp;#8217;s lifetime where something going wrong destroys customer loyalty? Which are hurting my business (and my customers) the most?&lt;/li&gt;

&lt;li&gt;How do my customers segment by behavior? By attitude? Which segments are important to understand, to deliver excellent customer service?&lt;/li&gt;

&lt;li&gt;Where should we prioritize improvements to our website? What is the expected return on that investment? What is the actual return?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These questions are hard to answer because:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There is no unambiguous, analytic approach to answering them. There are multiple approaches, each of which are defensible. Identifying and applying the most suitable approach is an art&lt;/li&gt;

&lt;li&gt;Answering the questions correctly requires, at a bare minimum, knowledge of the data and domain knowledge. In many cases it is also likely to require other data sources (including both quant and qualitative sources), that need to be brought together in a meaningful way&lt;/li&gt;

&lt;li&gt;The answers in many cases are likely to be hypotheses that need further testing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once again, the key thing to handling this complexity is to give the analyst the tools and the space to develop and experiment with different approaches. There is no general purpose tool that will be able to solve for all of the above, although there may be the possibility of specific tools to answer specific questions.&lt;/p&gt;
&lt;h3&gt;&lt;a name='tools'&gt;4. Poorly architected tools make the above &lt;i&gt;harder&lt;/i&gt; to deal with, rather than &lt;i&gt;easier&lt;/i&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The above sources of complexity make it clear why web analytics is hard. They present challenges for both web analysts, and web analytics tools.&lt;/p&gt;

&lt;p&gt;One approach to dealing with that complexity is to &amp;#8220;disguise it&amp;#8221;. The web analytics tools hides the underlying complexity behind a UI that presents specific cuts of the data. Many of the contributors to the &lt;a href='https://plus.google.com/109933174446684687846/posts/fCNTrop8HJz'&gt;Google+ thread&lt;/a&gt; argued that this was how GA manages to be simpler than SiteCatalyst. Certainly, you can hide &lt;em&gt;all&lt;/em&gt; the complexity behind a simple dashboard. But then, you can&amp;#8217;t use a dashboard to answer any of the above questions. In this case, what you gain in simplicity, you lose in power and transparency.&lt;/p&gt;

&lt;p&gt;Another approach, which is the one we have taken at Snowplow, is to expose the underlying data to the user in a format (data model) that is as easy as possible to understand, and in a data store that is easy to connect multiple different analytics tools. This doesn&amp;#8217;t disguise any of the complexity: instead, it exposes it all to the analyst. For many analysts, that is a terrifying prospect. But for some, it is truly liberating: the analyst can now use the analytic and technical approach she prefers to develop answers and insights, unconstrained by any assumed logic in the web analytics tool.&lt;/p&gt;

&lt;p&gt;A third approach, taken by Omniture with Sitecatalyst, manages to exacerbate the complexity because of two poor decisions made around Sitecatalyst&amp;#8217;s technical architecture:&lt;/p&gt;

&lt;h4 id='4a_sitecatalysts_data_model_is_not_event_or_entitycentric'&gt;4a. Sitecatalysts data model is not event or entity-centric&lt;/h4&gt;

&lt;p&gt;To implement Sitecat, you have to translate the events that occur on your website, and the entities a user navigating on your website engages with, into the arcane world of Traffic Variables, Success Events, Conversion Variables and Saint Classifications. Your data model is, in many cases, flatted to fit a set of pre-defined fields in Omniture. Contrast that with the much simpler, event-centric approach taken by just about everyone that&amp;#8217;s developed a platform in the last five years, including Mixpanel, Kissmetrics, KeenIO, Google Analytics and of course Snowplow.&lt;/p&gt;

&lt;h4 id='4b_in_sitecatalyst_data_capture_and_data_reporting_are_incredibly_tightly_coupled'&gt;4b. In Sitecatalyst, data capture and data reporting are incredibly tightly coupled&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;How&lt;/em&gt; you capture a data point in Sitecatalyst determines which reports that data point is used in, and how that data point is used subsequently. That is why, at a simplistic level, you absolutely need to understand Traffic Variables, Success Events, Conversion Variables and Saint Classifications, and how Sitecatalyst treats each of them, in order to do a Sitecatalyst implementation properly. That makes implementations significantly more complicated than the need to be, and they make the impact of &amp;#8220;bad&amp;#8221; implementations much more catastrophic than they need to be.&lt;/p&gt;

&lt;p&gt;In contrast, with Snowplow, &lt;strong&gt;no&lt;/strong&gt; restrictions are placed on how you use any data based on how you choose to capture it. That is because data analysis is completely decoupled from data capture: we only enable you to capture and warehouse your data. You then do whatever you want with it, often in a different tool.&lt;/p&gt;

&lt;p&gt;There is another fundamental disadvantages that arises from the tight coupling data capture and data reporting in Sitecatalyst: it is not possible to use Sitecatalyst to develop definitions, definitions and metrics which evolve over time, as you better understand your data. To give perhaps the most common example: if you want to categorise your users based on their behaviour on their site, you cannot do that in Sitecatalyst, apply those definitions to your data, and then evolve those definitions: either you have them on day one, and store them via the Javascript, or you don&amp;#8217;t. You can&amp;#8217;t perform an analysis on user behaviour, and then retrospectively categorize users based on the output of that analysis. (Companies generally export Sitecatalyst data out and ingest it in a datawarehouse, and then do the segmentation there.)&lt;/p&gt;

&lt;p&gt;Given the two massive disadvantages to the tight data coupling, it seems only fair to ask if there are any benefits associated with it. There is one that is worth exploring: when you collect your data properly in Sitecatalyst, Sitecat then ensures that that data point accommodated in every report it features. By taking more effort earlier on (at implementation time) to get your data to fit into Sitecat&amp;#8217;s rigid data model, you can then breathe easy down the line that anyone using the data via the UI is restricted so that they only use the data properly: they don&amp;#8217;t, for example, mix dimensions and metrics with different scope.&lt;/p&gt;

&lt;p&gt;We think this &amp;#8220;advantage&amp;#8221; is not really worth anything. We think it is much easier to work out what dimensions and metrics you should, and should not, plot against one another when you have the data in front of you, but that it is much harder when the data is just an idea at implementation time. Worse, if you cut your data in a way that doesn&amp;#8217;t make sense down the line, it is an easy mistake to spot and fix. In contrast, if you stuff up a Sitecat implementation, it can be hard to fix, and costly, and you might have lost months of data in the meantime.&lt;/p&gt;

&lt;p&gt;To Omniture&amp;#8217;s credit, the two technical decisions made above were committed in the late 1990s, when the web looked very different, and so they were not such bad decisions. Since then, Omniture has had to accommodate growing complexity in the web by making incremental approaches to their platform, rather than reinventing the core platform with a fresh perspective, the way we&amp;#8217;ve been able to with Snowplow. But that provides little comfort for the company that has to reimplement Sitecatalyst because they got the implementation wrong the first time.&lt;/p&gt;

&lt;h2 id='so_web_analytics_is_very_hard'&gt;So web analytics is very hard!&lt;/h2&gt;

&lt;p&gt;Yes! Web analytics is hard. But tools like Sitecatalyst make it harder than it needs to be, especially at implementation time. The idea that implementing Sitecatalyst is more difficult than Google Analytics or Snowplow because Sitecatalyst is more powerful than GA is only partly true at best. It is more difficult because reporting and data capture are too tightly coupled, and the data model is totally unnatural to the uninitiated.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/06/26/getting-started-with-r-for-data-analysis-and-visualization"/>
    <title>Getting started using R for data analysis</title>
    <updated>2013-06-26T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;a href='http://cran.r-project.org/'&gt;R&lt;/a&gt; is one of the most popular data analytics tools out there, with a rich and vibrant community of users and contributors. In spite of its popularity in general (and particularly with amongst academics and statisticians), R is not a common tool to find in business or web analysts arsenal, where Excel and Google Analytics tend to reign supreme.&lt;/p&gt;

&lt;p&gt;&lt;img alt='img1' src='/static/img/analytics/tools/r/boxplot.png' /&gt;&lt;/p&gt;

&lt;p&gt;That is a real shame. R is a fantastic tool for exploring data, reworking it, visualizing it, developing models, and then comparing those models against your data sets. My work crunching data has become immeasurably easier and more pleasurable since I learnt R: I spend less time cleaning and organising data, and performing other boring things like formatting visualizations and checking for errors. I spend much more time actually mining data, deriving insights, testing and building on those insights.&lt;/p&gt;

&lt;p&gt;Given how great it is, why is R used so little by business and web analysts? I think there are two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;People are put off by the command-line. The learning curve for command-line tools in general is steep initially. In addition, there are some quirks to the R language and syntax that make it puzzling for newbies who are familiar with other command-line tools for data analysis e.g. Python.&lt;/li&gt;

&lt;li&gt;Most business and web analysts spend more time describing data that actually deriving insights from it. For descriptive work, tools like Excel and BI / OLAP tools like Tableau are OK.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to encourage business and web analysts get started with R, we&amp;#8217;ve put together &lt;a href='/analytics/tools-and-techniques/get-started-analysing-snowplow-data-with-r.html'&gt;a tutorial&lt;/a&gt; in the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; geared specifically towards newbies who want to start playing with R. The purpose is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get started quickly, by working with real web analytics data from Snowplow&lt;/li&gt;

&lt;li&gt;Get started with visualizations, to make the data come alive&lt;/li&gt;

&lt;li&gt;Cover the basics of concepts like factors and data frames, which are core to the way R works&lt;/li&gt;

&lt;li&gt;Give enough of an overview with the example that someone who works through the tutorial can then go off and productively use the multitude of other resources out there to build their knowledge of R.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The recipe - &lt;a href='/analytics/tools-and-techniques/get-started-analysing-snowplow-data-with-r.html'&gt;Getting Started with R&lt;/a&gt;, is available &lt;a href='/analytics/tools-and-techniques/get-started-analysing-snowplow-data-with-r.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/06/05/tracking-olark-chat-events-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/06/05/tracking-olark-chat-events-with-snowplow"/>
    <title>Tracking Olark chat events with Snowplow</title>
    <updated>2013-06-05T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As some of you will have noticed, we recently installed &lt;a href='http://www.olark.com/'&gt;Olark&lt;/a&gt; on the Snowplow website. Olark powers the chat box you see on the bottom right of the screen: if you click on it, and if one of the Snowplow team is at their desks, you can talk directly to us.&lt;/p&gt;

&lt;p&gt;&lt;img alt='olark-logo' src='/static/img/blog/2013/06/olark/olark-logo.png' /&gt;&lt;/p&gt;

&lt;p&gt;Setting up Olark was an incredibly easy process - the Olark team provides a very straightforward &lt;a href='http://www.olark.com/customer/portal/articles/337453-getting-started-guide'&gt;quick start guide&lt;/a&gt;. We tested Olark for a week on the Snowplow website - we wanted to see:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Whether people visiting the site would want to chat to us.&lt;/li&gt;

&lt;li&gt;Whether the chats were &amp;#8220;useful&amp;#8221; i.e. did they drive visitors to become Snowplow users? Did they help us identify content on the site that was confusing and needed clarification? Does it help drive sales of &lt;a href='http://snowplowanalytics.com/services/index.html'&gt;Snowplow professional services&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What became clear very quickly was that only a minority of visitors to the Snowplow site were interested in chatting. However, the &amp;#8220;usefulness&amp;#8221; of those chats was very high - already two professional services contracts have been won off the back of initial conversations powered by Olark. And in addition, we learnt a lot about where out content needs clarifying.&lt;/p&gt;

&lt;p&gt;The testing of Olark to date has been qualitative however. Longer term, we want to assess the value Olark adds quantitatively. To do this, naturally, we turn to Snowplow. We want to track Olark chat events in Snowplow, so that we can then analyze the Snowplow data to understand:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The type of user behavior on the site that typically leads up to a user reaching out to us on Olark&lt;/li&gt;

&lt;li&gt;The type of user behavior on the site that leads to a user being responsive to us reaching out to them via Olark&lt;/li&gt;

&lt;li&gt;What impact do conversation on Olark have on the visitors subsequent browsing behavior (and likelihood to start using Snowplow and buying Snowplow professional services)?&lt;/li&gt;

&lt;li&gt;How good (quick) we are at responding to visitors reaching out to us via Olark?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this blog post, we will describe how we used &lt;a href='https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#wiki-custom-structured-events'&gt;Snowplow structured events&lt;/a&gt; to track Olark chat events. In a future blog post, we will analyze the data collected in Snowplow, to show how to answer the four questions outlined above. (We&amp;#8217;ll write this once we&amp;#8217;ve been running Olark for a while, so have a good data set to use to peform the analysis.)&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='tracking_olark_chat_events_in_snowplow'&gt;Tracking Olark chat events in Snowplow&lt;/h2&gt;

&lt;p&gt;Olark provides an easy-to-use Javascript API, that lets us call our own Javascript functions when an Olark chat event happens. There are two that are relevant for us - an event that fires when a visitor pings a message to an &amp;#8220;operator&amp;#8221; (i.e. the Snowplow team) via the Olark client:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;olark&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;api.chat.onMessageToOperator&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;event&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='c1'&gt;// our custom function here&lt;/span&gt;
&lt;span class='p'&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&amp;#8230;and an event that fires when an operator pings a message to a visitor:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;olark&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;api.chat.onMessageToVisitor&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;event&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='c1'&gt;// our custom function here&lt;/span&gt;
&lt;span class='p'&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We&amp;#8217;re running Google Tag Manager on the Snowplow website, so we want to push an event into the GTM dataLayer when a chat event happens, so that that event is visible to Snowplow tags configured in GTM. To do this, we add the following Javascript to our page template:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;olark&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;api.chat.onMessageToOperator&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;event&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;dataLayer&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;({&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;event&amp;#39;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;olarkMessageToOperator&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;});&lt;/span&gt;
&lt;span class='p'&gt;});&lt;/span&gt;

&lt;span class='nx'&gt;olark&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;api.chat.onMessageToVisitor&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nx'&gt;event&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;dataLayer&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;({&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;event&amp;#39;&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;olarkMessageToVisitor&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;});&lt;/span&gt;
&lt;span class='p'&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now that chat events trigger notifications in the dataLayer, we can create Snowplow tags via the GTM interface to track these events. We load up GTM, and create a new tag to trigger a &lt;a href='https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#wiki-custom-structured-events'&gt;structured event&lt;/a&gt; when a visitor messages the operator:&lt;/p&gt;

&lt;p&gt;&lt;img alt='gtm-create-tag-1' src='/static/img/blog/2013/06/olark/gtm-create-tag.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;Two things are worth noting about the tag. Firstly, the tag itself is very straightforward:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='c'&gt;&amp;lt;!-- Snowplow event tracking --&amp;gt;&lt;/span&gt;
&lt;span class='nt'&gt;&amp;lt;script &lt;/span&gt;&lt;span class='na'&gt;type=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class='nt'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setCollectorUrl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;collector.snplow.com&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackStructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;contact&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;olark_chat&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;message_to_operator&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nt'&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Secondly, we&amp;#8217;ve created a rule that fires the tag when an event occurs in the dataLayer called &amp;#8216;olarkMessageToOperator&amp;#8217;. This is triggered by Javascript that we added to out page template.&lt;/p&gt;

&lt;p&gt;Now that our first tag is setup, we need to create an analogous tag, one that fires when the operator sends a message to the visitor. (Rather than the visitor sending a message to the operator.) This is done as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img alt='gtm-create-tag-2' src='/static/img/blog/2013/06/olark/gtm-create-tag-2.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;And bingo! We create a new version in Google Tag Manager, and publish the changes. Simple! Now, for every visitor to our website, we will record in granular detail a line of data in Snowplow whenever they send a message to us, and another line of data every time we send a message back. We&amp;#8217;ll be able to analyze:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How quickly we respond to messages from visitors&lt;/li&gt;

&lt;li&gt;When visitors tend respond to messages from us. (When should we reach out, and when we shouldn&amp;#8217;t.)&lt;/li&gt;

&lt;li&gt;What impact chats on Olark have, in aggregate on visitor&amp;#8217;s subsequent behavior on the site. (Are they more likely to engage deeply with the site? Are they more likely to look at our Github repo? Are they more likely to look at the professional services pages? Does any of the above vary by the length of the chat? Or the location of the visitor? Or the amount of time the visitor had spend on our site prior to talking to us via Olark?)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll cover how to perform the above analysis in a future blog post.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements"/>
    <title>Snowplow 0.8.6 released with performance improvements</title>
    <updated>2013-06-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are very pleased to announce the release of Snowplow &lt;strong&gt;0.8.6&lt;/strong&gt;, with two significant performance-related improvements to the Hadoop ETL. These improvements are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hadoop ETL process is now much faster at processing raw Snowplow log files generated by the CloudFront Collector, because we have &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;tackled the Hadoop &amp;#8220;small files problem&amp;#8221;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;You can now configure your ETL process on Elastic MapReduce to use Task instances alongside your Master and Core instances; optionally these task instances can be spot (bid-based) instances rather than on-demand&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#speed-up'&gt;The ETL process speed-up&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#task-instances'&gt;Using task instances in your ETL process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='speed-up'&gt;1. The ETL process speed-up&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are very pleased in this release to finally address &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;Hadoop&amp;#8217;s &amp;#8220;small files problem&amp;#8221;&lt;/a&gt; for Snowplow users relying on our CloudFront Collector. As some of you may know, the CloudFront Collector can generate large numbers of very small files - and this is something that can really impede Hadoop&amp;#8217;s performance.&lt;/p&gt;

&lt;p&gt;With this fix in place, ETL processing speeds will be significantly faster if you previously were processing thousands of smaller CloudFront files. In particularly severe cases, we have seen speed-ups of &lt;strong&gt;1,867%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For more information on Hadoop&amp;#8217;s small files problem, how badly it was slowing down our ETL process and what we did to fix it, do check out our companion blog post, &lt;a href='http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem/'&gt;Dealing with Hadoop&amp;#8217;s small files problem&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name='task-instances'&gt;2. Using task instances in your ETL process&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;With this release you can now add Task instances to your ETL process, alongside your existing Master instance and Core instance(s). The additional configuration options in EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; look like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:jobflow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;...&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_count&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0&lt;/span&gt; &lt;span class='c1'&gt;# Increase to use spot instances&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_bid&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.015&lt;/span&gt; &lt;span class='c1'&gt;# In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the &lt;code&gt;:task_instance_bid:&lt;/code&gt; variable - this lets you bid an upper bound (in US Dollars) that you are willing to pay for Task instances to be added to your job. Leave this blank if you would prefer on-demand Task instances at the standard EMR prices.&lt;/p&gt;

&lt;p&gt;The best introduction to configuring the various instance groups for your job (Master, Core and Task) is in the post &lt;a href='http://aws.typepad.com/aws/2011/08/run-amazon-elastic-mapreduce-on-ec2-spot-instances.html'&gt;Run Amazon Elastic MapReduce on EC2 Spot Instances&lt;/a&gt; on the Amazon Web Services Blog. In the language of this blog post, the Snowpow ETL process is typically a &lt;strong&gt;Data-Critical Workload&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;#8220;If the overall cost is more important than the time to completion and you don&amp;#8217;t want to lose any partial work, run the Master and Core instance groups on On-Demand instances, making sure that you run enough Core instance groups to hold all of your data in HDFS. Add Spot Instances as needed to reduce the overall processing speed and the total cost.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We recommend you experiment with different Task instance configurations (including different bids) to find the best cost-time balance for you.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;two components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.2&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.3.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.2&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.6 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.6&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, you need to update the format of your &lt;code&gt;config.yml&lt;/code&gt;, specifically the &lt;code&gt;:jobflow:&lt;/code&gt; section. The new format looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:jobflow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:master_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:core_instance_count&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;2&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:core_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_count&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0&lt;/span&gt; &lt;span class='c1'&gt;# Increase to use spot instances&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_type&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;m1.small&lt;/span&gt;
    &lt;span class='l-Scalar-Plain'&gt;:task_instance_bid&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.015&lt;/span&gt; &lt;span class='c1'&gt;# In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note that almost all of these variables are either new or renamed.&lt;/strong&gt; For recommended settings for the new &lt;code&gt;task_instance&lt;/code&gt; settings, please see &lt;a href='/blog/2013/06/03/snowplow-0.8.6-released-with-performance-improvements#task-instances'&gt;Using task instances in your ETL process&lt;/a&gt; above.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.6 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=22&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/30/dealing-with-hadoops-small-files-problem"/>
    <title>Dealing with Hadoop's small files problem</title>
    <updated>2013-05-30T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Hadoop has a serious Small File Problem. It&amp;#8217;s widely known that Hadoop struggles to run MapReduce jobs that involve thousands of small files: Hadoop much prefers to crunch through tens or hundreds of files sized at or around the magic 128 megabytes. The technical reasons for this are well explained in this &lt;a href='http://blog.cloudera.com/blog/2009/02/the-small-files-problem/'&gt;Cloudera blog post&lt;/a&gt; - what is less well understood is how badly small files can slow down your Hadoop job, and what to do about it.&lt;/p&gt;
&lt;img src='/static/img/blog/2013/05/plowing-small-files.jpg' /&gt;
&lt;p&gt;In this blog post we will discuss the small files problem in terms of our experiences with it at Snowplow. &lt;strong&gt;And we will argue that dealing with the small files problem - if you have it - is the single most important optimisation you can perform on your MapReduce process.&lt;/strong&gt;&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='background'&gt;Background&lt;/h2&gt;

&lt;p&gt;To give some necessary background on our architecture: Snowplow event trackers send user events to a pixel hosted on CloudFront, which logs those raw events to Amazon S3. Amazon&amp;#8217;s CloudFront logging generates many small log files in S3: a relatively low-traffic e-commerce site using Snowplow generated 26,372 CloudFront log files over a six month period, containing 83,110 events - that&amp;#8217;s just 3.2 events per log file.&lt;/p&gt;

&lt;p&gt;Once the events have been collected in S3, &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-hadoop-enrich'&gt;Snowplow&amp;#8217;s Hadoop job&lt;/a&gt; (written in &lt;a href='https://github.com/twitter/scalding/wiki'&gt;Scalding&lt;/a&gt;) processes them, validating them and then enriching them with referer, geo-location and similar data; these enriched events are then written back to S3.&lt;/p&gt;

&lt;p&gt;So you can see how our Enrichment process ran pretty directly into Hadoop&amp;#8217;s small files problem. But quantifying the impact of small files on our job&amp;#8217;s performance was impossible until we had a solution in place&amp;#8230;&lt;/p&gt;

&lt;h2 id='quantifying_the_small_file_problem'&gt;Quantifying the small file problem&lt;/h2&gt;

&lt;p&gt;This week we implemented a solution to aggregate our tiny CloudFront logs into more sensibly sized input files - this enhancement will be released as part of &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=22&amp;amp;page=1&amp;amp;state=open'&gt;Snowplow 0.8.6&lt;/a&gt; shortly.&lt;/p&gt;

&lt;p&gt;In testing this code and running before- and after- performance tests, we realised just how badly the small file problem was slowing down our Enrichment process. This screenshot shows you what we found:&lt;/p&gt;
&lt;img src='/static/img/blog/2013/05/small-files-before-after.png' /&gt;
&lt;p&gt;That&amp;#8217;s right - aggregating with the small files first reduced total processing time from 2 hours 57 minutes to just 9 minutes - of which 3 minutes was the aggregation, and 4 minutes was running our actual Enrichment process. That&amp;#8217;s a speedup of &lt;strong&gt;1,867%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To make the comparison as helpful as possible, here is the exact specification of the before- and after- test. We have also added a second after- run with a more realistic cluster size.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Metric&lt;/th&gt;&lt;th&gt;Before (with small files)&lt;/th&gt;&lt;th&gt;After (with small files aggregated)&lt;/th&gt;&lt;th&gt;After (with smaller cluster)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Source log files&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;&lt;td style='text-align: left;'&gt;26,372&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Files read by job&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Source log files&lt;/td&gt;&lt;td style='text-align: left;'&gt;Aggregated log files&lt;/td&gt;&lt;td style='text-align: left;'&gt;Aggregated log files&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Location of files&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Amazon S3&lt;/td&gt;&lt;td style='text-align: left;'&gt;HDFS on Core instances&lt;/td&gt;&lt;td style='text-align: left;'&gt;HDFS on Core instances&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;File compression&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Gzip&lt;/td&gt;&lt;td style='text-align: left;'&gt;LZO&lt;/td&gt;&lt;td style='text-align: left;'&gt;LZO&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;part- files out&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;23,618&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Events out&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;&lt;td style='text-align: left;'&gt;83,110&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Cluster&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.large, 18 x m1.medium&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.large, 18 x m1.medium&lt;/td&gt;&lt;td style='text-align: left;'&gt;1 x m1.small, 1 x m1.small&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Execution time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;177 minutes&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;9 minutes&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;39 minutes&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Aggregate step time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;3 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;11 minutes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;ETL step time&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;166 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;4 minutes&lt;/td&gt;&lt;td style='text-align: left;'&gt;25 minutes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;strong&gt;Norm. instance hours&lt;/strong&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;120&lt;/td&gt;&lt;td style='text-align: left;'&gt;40&lt;/td&gt;&lt;td style='text-align: left;'&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Health warning:&lt;/strong&gt; this is one single benchmark, measuring the performance of the &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-hadoop-enrich'&gt;Snowplow Hadoop job&lt;/a&gt; using a single data set. We encourage you to run your own benchmarks.&lt;/p&gt;

&lt;p&gt;This is an astonishing speed-up, which shows how badly the small files problem was impacting our Hadoop job. And aggregating the small files had another beneficial effect: the much smaller number of &lt;code&gt;part-&lt;/code&gt; output files meant much faster loading of events into Redshift.&lt;/p&gt;

&lt;p&gt;So how did we fix the small files problem for Snowplow? In the next section we will discuss possible solutions for you to consider, and in the last section we will go into some more detail on the solution we chose.&lt;/p&gt;

&lt;h2 id='options_for_dealing_with_small_files_on_hadoop'&gt;Options for dealing with small files on Hadoop&lt;/h2&gt;

&lt;p&gt;As we did our background research into solutions to the small files problem, three main schools of thought emerged:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Change your &amp;#8220;feeder&amp;#8221; software&lt;/strong&gt; so it doesn&amp;#8217;t produce small files (or perhaps files at all). In other words, if small files are the problem, change your upstream code to stop generating them&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Run an offline aggregation process&lt;/strong&gt; which aggregates your small files and re-uploads the aggregated files ready for processing&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Add an additional Hadoop step&lt;/strong&gt; to the start of your jobflow which aggregates the small files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For us, option 1 was out of the question, as we have no control over how CloudFront writes its log files.&lt;/p&gt;

&lt;p&gt;Option 2 was interesting - and we have had Snowplow users such as 99designs successfully adopt this approach; if you are interested in exploring this further, &lt;a href='https://github.com/larsyencken'&gt;Lars Yencken&lt;/a&gt; from 99designs has shared a &lt;a href='https://gist.github.com/larsyencken/4076413'&gt;CloudFront log aggregation script in Python&lt;/a&gt; as a gist. However, overall option 2 seemed to us to introduce more complexity - with a new long-running process to run, and potentially fragility - with a manifest file now to maintain. We had super-interesting discussions with the Snowplow community about this in &lt;a href='https://groups.google.com/forum/#!topic/snowplow-user/xdhegsztJlA'&gt;this Google Groups thread&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/82'&gt;this GitHub issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the end, though, we opted for option 3, for a few reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We are starting a Hadoop cluster anyway to run our ETL job, so this reduces the number of additional moving parts required&lt;/li&gt;

&lt;li&gt;We hoped for performance improvements in moving the small files to the cluster&amp;#8217;s local HDFS filesystem during the aggregation&lt;/li&gt;

&lt;li&gt;We hoped that accessing Amazon S3 from a cluster rather than a single machine would mean more parallel connections to S3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that decided, we then looked for options to aggregate and compact small files on Hadoop, identifying three possible solutions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/edwardcapriolo/filecrush'&gt;&lt;strong&gt;filecrush&lt;/strong&gt;&lt;/a&gt; - a highly configurable tool by &lt;a href='https://github.com/edwardcapriolo'&gt;Edward Capriolo&lt;/a&gt; to &amp;#8220;crush&amp;#8221; small files on HDFS. It supports a rich set of configuration arguments and is available as a jarfile (&lt;a href='http://www.jointhegrid.com/hadoop_filecrush/'&gt;download it here&lt;/a&gt;) ready to run on your cluster. It&amp;#8217;s a sophisticated tool - for example, by default it won&amp;#8217;t bother crushing a file which is within 75% of the HDFS block size already. Unfortunately, it does not work yet with Amazon&amp;#8217;s s3:// paths, only hdfs:// paths - and our &lt;a href='https://github.com/edwardcapriolo/filecrush/pull/2'&gt;pull request&lt;/a&gt; to add this functionality is incomplete&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/nathanmarz/dfs-datastores/blob/develop/dfs-datastores/src/main/java/com/backtype/hadoop/Consolidator.java'&gt;&lt;strong&gt;Consolidator&lt;/strong&gt;&lt;/a&gt; - a Hadoop file consolidation tool from the &lt;a href='https://github.com/nathanmarz/dfs-datastores'&gt;dfs-datastores&lt;/a&gt; library, written by &lt;a href='https://github.com/nathanmarz'&gt;Nathan Marz&lt;/a&gt;. There is scant documentation for this - we could only find one paragraph, &lt;a href='https://groups.google.com/forum/?fromgroups#!topic/cascalog-user/ovYSq2vTyYE'&gt;in this email thread&lt;/a&gt;. It has fewer capabilities than filecrush, and could do with a CLI-like wrapper to invoke it (we started writing just such a wrapper but then we found filecrush)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html'&gt;&lt;strong&gt;S3DistCp&lt;/strong&gt;&lt;/a&gt; - created by Amazon as an S3-friendly adaptation of Hadoop&amp;#8217;s &lt;a href='http://hadoop.apache.org/docs/stable/distcp.html'&gt;DistCp&lt;/a&gt; utility for HDFS. Don&amp;#8217;t be fooled by the name - if you are running on Elastic MapReduce, then this can deal with your small files problem using its &lt;code&gt;--groupBy&lt;/code&gt; option for aggregating files (which the original DistCp seems to lack)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After trying to work with filecrush and Consolidator, ultimately we went with S3DistCp for Snowplow. In the next section, we will look at exactly how we set it up.&lt;/p&gt;

&lt;h2 id='running_s3distcp'&gt;Running S3DistCp&lt;/h2&gt;

&lt;p&gt;Once we had chosen S3DistCp, we had to update our ETL process to include it in our jobflow. Luckily, the &lt;a href='http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html'&gt;S3DistCp documentation&lt;/a&gt; has an example on aggregating CloudFront files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./elastic-mapreduce --jobflow j-3GY8JC4179IOK --jar \
/home/hadoop/lib/emr-s3distcp-1.0.jar \
--args &amp;#39;--src,s3://myawsbucket/cf,\
--dest,hdfs:///local,\
--groupBy,.*XABCD12345678.([0-9]+-[0-9]+-[0-9]+-[0-9]+).*,\
--targetSize,128,\
--outputCodec,lzo,--deleteOnSuccess&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that as well as aggregating the small files into 128 megabyte files, this step also changes the encoding to &lt;a href='http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer'&gt;LZO&lt;/a&gt;. As the Amazon documentation explains it:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;#8220;Data compressed using LZO can be split into multiple maps as it is decompressed, so you don&amp;#8217;t have to wait until the compression is complete, as you do with Gzip. This provides better performance when you analyze the data using Amazon EMR.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We only needed to make a few changes to this example code for our own ETL:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We can&amp;#8217;t predict the prefix on the CloudFront log files - and it certainly won&amp;#8217;t be &lt;code&gt;XABCD12345678&lt;/code&gt;, so it made more sense to drop this&lt;/li&gt;

&lt;li&gt;Grouping files to the hour is overkill - we can roll up to the day and S3DistCp will happily split files if they are larger than &lt;code&gt;targetSize&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;--deleteOnSuccess&lt;/code&gt; is dangerous - we don&amp;#8217;t want to delete our source data and leave the only copy on a transient Hadoop cluster&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Given the above, our updated &lt;code&gt;--groupBy&lt;/code&gt; regular expression was:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;.*\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\..*&amp;quot;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, all we needed to do was add the call to S3DistCp into our jobflow before our main ETL step. We use the excellent &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; Ruby library by &lt;a href='https://github.com/rslifka'&gt;Rob Slifka&lt;/a&gt; to execute our jobs, so calling S3DistCp was a matter of adding the extra step to our jobflow in Ruby:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='n'&gt;hadoop_input&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;hdfs:///local/snowplow-logs&amp;quot;&lt;/span&gt;

&lt;span class='c1'&gt;# Create the Hadoop MR step for the file crushing&lt;/span&gt;
&lt;span class='n'&gt;filecrush_step&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='ss'&gt;Elasticity&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt;&lt;span class='ss'&gt;:CustomJarStep&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;/home/hadoop/lib/emr-s3distcp-1.0.jar&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; 

&lt;span class='n'&gt;filecrush_step&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;arguments&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--src&amp;quot;&lt;/span&gt;               &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:processing&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--dest&amp;quot;&lt;/span&gt;              &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;hadoop_input&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--groupBy&amp;quot;&lt;/span&gt;           &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;.*&lt;/span&gt;&lt;span class='se'&gt;\\&lt;/span&gt;&lt;span class='s2'&gt;.([0-9]+-[0-9]+-[0-9]+)-[0-9]+&lt;/span&gt;&lt;span class='se'&gt;\\&lt;/span&gt;&lt;span class='s2'&gt;..*&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--targetSize&amp;quot;&lt;/span&gt;        &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;128&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--outputCodec&amp;quot;&lt;/span&gt;       &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;lzo&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='s2'&gt;&amp;quot;--s3Endpoint&amp;quot;&lt;/span&gt;        &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:endpoint&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='o'&gt;]&lt;/span&gt;

&lt;span class='c1'&gt;# Add to our jobflow&lt;/span&gt;
&lt;span class='vi'&gt;@jobflow&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;add_step&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;filecrush_step&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then we had to update our ETL job to take the &lt;code&gt;--dest&lt;/code&gt; of the S3DistCp step as its own input:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='n'&gt;hadoop_step&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;arguments&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='o'&gt;[&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Job to run&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--hdfs&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Always --hdfs mode, never --local&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--input_folder&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;hadoop_input&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;# Output of our S3DistCp step&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--input_format&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:etl&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:collector_format&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--maxmind_file&amp;quot;&lt;/span&gt;      &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:maxmind_asset&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--output_folder&amp;quot;&lt;/span&gt;     &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;partition&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;call&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:out&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt;
            &lt;span class='s2'&gt;&amp;quot;--bad_rows_folder&amp;quot;&lt;/span&gt;   &lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;partition&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;call&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;config&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:s3&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:buckets&lt;/span&gt;&lt;span class='o'&gt;][&lt;/span&gt;&lt;span class='ss'&gt;:out_bad_rows&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
          &lt;span class='o'&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that was it! If you want to see all of the code excerpted above, you can find it in the &lt;a href='https://github.com/snowplow/snowplow/blob/feature/perf-improvements/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_jobs.rb'&gt;Snowplow project on GitHub&lt;/a&gt;. We did not have to make any changes to our main Hadoop ETL job, because Elastic MapReduce can handle LZO-compressed files invisibly to the job reading them. And no doubt the switch to LZO also contributed to the excellent performance we saw above.&lt;/p&gt;

&lt;p&gt;So that&amp;#8217;s everything - hopefully this post has helped to illustrate just how badly small files can slow down your Hadoop job, and what you can do about it: if you have a small file problem on Hadoop, there&amp;#8217;s now no excuse not to fix it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes"/>
    <title>Snowplow 0.8.5 released with ETL bug fixes</title>
    <updated>2013-05-24T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.5&lt;/strong&gt;. This is a bug fixing release, following on from our launch last week of Snowplow 0.8.4 with geo-IP lookups.&lt;/p&gt;

&lt;p&gt;This release fixes one showstopper issue with Snowplow 0.8.4, and also includes a set of smaller enhancements to help the Scalding ETL better handle &amp;#8220;bad quality&amp;#8221; event data from webpages. We recommend everybody on the Snowplow 0.8.x series upgrade to this version.&lt;/p&gt;

&lt;p&gt;Many thanks to community members &lt;a href='https://github.com/petervanwesep'&gt;Peter van Wesep&lt;/a&gt; and &lt;a href='https://github.com/rgabo'&gt;Gabor Ratky&lt;/a&gt; for their help identifying and debugging the issues fixed in this release!&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#showstopper'&gt;The showstopper bug fix&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#other-improvements'&gt;Other improvements&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/24/snowplow-0.8.5-released-with-etl-bug-fixes#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='showstopper'&gt;1. The showstopper bug fix&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Many thanks to Peter van Wesep for spotting the &lt;a href='https://github.com/snowplow/snowplow/issues/258'&gt;showstopper issue&lt;/a&gt; in the Snowplow 0.8.4 release: when the Snowplow ETL process was run from an Amazon Web Services account other than Snowplow&amp;#8217;s own, the Hadoop ETL code was unable to read the MaxMind geo-IP data file from an S3:// link hosted from a Snowplow public bucket. This issue did not affect users who are self-hosting the ETL assets.&lt;/p&gt;

&lt;p&gt;This has now been fixed - we now provide the MaxMind geo-IP file on an HTTP:// link, and the Scalding ETL downloads it and adds it to HDFS before running.&lt;/p&gt;
&lt;h2&gt;&lt;a name='other-improvements'&gt;2. Other improvements&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We have made a series of other improvements to the Scalding ETL, to make it more robust. These improvements are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We have widened the &lt;code&gt;page_urlport&lt;/code&gt; and &lt;code&gt;refr_urlport&lt;/code&gt; fields&lt;/li&gt;

&lt;li&gt;We now strip control characters (e.g. nulls) from fields alongside tabs and newlines, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;The ETL no longer dies if a huge (larger than an integer) numeric value is sent in for a screen/view dimension&lt;/li&gt;

&lt;li&gt;We have increased the size of &lt;code&gt;se_value&lt;/code&gt; from a float to a double&lt;/li&gt;

&lt;li&gt;&lt;code&gt;se_value&lt;/code&gt; is always now output as a plain string, never in scientific notation, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;It is now possible to build the ETL locally (we added a missing dependency to the project configuration)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;three components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.1&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.2.1&lt;/li&gt;

&lt;li&gt;The Redshift events table, to version 0.2.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively, if you are still using Infobright with the legacy Hive ETL, you can upgrade your Infobright events table, to version 0.0.9.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:snowplow&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:hadoop_etl_version&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;0.3.1&lt;/span&gt; &lt;span class='c1'&gt;# Version of the Hadoop ETL&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.5 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.5&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition - you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.2.0_to_0.2.1.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='infobright_events_table'&gt;Infobright events table&lt;/h3&gt;

&lt;p&gt;If you are storing your events in Infobright Community Edition, you can also update your table definition. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_008_to_009.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_009&lt;/code&gt; (version 0.0.9 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_008&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_009&lt;/code&gt; table, not your old &lt;code&gt;events_008&lt;/code&gt; table:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;:storage&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;:table&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;    &lt;span class='l-Scalar-Plain'&gt;events_009&lt;/span&gt; &lt;span class='c1'&gt;# NOT &amp;quot;events_008&amp;quot; any more&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.5 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=24&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/22/measuring-how-much-individual-items-in-your-catalog-contribute-to-inbound-marketing"/>
    <title>Measuring how much traffic individual items in your catalog drive to your website</title>
    <updated>2013-05-22T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just added a &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;new recipe&lt;/a&gt; to the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;. This recipe describes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to measure how effectively different items in your catalog drive visits to your website.&lt;/li&gt;

&lt;li&gt;How to use the data to unpick &lt;em&gt;how&lt;/em&gt; each item drives that traffic.&lt;/li&gt;
&lt;/ol&gt;
&lt;a href='/static/img/analytics/catalog-analytics/driving-traffic/2.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/driving-traffic/2-truncated.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;In digital marketing, we can distinguish classic &amp;#8220;outbound marketing&amp;#8221;, where we &lt;em&gt;push&lt;/em&gt; visitors to our website using paid ad campaigns, for example, with &amp;#8221;&lt;a href='http://en.wikipedia.org/wiki/Inbound_marketing'&gt;inbound market&lt;/a&gt;&amp;#8221;, where we &lt;em&gt;pull&lt;/em&gt; visitors into our site by producing attractive, linkable content.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Each item in our catalog (be it a product on a retail site, an article on a news website, a post on a blog or a video on a media site) has the potential to contribute to inbound marketing, by:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Providing new content that people might create links to&lt;/li&gt;

&lt;li&gt;Providing additional content, including keywords and links for search engine crawlers&lt;/li&gt;

&lt;li&gt;Providing additional potential landing pages to run advertising against.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This recipe shows how we can measure how effectively different items contribute to inbound marketing, and drill into the data to explore how it contributes. (Which of the three effects above does it support.)&lt;/p&gt;

&lt;p&gt;In our previous recipe for &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;measuring product page performance&lt;/a&gt;, we showed how we can measure how effectively product pages on a retailer site drive &lt;em&gt;add-to-basket&lt;/em&gt; events. In our previous recipe for measuring &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;content page performance&lt;/a&gt;, we showed how to measure how much engagement different individual content items drive. &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;This recipe&lt;/a&gt; builds on the two previous recipes, by showing how we can measure how good each item is at drawing visitors to our website.&lt;/p&gt;

&lt;p&gt;The recipe uses &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt;. It is, however, possible to use alternative OLAP or analysis tools (e.g. &lt;a href='http://www.microsoft.com/en-us/bi/PowerPivot.aspx'&gt;Microsoft PowerPivot&lt;/a&gt; or &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;): the same methodology applies.&lt;/p&gt;

&lt;p&gt;The recipe is available &lt;a href='/analytics/catalog-analytics/measuring-how-much-traffic-different-items-in-your-catalog-drive-to-your-website.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/20/performing-market-basket-analysis-with-r-arules-and-snowplow"/>
    <title>Performing market basket analysis on web analytics data with R</title>
    <updated>2013-05-20T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just added a &lt;a href='/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html'&gt;new recipe&lt;/a&gt; to the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;: one that walks through the process of performing a &lt;a href='/analytics/catalog-analytics/market-basket-analysis-identifying-products-that-sell-well-together.html'&gt;market basket analysis&lt;/a&gt;, to identify associations between products and/or content items based on user purchase / viewing behavior. The recipe covers performing the analysis on Snowplow data using &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt; and the &lt;a href='http://cran.r-project.org/web/packages/arules/index.html'&gt;arules&lt;/a&gt; package in particular. Although the example walked through uses Snowplow data, the same approach &lt;em&gt;can&lt;/em&gt; be used with other data sets: I&amp;#8217;d be interested in finding out if members of the #measure community can describe how to do the comparable analysis using data from Google Analytics.&lt;/p&gt;
&lt;p style='text-align: center'&gt;&lt;img src='/static/img/analytics/catalog-analytics/market-basket-analysis/market-basket-analysis-scatter-plot-arulesviz.JPG' width='400' /&gt;&lt;/p&gt;
&lt;p&gt;Market basket analysis is the mining of transaction data to identify associations between different items. This is typically performed by retailers who use it to identify products that a customer is likely to buy, given the products that they have already bought (or added to basket): most famously, it is the approach behind Amazon&amp;#8217;s &lt;em&gt;users who bought this product also bought these items&lt;/em&gt;&amp;#8230;&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Although people usually think of market basket analysis with respect to retailer transaction data, exactly the same algorithms and approaches can be uses with viewing data on media sites. The results of this type of analysis can be used to inform website design (how items are grouped together) and to power recommendation engines and targeted marketing. (E.g. advertising content items or products that people are more likely to be interested in, based on their past behavior.)&lt;/p&gt;

&lt;p&gt;This is the first recipe that primarily uses &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;. We&amp;#8217;re big fans of R at Snowplow, and a big motivation in building Snowplow was to enable the use of sophisticated data analysis tools like R on granular event-level data. We have a number of other recipes for R we hope to publish in the near future. This is the third recipe added to the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, if there are specific types of analysis you&amp;#8217;d like us to cover, then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;, either directly or by dropping us a comment below.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip"/>
    <title>Snowplow 0.8.4 released with MaxMind geo-IP lookups</title>
    <updated>2013-05-16T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.4&lt;/strong&gt;. This is a big release, which adds geo-IP lookups to the Snowplow Enrichment stage, using the excellent &lt;a href='http://dev.maxmind.com/geoip/legacy/geolite'&gt;GeoLite City database&lt;/a&gt; from &lt;a href='http://www.maxmind.com'&gt;MaxMind, Inc&lt;/a&gt;. This has been one of the most requested features from the Snowplow community, so we are delighted to launch it. Now you can determine the location of your website visitors directly from the Snowplow events table, and plot that data on a wide range of mapping tools including &lt;a href='http://kb.tableausoftware.com/articles/knowledgebase/mapping-basics'&gt;Tableau&lt;/a&gt; or &lt;a href='http://wrobstory.github.io/2013/04/python-maps-chloropleth.html'&gt;Vincent&lt;/a&gt;:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/pbz-global-visitors.jpg'&gt;&lt;img src='/static/img/blog/2013/05/pbz-global-visitors.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the image above to enlarge it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here is some example geo-IP data:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/geoip-data.png'&gt;&lt;img src='/static/img/blog/2013/05/geoip-data.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the image above to enlarge it&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As well as geo-IP enrichment, there are a number of other code improvements to the Hadoop ETL, plus some minor improvements to EmrEtlRunner and some corresponding updates to the Redshift table. In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#geoip'&gt;The new geo-IP capabilities&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#other-changes'&gt;Other changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/16/snowplow-0.8.4-released-with-maxmind-geoip#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='geoip'&gt;1. The new geo-IP capabilities&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When we released Snowplow 0.8.0 back in April, we promised that the new Scalding-based ETL process would provide a solid bedrock on which we could build a bunch of data enrichments to perform on the raw Snowplow logs to make Snowplow data more interesting to analyse. With version 0.8.1, we included a referer parsing enrichment, which looked up external page referers against a database of search engines and social networks, and used associatd rules to infer additional data about what drove those visitors to your site. This release adds our second big enrichment: the ETL process now lookups every IP address against MaxMind&amp;#8217;s &lt;a href='http://dev.maxmind.com/geoip/legacy/geolite'&gt;GeoLite City database&lt;/a&gt; in order to determine the location of a visitors - specifically (wherever possible):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;geo_country&lt;/code&gt; - the two-letter &lt;a href='http://en.wikipedia.org/wiki/ISO_3166-1_alpha-2'&gt;ISO 3166-1 alpha-2&lt;/a&gt; country code associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_region&lt;/code&gt; - the two letter &lt;a href='http://en.wikipedia.org/wiki/ISO_3166-2'&gt;ISO-3166-2&lt;/a&gt; or &lt;a href='http://en.wikipedia.org/wiki/FIPS_10-4'&gt;FIPS 10-4&lt;/a&gt; code for the state or region associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_city&lt;/code&gt; - the city or town name associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_zipcode&lt;/code&gt; - the zip or postal code associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_latitude&lt;/code&gt; - the latitude associated with the IP address&lt;/li&gt;

&lt;li&gt;&lt;code&gt;geo_longitude&lt;/code&gt; - the longitude associated with the IP address&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information on these six fields we recommend reading the &lt;a href='http://dev.maxmind.com/geoip/legacy/csv'&gt;GeoIP CSV Databases technical reference&lt;/a&gt; on the MaxMind website.&lt;/p&gt;

&lt;p&gt;Providing all this data directly in the Snowplow events table makes it easy to create geographic maps like the one below using Snowplow data directly: we will cover how to do this in a forthcoming blog post.&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/pbz-europe-visitors.jpg'&gt;&lt;img src='/static/img/blog/2013/05/pbz-europe-visitors.jpg' /&gt;&lt;/a&gt;&lt;h2&gt;&lt;a name='other-changes'&gt;2. Other changes&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In this release we have also made some functional improvements to the Hadoop (Scalding) ETL, plus some minor improvements to EmrEtlRunner and some updates to the Redshift table. To take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;We have made various improvements to the Hadoop ETL:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have bumped the version of &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; - the latest version includes a fix to better attribute Google referer URLs&lt;/li&gt;

&lt;li&gt;We have added truncation of &lt;code&gt;refr_urlpath&lt;/code&gt;, &lt;code&gt;refr_urlquery&lt;/code&gt; and &lt;code&gt;urlfragment&lt;/code&gt;, to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;We now remove tabs and newlines from referer search terms (&lt;code&gt;refr_term&lt;/code&gt;), again to prevent Redshift load errors&lt;/li&gt;

&lt;li&gt;We have fixed a nasty bug where the client timestamp was being inaccurately localised to the Hadoop cluster&amp;#8217;s local time (&lt;a href='https://github.com/snowplow/snowplow/issues/238'&gt;issue #238&lt;/a&gt;) - thanks &lt;a href='https://github.com/rgabo'&gt;Gabor&lt;/a&gt; for spotting this&lt;/li&gt;

&lt;li&gt;We have made the code around page URL extraction more robust in the case that a page URL cannot be extracted&lt;/li&gt;

&lt;li&gt;If you are running the latest version of the Clojure Collector, then the specific version number will now be extracted into the &lt;code&gt;v_collector&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;We have updated EmrEtlRunner to supply the location of the MaxMind GeoLite City database to the Scalding ETL.&lt;/p&gt;

&lt;p&gt;We have also improved the notification messages when the ETL job is started on Elastic MapReduce, and the notification message if the job should fail.&lt;/p&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift events table to include new fields for the geo-IP location - see &lt;a href='#geoip'&gt;above&lt;/a&gt; for the six new field names.&lt;/p&gt;

&lt;p&gt;Also, we have renamed the five &lt;code&gt;ev_&lt;/code&gt; fields in the Redshift table definition to start with &lt;code&gt;se_&lt;/code&gt;, e.g. &lt;code&gt;se_action&lt;/code&gt;. This is to make these column names consistent with our structured events terminology.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;3. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are &lt;strong&gt;three components&lt;/strong&gt; to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Scalding ETL, to version 0.3.0&lt;/li&gt;

&lt;li&gt;EmrEtlRunner, to version 0.2.0&lt;/li&gt;

&lt;li&gt;The Redshift events table, to version 0.2.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s take these in turn:&lt;/p&gt;

&lt;h3 id='hadoop_etl'&gt;Hadoop ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.3.0 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='emretlrunner'&gt;EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;You need to upgrade your EmrEtlRunner installation to the latest code (0.8.4 release) on GitHub:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/snowplow/snowplow.git
$ git checkout 0.8.4&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='redshift_events_table'&gt;Redshift events table&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition - you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.1.0_to_0.2.0.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.4 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=17&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/14/snowplow-unstructured-events-guide</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/14/snowplow-unstructured-events-guide"/>
    <title>A guide to unstructured events in Snowplow 0.8.3</title>
    <updated>2013-05-14T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier today we &lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events'&gt;announced the release of Snowplow 0.8.3&lt;/a&gt;, which updated our JavaScript Tracker to add the ability to send custom unstructured events to a Snowplow collector with &lt;code&gt;trackUnstructEvent()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In our earlier blog post we briefly introduced the capabilities of &lt;code&gt;trackUnstructEvent&lt;/code&gt; with some example code. In this blog post, we will take a detailed look at Snowplow&amp;#8217;s custom unstructured events functionality, so you can understand how best to send unstructured events to Snowplow.&lt;/p&gt;

&lt;p&gt;Understanding the unstructured event format is important because our Enrichment process does not yet extract unstructured events, so you will not get any feedback yet from the ETL as to whether you are tracking them correctly. (Nor do we have validation for unstructured event properties in our JavaScript Tracker yet.)&lt;/p&gt;

&lt;p&gt;In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#basic-usage'&gt;Basic usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#properties-object'&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript object&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#supported-datatypes'&gt;Supported datatypes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='basic-usage'&gt;1. Basic usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Tracking an unstructured event with the JavaScript Tracker is very straightforward - use the &lt;code&gt;trackUnstructEvent(name, properties)&lt;/code&gt; function:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; is the name of the unstructured event. This is case-sensitive; spaces etc are allowed&lt;/li&gt;

&lt;li&gt;&lt;code&gt;properties&lt;/code&gt; is a JavaScript object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackUnstructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Viewed Product&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='p'&gt;{&lt;/span&gt;
                    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;brand&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ACME&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;returning&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;price&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.95&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
                    &lt;span class='nx'&gt;available_since$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;7&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
            &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Every call to &lt;code&gt;trackUnstructEvent&lt;/code&gt; has the same structure - the complexity comes from knowing how to structure the &lt;code&gt;properties&lt;/code&gt; JavaScript object. We will discuss this next:&lt;/p&gt;
&lt;h2&gt;&lt;a name='properties-object'&gt;2. The 'properties' JavaScript object&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript consists of a set of individual &lt;code&gt;name: value&lt;/code&gt; properties.&lt;/p&gt;

&lt;p&gt;The structure must be flat - in other words, properties cannot be nested. Continuing with the exampe code above, this means that the following is &lt;strong&gt;not&lt;/strong&gt; allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt; &lt;span class='nx'&gt;primary&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Womenswear&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='nx'&gt;secondary&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;},&lt;/span&gt; &lt;span class='c1'&gt;// NOT allowed&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;properties&lt;/code&gt; JavaScript object supports a wide range of datatypes - see below for details.&lt;/p&gt;
&lt;h2&gt;&lt;a name='supported-datatypes'&gt;3. Supported datatypes&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow unstructured events support a relatively rich set of datatypes. Because these datatypes do not always map directly onto JavaScript datatypes, we have introduced some &amp;#8220;type suffixes&amp;#8221; for the JavaScript property names, to tell Snowplow what Snowplow datatype we want the JavaScript data to map onto.&lt;/p&gt;

&lt;p&gt;Our datatypes, then, are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Snowplow datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;JavaScript datatype&lt;/th&gt;&lt;th&gt;Type suffix(es)&lt;/th&gt;&lt;th&gt;Supports array?&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Null&lt;/td&gt;&lt;td style='text-align: left;'&gt;Absence of a value&lt;/td&gt;&lt;td style='text-align: left;'&gt;Null&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;String&lt;/td&gt;&lt;td style='text-align: left;'&gt;String of characters&lt;/td&gt;&lt;td style='text-align: left;'&gt;String&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Boolean&lt;/td&gt;&lt;td style='text-align: left;'&gt;True or false&lt;/td&gt;&lt;td style='text-align: left;'&gt;Boolean&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number without decimal&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$int&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Floating point&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number with decimal&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$flt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Geo-coordinates&lt;/td&gt;&lt;td style='text-align: left;'&gt;Longitude and latitude&lt;/td&gt;&lt;td style='text-align: left;'&gt;[Number, Number]&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$geo&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Date&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date and time (ms precision)&lt;/td&gt;&lt;td style='text-align: left;'&gt;Number&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;$dt&lt;/code&gt;, &lt;code&gt;$ts&lt;/code&gt;, &lt;code&gt;$tms&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Array&lt;/td&gt;&lt;td style='text-align: left;'&gt;Array of values&lt;/td&gt;&lt;td style='text-align: left;'&gt;[x, y, z]&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Let&amp;#8217;s go through each of these in turn, providing some examples as we go:&lt;/p&gt;

&lt;h3 id='null'&gt;Null&lt;/h3&gt;

&lt;p&gt;Tracking a Null value for a given field is straightforward:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;returns_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='string'&gt;String&lt;/h3&gt;

&lt;p&gt;Tracking a String is easy:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt; &lt;span class='c1'&gt;// Or &amp;quot;ASO01043&amp;quot;&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='boolean'&gt;Boolean&lt;/h3&gt;

&lt;p&gt;Tracking a Boolean is also straightforward:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;trial&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='integer'&gt;Integer&lt;/h3&gt;

&lt;p&gt;To track an Integer, use a JavaScript Number but add a type suffix like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;in_stock$int&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;23&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; if you do not add the &lt;code&gt;$int&lt;/code&gt; type suffix, Snowplow will assume you are tracking a Floating point number.&lt;/p&gt;

&lt;h3 id='floating_point'&gt;Floating point&lt;/h3&gt;

&lt;p&gt;To track a Floating point number, use a JavaScript Number; adding a type suffix is optional:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;price$flt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;4.99&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
    &lt;span class='nx'&gt;sales_tax&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.99&lt;/span&gt; &lt;span class='c1'&gt;// Same as $sales_tax:$flt&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='geocoordinates'&gt;Geo-coordinates&lt;/h3&gt;

&lt;p&gt;Tracking a pair of Geographic coordinates is done like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;check_in$geo&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='mf'&gt;40.11041&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;88.21337&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='c1'&gt;// Lat, long&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that the datatype takes the format &lt;strong&gt;latitude&lt;/strong&gt; followed by &lt;strong&gt;longitude&lt;/strong&gt;. That is the same order used by services such as Google Maps.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; if you do not add the &lt;code&gt;$geo&lt;/code&gt; type suffix, then the value will be incorrectly interpreted by Snowplow as an Array of Floating points.&lt;/p&gt;

&lt;h3 id='date'&gt;Date&lt;/h3&gt;

&lt;p&gt;Snowplow Dates include the date &lt;em&gt;and&lt;/em&gt; the time, with milliseconds precision. There are three type suffixes supported for tracking a Date:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$dt&lt;/code&gt; - the Number of days since the epoch&lt;/li&gt;

&lt;li&gt;&lt;code&gt;$ts&lt;/code&gt; - the Number of seconds since the epoch&lt;/li&gt;

&lt;li&gt;&lt;code&gt;$tms&lt;/code&gt; - the Number of milliseconds since the epoch. This is the default for JavaScript Dates if no type suffix supplied&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can track a date by adding either a JavaScript Number &lt;em&gt;or&lt;/em&gt; JavaScript Date to your &lt;code&gt;properties&lt;/code&gt; object. The following are all valid dates:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;birthday$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;1980&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;11&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as birthday$dt: 3996&lt;/span&gt;
    &lt;span class='nx'&gt;birthday2$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;3996&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// ^ Same as above&lt;/span&gt;
    
    &lt;span class='nx'&gt;registered$ts&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;05&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;13&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;14&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;20&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as registered$ts: 1371129610&lt;/span&gt;
    &lt;span class='nx'&gt;registered2$ts&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1371129610&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// Same as above&lt;/span&gt;
    
    &lt;span class='nx'&gt;last_action$tms&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1368454114215&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='c1'&gt;// Accurate to milliseconds&lt;/span&gt;
    &lt;span class='nx'&gt;last_action2&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='c1'&gt;// Sent to Snowplow as last_action2$tms: 1368454114215&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the type prefix only indicates how the JavaScript Number sent to Snowplow is interpreted - all Snowplow Dates are stored to milliseconds precision (whether or not they include that level of precision).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two warnings:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If you specify a JavaScript Number but do not add a valid Date suffix (&lt;code&gt;$dt&lt;/code&gt;, &lt;code&gt;$ts&lt;/code&gt; or &lt;code&gt;$tms&lt;/code&gt;), then the value will be incorrectly interpreted by Snowplow as a Number, not a Date&lt;/li&gt;

&lt;li&gt;If you specify a JavaScript Number but add the wrong Date suffix, then the Date will be incorrectly interpreted by Snowplow, for example:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;last_ping$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mi'&gt;1371129610&lt;/span&gt; &lt;span class='c1'&gt;// Should have been $ts. Snowplow will interpret this as the year 3756521449&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id='arrays'&gt;Arrays&lt;/h3&gt;

&lt;p&gt;You can track an Array of values of any data type other than Null.&lt;/p&gt;

&lt;p&gt;Arrays must be homogeneous - in other words, all values within the Array must be of the same datatype. This means that the following is &lt;strong&gt;not&lt;/strong&gt; allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;28&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;38&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='c1'&gt;// NOT allowed&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By contrast, the following are all allowed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
    &lt;span class='nx'&gt;session_starts$ts&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='mi'&gt;1371129610&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1064329730&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;1341127611&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
    &lt;span class='nx'&gt;check_ins$geo&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[[&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;88.21337&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mf'&gt;40.11041&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='o'&gt;-&lt;/span&gt;&lt;span class='mf'&gt;78.81557&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mf'&gt;30.22047&lt;/span&gt;&lt;span class='p'&gt;]]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above guide, please do get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And if you have any ideas or feedback for Snowplow&amp;#8217; custom unstructured events, do please share them, either in the comments below or through the usual channels.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events"/>
    <title>Snowplow 0.8.3 released with unstructured events</title>
    <updated>2013-05-14T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to announce the release of Snowplow &lt;strong&gt;0.8.3&lt;/strong&gt;. This release updates our JavaScript Tracker to version 0.11.2, adding the ability to send custom unstructured events to a Snowplow collector with &lt;code&gt;trackUnstructEvent()&lt;/code&gt;. The Clojure Collector is also bumped to 0.5.0, to include some important bug fixes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that this release only adds unstructured events to the JavaScript Tracker - adding unstructured events to our Enrichment process and storage targets is on the roadmap - but rest assured we are working on it!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many thanks to community members &lt;a href='https://github.com/rgabo'&gt;Gabor Ratky&lt;/a&gt;, &lt;a href='https://github.com/tarsolya'&gt;Andras Tarsoly&lt;/a&gt; and &lt;a href='https://github.com/lackac'&gt;Laszlo Bacsi&lt;/a&gt;, all from &lt;a href='http://secretsaucepartners.com/'&gt;Secret Sauce Partners&lt;/a&gt;, for contributing this great feature: Gabor and his team took JavaScript unstructured events from an item on our roadmap to a code-complete feature, big thanks guys! (And if you are interested in seeing how the design and implementation of this powerful feature evolved, do have a read of the &lt;a href='https://github.com/snowplow/snowplow/pull/198'&gt;original GitHub pull request&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#definition'&gt;What are unstructured events?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#when'&gt;When to use unstructured events?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#usage'&gt;Usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#roadmap'&gt;Roadmap for unstructured events&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/14/snowplow-0.8.3-released-with-unstructured-events/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='definition'&gt;1. What are unstructured events?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customunstruct'&gt;Custom unstructured events&lt;/a&gt; are user events which do not fit one of the existing Snowplow event types (page views, ecommerce transactions etc), and do not fit easily into our existing &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured event&lt;/a&gt; format. A custom unstructured event consists of two elements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;name&lt;/code&gt;, e.g. &amp;#8220;Game saved&amp;#8221; or &amp;#8220;returned-order&amp;#8221;&lt;/li&gt;

&lt;li&gt;A set of &lt;code&gt;name: value&lt;/code&gt; properties (also known as a hash, associative array or dictionary)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You might recognise what we call custom unstructured events from other analytics tools including MixPanel, KISSmetrics and Keen.io, where they are the primary trackable event type.&lt;/p&gt;
&lt;h2&gt;&lt;a name='when'&gt;2. When to use unstructured events?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Custom unstructured events are great for a couple of use cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Where you want to track event types which are proprietary/specific to your business (i.e. not already part of Snowplow)&lt;/li&gt;

&lt;li&gt;Where you want to track events which have unpredictable or frequently changing properties&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; because unstructured events are &lt;em&gt;not&lt;/em&gt; currently processed by the ETL and enrichment step, or added to storage, we recommend using &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured events&lt;/a&gt; for custom events types, assuming that you can fit your events into our custom structured event schema.&lt;/p&gt;
&lt;h2&gt;&lt;a name='usage'&gt;3. Usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Tracking an unstructured event with the JavaScript Tracker is very straightforward - use the &lt;code&gt;trackUnstructEvent(name, properties)&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;Here is an example taken from our codebase:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackUnstructEvent&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Viewed Product&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                &lt;span class='p'&gt;{&lt;/span&gt;
                    &lt;span class='nx'&gt;product_id&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ASO01043&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;category&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;Dresses&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;brand&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;ACME&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;returning&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='kc'&gt;true&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;price&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='mf'&gt;49.95&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                    &lt;span class='nx'&gt;sizes&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;xs&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;xxl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;],&lt;/span&gt;
                    &lt;span class='nx'&gt;available_since$dt&lt;/span&gt;&lt;span class='o'&gt;:&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='nb'&gt;Date&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;2013&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;3&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;&lt;span class='mi'&gt;7&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
                &lt;span class='p'&gt;}&lt;/span&gt;
            &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We have written a &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;follow-up blog post&lt;/a&gt; to provide more information on using the new &lt;code&gt;trackUnstructEvent&lt;/code&gt; functionality - please &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;read this post&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;4. Upgrading&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are two components to upgrade in this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The JavaScript Tracker, to version 0.11.2&lt;/li&gt;

&lt;li&gt;The Clojure Collector, to version 0.5.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are running the Clojure Collector, you must upgrade the Clojure Collector &lt;strong&gt;before&lt;/strong&gt; upgrading the JavaScript Tracker, or you will experience some data loss.&lt;/p&gt;

&lt;h3 id='clojure_collector'&gt;Clojure Collector&lt;/h3&gt;

&lt;p&gt;This release bumps the Clojure Collector to version &lt;strong&gt;0.5.0&lt;/strong&gt;. To upgrade to this release:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the new warfile by right-clicking on &lt;a href='http://s3-eu-west-1.amazonaws.com/snowplow-hosted-assets/2-collectors/clojure-collector/clojure-collector-0.5.0-standalone.war'&gt;this link&lt;/a&gt; and selecting &amp;#8220;Save As&amp;#8230;&amp;#8221;&lt;/li&gt;

&lt;li&gt;Log in to your Amazon Elastic Beanstalk console&lt;/li&gt;

&lt;li&gt;Browse to your Collector&amp;#8217;s application&lt;/li&gt;

&lt;li&gt;Click the &amp;#8220;Upload New Version&amp;#8221; and upload your warfile&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='javascript_tracker'&gt;JavaScript Tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) or tag manager to use the latest version of the JavaScript Tracker, which is version 0.11.2. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.2/sp.js&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a name='roadmap'&gt;5. Roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are well aware that this release is only the start of adding custom unstructured events to Snowplow.&lt;/p&gt;

&lt;p&gt;It makes sense to work next on extracting unstructured events in our Enrichment process; unfortunately this is not trivial, because our Enrichment process currently only outputs to Redshift, and Redshift has no support for JSON objects or maps of properties, which we would need to store the unstructured event properties.&lt;/p&gt;

&lt;p&gt;Therefore we are exploring two different strands:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Storing Snowplow events in Avro&lt;/strong&gt;. Avro is a rich data serialization system that will allow us to store the unstructured event properties within the event object. Initially, you would be able to query these Avro-serialized events using a range of tools on Hadoop including Pig, Hive, Scalding and Cascalog. It should also be relatively straightforward to load these events into NoSQL databases such as MongoDB. We would then work on mapping the Avro events into Redshift&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Storing Snowplow events in PostgreSQL&lt;/strong&gt;. Postgres has a JSON datatype, although the querying capabilities on that JSON datatype are so-far very primitive. Nonetheless, it should be possible to at least store the unstructured event properties in an appropriate JSON field in Postgres&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have a preference for one of the two above options, or a suggested third approach, then &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;get in touch&lt;/a&gt; and let us know as soon as possible, as we are thining through these alternatives now.&lt;/p&gt;

&lt;p&gt;Please keep an eye on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap wiki page&lt;/a&gt; to see how Snowplow&amp;#8217;s support for unstructured events evolves.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And if you want to find out more about the syntax for &lt;code&gt;trackUnstructEvent&lt;/code&gt;, do checkout our &lt;a href='/blog/2013/05/14/snowplow-unstructured-events-guide'&gt;Snowplow Unstructured Events Guide&lt;/a&gt;, which was also published today.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/10/where-does-your-traffic-really-come-from</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/10/where-does-your-traffic-really-come-from"/>
    <title>Where does your traffic *really* come from?</title>
    <updated>2013-05-10T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;img src='/static/img/blog/2013/05/lone-traveller.jpg' /&gt;
&lt;p&gt;Web analysts spend a lot of time exploring where visitors to their websites come from:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which sites and marketing campaigns are driving visitors to your website?&lt;/li&gt;

&lt;li&gt;How valuable are those visitors?&lt;/li&gt;

&lt;li&gt;What should you be doing to drive up the number of high quality users? (In terms of spending more marketing, engaging with other websites / blogs / social networks etc.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, identifying where your visitors come from is &lt;strong&gt;not&lt;/strong&gt; as straightforward as it often seems. In this post, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from/#how'&gt;How, technically, can we determine where visitors have come from?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#errors'&gt;Potential sources of errors&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#ga'&gt;Problems with relying on the Google Analytics approach, and why the Snowplow approach is superior&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from#adwords'&gt;Surprises when examining visitors acquired from AdWords search campaigns: most visitors clicked on an ad that was not shown on a Google domain&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/05/10/where-does-your-traffic-really-come-from/#conclusion'&gt;Pulling all the findings together: the value of high-fidelity data in determining where your visitors come from&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='how'&gt;&lt;h2&gt;1. How, technically, can we determine where visitors have come from?&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;There are two sources of raw data that we can use to determine where a vistor to a website has come from: the &lt;a href='#page-referer'&gt;page referer&lt;/a&gt; and the &lt;a href='#page-url'&gt;page URL&lt;/a&gt;.&lt;/p&gt;
&lt;a name='page-referer'&gt;&lt;h3&gt;Page referer&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;When you load a web page in your browser, your browser makes an HTTP request to a web server to deliver that page. That request includes a header field that identifies the address of the web page that linked to the resource being requested: this is called the &lt;a href='http://en.wikipedia.org/wiki/HTTP_referer'&gt;HTTP referer&lt;/a&gt; (sic). It is also possible to access the current page&amp;#8217;s referer information from the browser itself, using &lt;code&gt;document.referrer&lt;/code&gt; in JavaScript.&lt;/p&gt;

&lt;p&gt;Web analytics programs typically read the HTTP referer header or JavaScript&amp;#8217;s &lt;code&gt;document.referrer&lt;/code&gt;, and use that page referer data as one the inputs to infer where a visitor has come from.&lt;/p&gt;
&lt;a name='page-url'&gt;&lt;h3&gt;Page URL&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Page referers are a technical solution to identifying where traffic comes from. In addition, digital marketers may want to label incoming traffic so that they can identify which marketing campaigns that traffic should be attributed to. This is typically done by adding a querystring to the landing page URL.&lt;/p&gt;

&lt;p&gt;To give an example of how this technique works in practice, let&amp;#8217;s imagine that I am marketing the website &lt;code&gt;www.flowersdirect.com&lt;/code&gt;. I run a campaign on AdWords called &amp;#8220;November promotion&amp;#8221;. In my AdWords ad, I include a link (that I hope viewers of the add will click) to my homepage (&lt;code&gt;www.flowersdirect.com&lt;/code&gt;). However, instead of just including the standard link in my ad, i.e.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='nt'&gt;&amp;lt;a&lt;/span&gt; &lt;span class='na'&gt;href=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://www.flowersdirect.com&amp;quot;&lt;/span&gt;&lt;span class='nt'&gt;&amp;gt;&lt;/span&gt;www.flowersdirect.com&lt;span class='nt'&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I add a query parameter onto the end of my link labelling the campaign:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='nt'&gt;&amp;lt;a&lt;/span&gt; &lt;span class='na'&gt;href=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://www.flowersdirect.com?utm_campaign=November_promotion&amp;quot;&lt;/span&gt;&lt;span class='nt'&gt;&amp;gt;&lt;/span&gt;www.flowersdirect.com&lt;span class='nt'&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Adding the query parameter does not change the experience of the user clicking on the ad. Then, on the landing page (in this case, the &lt;code&gt;www.flowersdirect.com&lt;/code&gt; homepage) the web analytics JavaScript tag will pass the querystring to the web analytics program, which can then infer that the traffic should be attributed to the &amp;#8220;November promotion&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Different web analytics programs look for different query parameters when assigning traffic to different marketing campaigns. In the case of &lt;a href='https://support.google.com/analytics/answer/1033863?hl=en-GB'&gt;Google Analytics&lt;/a&gt;, the following parameters are used to set the following fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Field in GA&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_medium&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Medium&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_source&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Source&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_term&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Term&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_content&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Content&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;utm_campaign&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Campaign&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;To keep things simple, Snowplow uses the same query parameters, so that businesses running Snowplow alongside GA only need to set the parameters once for each campaign in order to successfully track them in both GA and Snowplow.&lt;/p&gt;

&lt;p&gt;Web analytics programs use a combination of the page URL and the page referer to infer where traffic to the website has come from.&lt;/p&gt;
&lt;a name='errors'&gt;&lt;h2&gt;2. Potential sources of errors&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;In general, there is much more scope for errors to arise deducing the source of traffic from the querystring on the page URL than there are when using the HTTP referer field. This is because querystring parameters are set manually by humans, rather than programmatically by machines. The following are two of the most common sources of errors:&lt;/p&gt;

&lt;h3 id='a_visitors_share_page_urls_with_campaign_parameters_in_the_querystring_using_copyandpaste'&gt;(a) Visitors share page URLs with campaign parameters in the querystring, using copy-and-paste&lt;/h3&gt;

&lt;p&gt;Many times, you will see a link in e.g. a Twitter post containing a &lt;code&gt;utm_&lt;/code&gt; parameter that suggests it is a CPC campaign or some other non-social channel. This sort of error arises when, for example, a visitor clicks on a link from a CPC campaign, views the web page, then wants to share the web page - and does so by copying and pasting the URL. Website visitors are mostly oblivious to marketing parameters in the page&amp;#8217;s URL, and will leave them in place. Everywhere that the user pastes that link, that link will contain the query parameter; any other users clicking on that link will be misclassified as coming from a CPC campaign.&lt;/p&gt;

&lt;h3 id='b_typos_in_the_campaign_parameters_on_the_querystring'&gt;(b) Typos in the campaign parameters on the querystring&lt;/h3&gt;

&lt;p&gt;The individual who sets up the campaign may make a mistake adding the querystring to the link in the ad. This is very easy to do: setting up campaigns can be tedious (especially if many are set up at the same time) and error-prone. An error as simple as typing &lt;code&gt;utm-campaign&lt;/code&gt; instead of &lt;code&gt;utm_campaign&lt;/code&gt; is enough that most web analytics software will misclassify &lt;em&gt;all&lt;/em&gt; the visitors who clicked on that link.&lt;/p&gt;

&lt;p&gt;Note that using a traditional web analytics solution, it is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Impossible to spot that an error has been made on the query string in most cases&lt;/li&gt;

&lt;li&gt;Even if you can spot the error, it is impossible to reprocess the data (and correct the error) or even &amp;#8216;ignore&amp;#8217; the erroneous data&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='identifying_errors_with_snowplow'&gt;Identifying errors with Snowplow&lt;/h3&gt;

&lt;p&gt;Using Snowplow, however, spotting errors is easy, because you have access to the raw page URL parameters in your Snowplow events table. The following is some example data from &lt;a href='http://www.psychicbazaar.com/'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer running Snowplow. We have executed the following query to identify page views where the referer is not internal (so that we only look at the first page view in each visit, i.e. the one with all the interesting page referer and page URL data to determine the source of traffic):&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;collector_tstamp&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlpath&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_urlquery&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_term&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_campaign&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlpath&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlquery&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_term&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;refr_medium&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;internal&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how for each line of data, you can see the raw page URL data (&lt;code&gt;page_urlquery&lt;/code&gt; in particular) that is used to derive the marketing source, medium and campaign (&lt;code&gt;mkt_source&lt;/code&gt;, &lt;code&gt;mkt_medium&lt;/code&gt; and &lt;code&gt;mkt_campaign&lt;/code&gt; fields):&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/inferring-mkt-source-medium-term-campaign-from-pageurl.png'&gt;&lt;img src='/static/img/blog/2013/05/inferring-mkt-source-medium-term-campaign-from-pageurl.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the above image above to zoom in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This makes errors very easy to spot. As you can see from the screenshot below, Psychic Bazaar ran a campaign from September through November last year where there was a simple typo in the querystring. As a result, Snowplow did not set the marketing fields correctly for visitors who clicked on these links. Nonetheless, it is straightforward to spot the mistake, and adjust the data accordingly:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/erroneous-query-string-data.png'&gt;&lt;img src='/static/img/blog/2013/05/erroneous-query-string-data.png' /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Click on the above image above to zoom in.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In Google Analytics, SiteCatalyst or most other tools, spotting the above error and handling it is correctly is impossible.&lt;/p&gt;
&lt;a name='ga'&gt;&lt;h2&gt;3. Problems with relying on Google Analytics approach, and why the Snowplow approach is superior&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The traditional approach to inferring a visitor&amp;#8217;s origin has further weaknesses - related to the way in which page referer data is combined with page URL data to make these inferences.&lt;/p&gt;

&lt;p&gt;To work out where a visitor has come from, Google Analytics &lt;em&gt;first&lt;/em&gt; examines the page URL for the first page view of the visit, and &lt;em&gt;then&lt;/em&gt;, if no suitable &lt;code&gt;utm_&lt;/code&gt; parameters are found on the querystring, looks at the page referer to see if a URL for a referring web page is available. (For a more detailed view of the process GA uses, see &lt;a href='https://developers.google.com/analytics/devguides/platform/features/campaigns-trafficsources'&gt;this detailed description from Google&lt;/a&gt;.) There are two problems with this approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Google Analytics &lt;em&gt;starts&lt;/em&gt; with the source of data that is inherently more error-prone, before looking at the more reliable (if sometimes less informative) data source&lt;/li&gt;

&lt;li&gt;Google Analytics does not apply any intelligence to using the combination of both sources together to infer where a visitor has come from. If, for example, a visitor has clicked on a link from a webmail client (evident from the HTTP referer URL), but that link contains a &lt;code&gt;utm_source=bing&lt;/code&gt; parameter, we can guess, fairly reliably, that the user has not clicked on a paid Bing search campaign, but on a link that was shared by someone who originally clicked on the Bing search campaign ad. Google Analytics only considers each data point in isolation, and does not make the raw data available to us, as analysts, to make a more intelligent attribution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In Snowplow, by contrast, we do &lt;strong&gt;not&lt;/strong&gt; collapse the two separate data sources (page URL parameters and page referer) into a single &amp;#8216;source&amp;#8217; for the visitor. Instead, we expose both to the analyst: we offer five &lt;code&gt;mkt&lt;/code&gt; fields that are set based on &lt;code&gt;utm&lt;/code&gt; parameters on the page URL (&lt;code&gt;mkt_medium&lt;/code&gt;, &lt;code&gt;mkt_source&lt;/code&gt;, &lt;code&gt;mkt_term&lt;/code&gt;, &lt;code&gt;mkt_content&lt;/code&gt;, &lt;code&gt;mkt_campaign&lt;/code&gt;) and separately we offer three &lt;code&gt;refr&lt;/code&gt; fields (&lt;code&gt;refr_medium&lt;/code&gt;, &lt;code&gt;refr_source&lt;/code&gt;, &lt;code&gt;refr_term&lt;/code&gt;) that are derived from the HTTP referer header. We can view these fields alongside one-another by executing the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;mkt_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;mkt_term&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_medium&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_source&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;refr_term&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;refr_medium&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;internal&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And check the results below. Note how in some cases the &lt;code&gt;mkt&lt;/code&gt; fields are set, but the &lt;code&gt;refr&lt;/code&gt; fields are not and vice-versa. On other occasions, both sets of fields are set:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/mkt-and-refr-fields-alongside-one-another.png'&gt;&lt;img src='/static/img/blog/2013/05/mkt-and-refr-fields-alongside-one-another.png' /&gt;&lt;/a&gt;
&lt;p&gt;This is part of the Snowplow commitment to &lt;a href='/blog/2013/04/10/snowplow-event-validation/'&gt;high fidelity analytics&lt;/a&gt;, a concept we introduced in &lt;a href='/blog/2013/04/10/snowplow-event-validation/'&gt;this blog post&lt;/a&gt;.&lt;/p&gt;
&lt;a name='adwords'&gt;&lt;h2&gt;4. Surprises when examining visitors acquired from AdWords search campaigns: most visitors clicked on ads that were not shown on Google domains&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Another advantage of keeping your referer data separate to your marketing campaign data is that you can learn more about &lt;em&gt;where&lt;/em&gt; your marketing ads are displayed based on the additional referer data that GA ignores.&lt;/p&gt;

&lt;p&gt;To give a concrete example: Psychic Bazaar buys AdWords on the Google Search network. It does not buy ads on the Google Display network. By running the following query, we can identify which domains those AdWords ads that were clicked on were displayed:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='psql'&gt;&lt;span class='cm'&gt;/* PostgreSQL / Redshift */&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;refr_urlhost&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;Number of click-throughs&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class='mf'&gt;.&lt;/span&gt;&lt;span class='s-Name'&gt;&amp;quot;events_new&amp;quot;&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;mkt_source&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;GoogleSearch&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;refr_urlhost&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='s-Name'&gt;&amp;quot;Number of click-throughs&amp;quot;&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the results in Tableau, there are a few surprises:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/google-adwords-referer-domain-analysis.jpg'&gt;&lt;img src='/static/img/blog/2013/05/google-adwords-referer-domain-analysis.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;The top two domains by amount of AdWords traffic directed towards Psychic Bazaar are &lt;strong&gt;not&lt;/strong&gt; Google owned domains. They are eBay and Amazon - both websites that Psychic Bazaar sells on as a third party merchant.&lt;/p&gt;

&lt;p&gt;We expected &lt;em&gt;some&lt;/em&gt; of the domains to be non-Google domains - after all, we were aware that search engines like Ask serve results and advertising powered by Google. We &lt;em&gt;were&lt;/em&gt; surprised, however, that Amazon and eBay would do this: it seems strange that they would show ads for merchants who are competing with themselves and their own merchants. Nonetheless, if you visit either website, perform a search, and scroll down to the bottom of the result set, you will see AdWords ads displayed at the bottom:&lt;/p&gt;
&lt;a href='/static/img/blog/2013/05/amazon-with-adwords-links-screenshot.png'&gt;&lt;img src='/static/img/blog/2013/05/amazon-with-adwords-links-screenshot.png' /&gt;&lt;/a&gt;
&lt;p&gt;This puts Psychic Bazaar in the uncomfortable position of competing not only with other merchants on eBay and Amazon, but also competing with its own website ads.&lt;/p&gt;

&lt;p&gt;We were also surprised to learn that in total, 69% of the click-throughs received were from non-Google domains: in this case at least, powering search advertising on other sites doesn&amp;#8217;t simply add additional advertising inventory to Google&amp;#8217;s core search inventory, it actually makes up the bulk of that inventory. (We&amp;#8217;d be interested in finding out from other Snowplow users who buy on AdWords whether they see similar results.)&lt;/p&gt;
&lt;a name='conclusion'&gt;&lt;h2&gt;Pulling all the findings together: the value of high-fidelity data in determining where your visitors come from&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;In this post, we have seen that the extra level of data provided by Snowplow related to where visitors come from, over-and-above that provided by standard web analytics programs like Google Analytics, is incredibly valuable for a number of reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It makes it possible to identify and manage errors that are invariably introduced in the data&lt;/li&gt;

&lt;li&gt;It leads to more intelligent and robust inferences about where you traffic comes from&lt;/li&gt;

&lt;li&gt;It identifies surprising results related to the placement of your paid campaigns, which may have significant implications for your overall marketing strategy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='want_to_do_more_intelligent_more_robust_attribution'&gt;Want to do more intelligent, more robust attribution?&lt;/h2&gt;

&lt;p&gt;Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; with the Snowplow Professional Services team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/05/08/snowplow-0.8.2-released-with-clojure-collector-enhancements</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/05/08/snowplow-0.8.2-released-with-clojure-collector-enhancements"/>
    <title>Snowplow 0.8.2 released with Clojure Collector enhancements</title>
    <updated>2013-05-08T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to announce the immediate availability of Snowplow &lt;strong&gt;0.8.2&lt;/strong&gt;. This release updates the Clojure Collector only; if you are using the CloudFront Collector, then no upgrade to 0.8.2 is necessary.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/butlermh'&gt;Mark H. Butler&lt;/a&gt; for his major contributions to this release - much appreciated Mark!&lt;/p&gt;

&lt;p&gt;This release bumps the Clojure Collector to version &lt;strong&gt;0.4.0&lt;/strong&gt;. There are three main changes to the Collector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Building the Collector&amp;#8217;s warfile is now much simpler, thanks to a new &lt;code&gt;lein aws&lt;/code&gt; command contributed by Mark&lt;/li&gt;

&lt;li&gt;We have fixed a bug (&lt;a href='https://github.com/snowplow/snowplow/issues/220'&gt;#220&lt;/a&gt;) where occasionally the Collector&amp;#8217;s event logging would log &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; (empty string) rather than &lt;code&gt;&amp;quot;-&amp;quot;&lt;/code&gt; for a missing referer. While rare, when this occurred this would cause the Snowplow ETL process to break (requiring manual editing of the offending log file)&lt;/li&gt;

&lt;li&gt;Some code tidy-up, including making the Clojure Collector work with newer versions of Leiningen&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To upgrade to this release:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the new warfile by right-clicking on &lt;a href='http://s3-eu-west-1.amazonaws.com/snowplow-hosted-assets/2-collectors/clojure-collector/clojure-collector-0.4.0-standalone.war'&gt;this link&lt;/a&gt; and selecting &amp;#8220;Save As&amp;#8230;&amp;#8221;&lt;/li&gt;

&lt;li&gt;Log in to your Amazon Elastic Beanstalk console&lt;/li&gt;

&lt;li&gt;Browse to your Collector&amp;#8217;s application&lt;/li&gt;

&lt;li&gt;Click the &amp;#8220;Upload New Version&amp;#8221; and upload your warfile&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And that should be it. Thanks again to Mark for his contribution to this release - and we&amp;#8217;re excited to have some further community contributions coming very soon to Snowplow, so watch this space!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/23/performing-funnel-analysis-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/23/performing-funnel-analysis-with-snowplow"/>
    <title>Funnel analysis with Snowplow (Platform analytics part 1)</title>
    <updated>2013-04-23T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow/'&gt;Eleven days ago&lt;/a&gt;, we started building out the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;Catalog Analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;, with a set of recipes covering how to measure the performance of &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;content pages&lt;/a&gt; and &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;product pages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today we&amp;#8217;ve published the first set of recipes in the new &lt;a href='/analytics/platform-analytics/overview.html'&gt;platform analytics&lt;/a&gt; section of the Cookbook. By &amp;#8216;platform analytics&amp;#8217;, we mean analytics performed to answer questions about how your platform (or &amp;#8216;website&amp;#8217;, &amp;#8216;application&amp;#8217; or &amp;#8216;product&amp;#8217;) performs. This is one of the most important branches of analytics that can be performed with clickstream, event data.&lt;/p&gt;

&lt;p&gt;The first recipes published cover how to perform &lt;a href='/analytics/platform-analytics/funnel-analysis.html'&gt;funnel analysis&lt;/a&gt; with Snowplow, like the example below, that compares the purchase funnel for an online shop by month.&lt;/p&gt;

&lt;p&gt;&lt;img alt='funnel-analysis' src='/static/img/analytics/platform-analytics/funnel-analysis/visualization-in-tableau.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Funnel analysis is an important example of platform analytics. Snowplow makes it straightforward to define and analyse funnels on the fly: unlike Google Analytics, you do not have to predefine funnels in advance, then collect data, before you can analyse how users progress through them. This is important as it makes it possible to spot e.g. that your visitors are using your platform in a particular (perhaps unexpected) way, and then immediately drill into how many exactly, are doing so, and at what point in that workflow do different users &amp;#8216;drop out&amp;#8217; of the funnel.&lt;/p&gt;

&lt;p&gt;In addition, Snowplow also makes it easy to compare multiple funnels (e.g. like the time series example above, or, instead, comparing how successfully different audience segments progress through different funnels).&lt;/p&gt;

&lt;p&gt;This will be the first of many platform analytics recipes - if there are specific analyses you&amp;#8217;d like us to cover, or would like to contribute yourself, then let us know below or by &lt;a href='/about/index.html'&gt;getting in touch&lt;/a&gt; directly.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/18/measuring-content-page-performance-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/18/measuring-content-page-performance-with-snowplow"/>
    <title>Measuring content page performance with Snowplow (Catalog Analytics part 2)</title>
    <updated>2013-04-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This is the second part in our blog post series on Catalog Analytics. The &lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow/'&gt;first part&lt;/a&gt; was published last week.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Last week, we started building out the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;Catalog Analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;, with a section documenting how to &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;measure the effectiveness of your product pages&lt;/a&gt;. Those recipes were geared specifically towards retailers.&lt;/p&gt;

&lt;p&gt;This week, we&amp;#8217;ve added an extra section to the cookbook, covering &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html'&gt;how to measure engagement levels with content pages&lt;/a&gt;. The recipes covered should be of interest to any company that produces content-rich web pages. (Indeed, all the example analytics were performed using data from this very website.) However, they should be of special interest to publishers and newspaper sites that depend on driving high levels of user engagement with content to make money&lt;/p&gt;

&lt;p&gt;In the new section, we cover a range of recipes, including comparing web pages by what fraction of them is read, on average, by visitors to those pages:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/fraction-of-web-page-read.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/fraction-of-web-page-read.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Plotting the distribution of visitors to a particular web page by the fraction of the web page that they have viewed:&lt;/p&gt;
&lt;!--more--&gt;&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/distribution-of-readers-by-fraction-of-hive-udf-post-read.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/distribution-of-readers-by-fraction-of-hive-udf-post-read.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Comparing how long visitors dwell on average on different pages:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/average-pings-per-page-blog-only.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/average-pings-per-page-blog-only.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Visualizing individual user journeys through a site, including identifying web pages on that visit that were particularly significant:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/customer-journey-1.jpg'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/customer-journey-1.jpg' /&gt;&lt;/a&gt;
&lt;p&gt;Visualizing an individual user journey across a web page:&lt;/p&gt;
&lt;a href='/static/img/analytics/catalog-analytics/content-page-performance/tableau-visualization-2.JPG'&gt;&lt;img src='/static/img/analytics/catalog-analytics/content-page-performance/tableau-visualization-2.JPG' /&gt;&lt;/a&gt;
&lt;p&gt;We have much more content planned for the &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt;. As always, we invite suggestions as to the type of analysis you&amp;#8217;d like us to cover, and contributions from people who&amp;#8217;ve used Snowplow data to perform useful analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing"/>
    <title>Snowplow 0.8.1 released with referer URL parsing</title>
    <updated>2013-04-12T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Just nine days after our Snowplow 0.8.0 release, we are pleased to have our next release ready: Snowplow &lt;strong&gt;0.8.1&lt;/strong&gt;. With the last release we promised that the new Scalding-based ETL/enrichment process would lay a strong technical foundation for our roadmap - and hopefully this release bears that out!&lt;/p&gt;

&lt;p&gt;Until this release, Snowplow has provided users the raw referer URL, from which analysts can deduce who the referer was. In this release, Snowplow processes that referer URL to identify what drove a visitor to your website, specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Were they driven by a search engine, social network, or link in an email program?&lt;/li&gt;

&lt;li&gt;If so, which search engine / social network / email program?&lt;/li&gt;

&lt;li&gt;If they were driven by a search engine, what query did they enter?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This data is key for performing attribution analytics.&lt;/p&gt;

&lt;p&gt;Snowplow delivers the above functionality by parsing the page referer URIs which the JavaScript tracker sends to the collector. The Snowplow enrichment layer does a couple of things with these referer URIs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It splits the referer URL into its six components (scheme, host, port, path, query, fragment). This makes querying referer data significantly easier, as we hope to show in future blog posts and attribution analytics recipes&lt;/li&gt;

&lt;li&gt;It looks up the referer URL in a database of known referers and attempts to extract details about this referer, which you can then use for marketing attribution. (For example - is the referer a search engine, or social network? What query did the user enter in the search engine?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will publish a post on how to use the data in a blog post in the near-future. In the rest of this post, then, we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#referer-parsing'&gt;Referer parsing implementation&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#example-data'&gt;Some example data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#upgrading-usage'&gt;Upgrading and usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/snowplow-0.8.1-released-with-referer-url-parsing#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='referer-parsing'&gt;1. Referer parsing implementation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The extraction of the referer URL into its six components is relatively straightforward. The second stage, looking up the referer URL in a database of known referers, is worth discussing in a little more detail.&lt;/p&gt;

&lt;p&gt;Our referer analysis process uses the latest version of our separate and standalone &lt;a href='https://github.com/snowplow/referer-parser/tree/feature/social'&gt;referer-parser&lt;/a&gt; library. This library comes with a sizeable database of known referers, including search engines, social networks and webmail providers. You can view the library &lt;a href='https://github.com/snowplow/referer-parser/blob/feature/social/referers.yml'&gt;here&lt;/a&gt;. It is a straightforward YAML file containing a long list of referers. Because this is an open source list, anyone can contribute additional referers to it. In this way, we hope it will remain one of the most extensive and authoritative lists of referers to use in web analytics.&lt;/p&gt;

&lt;p&gt;Snowplow feeds the referer URI and page URI to referer-parser to identify which &lt;code&gt;refr_medium&lt;/code&gt; this referer URL belongs to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&amp;#8220;search&amp;#8221; means that this referer is a known search engine&lt;/li&gt;

&lt;li&gt;&amp;#8220;social&amp;#8221; means a social network or similar site&lt;/li&gt;

&lt;li&gt;&amp;#8220;email&amp;#8221; means a webmail provider such as Yahoo! Mail&lt;/li&gt;

&lt;li&gt;&amp;#8220;internal&amp;#8221; means that the referer was another page on the same domain&lt;/li&gt;

&lt;li&gt;&amp;#8220;unknown&amp;#8221; means that there was a referer, but we couldn&amp;#8217;t identify it as belonging to one of the other categories&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the referer can be found in the referer-parser database, Snowplow stores the name of the refering site in the &lt;code&gt;refr_source&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;Finally, if the referer is a &amp;#8220;search&amp;#8221; referer and Snowplow can pull out a search query from the referer URL, it then stores this search term in the &lt;code&gt;refr_term&lt;/code&gt; field.&lt;/p&gt;
&lt;h2&gt;&lt;a name='example-data'&gt;2. Example data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Here is an excerpt of referer data from an ecommerce retailer - right-click and select &amp;#8220;Open in New Tab&amp;#8221; to see this at full size:&lt;/p&gt;

&lt;p&gt;&lt;img alt='parsed-referers-img' src='/static/img/blog/2013/04/parsed-referers.png' /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, in this excerpt we have a variety of different referers - some internal pages and some search pages (Google, Google Images, Bing Images and AOL).&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading-usage'&gt;3. Upgrading and usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As with the 0.8.0 release, this new release assumes that you are running the Hadoop (Scalding) ETL and feeding your data into Redshift.&lt;/p&gt;

&lt;p&gt;To upgrade to 0.8.1 from 0.8.0, follow these steps:&lt;/p&gt;

&lt;h3 id='31_etl'&gt;3.1 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the latest version of the Hadoop ETL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :hadoop_etl_version: 0.2.0 # Version of the Hadoop ETL&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='32_redshift'&gt;3.2 Redshift&lt;/h3&gt;

&lt;p&gt;We have updated the Redshift table definition, you can find the latest version in the GitHub repository &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/table-def.sql'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you already have your Snowplow data in the previous version of the Redshift events table, then we have written &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/migrate_0.0.1_to_0.1.0.sql'&gt;a migration script&lt;/a&gt; to handle the upgrade. &lt;strong&gt;Please review this script carefully before running and check that you are happy with how it handles the upgrade.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Also please note that we have had to remove the &amp;#8220;raw&amp;#8221; &lt;code&gt;referrer_url&lt;/code&gt; field from our Redshift events table for space reasons. This means that your historical data will lose &lt;strong&gt;all&lt;/strong&gt; referer information in your events table unless you run a re-computation, see below.&lt;/p&gt;

&lt;h3 id='33_optional_recomputation'&gt;3.3 (Optional) Re-computation&lt;/h3&gt;

&lt;p&gt;If you would like to see referer details for historic Snowplow events (i.e. events already in your Snowplow events table in Redshift), then we recommend re-running your Snowplow ETL process across all of your historical raw data.&lt;/p&gt;

&lt;p&gt;This is also advisable given that we have removed the raw &lt;code&gt;referrer_url&lt;/code&gt; field from our Redshift table definition for space reasons.&lt;/p&gt;

&lt;p&gt;To re-run your Snowplow ETL process across all your historical data, please see our answer to &lt;a href='https://github.com/snowplow/snowplow/wiki/Troubleshooting#wiki-recompute-events'&gt;I want to recompute my Snowplow events, how?&lt;/a&gt; on the Troubleshooting wiki page.&lt;/p&gt;

&lt;h3 id='34_usage'&gt;3.4 Usage&lt;/h3&gt;

&lt;p&gt;And that&amp;#8217;s it! Once you have made these changes, you should have Snowplow populating the referer details for all new events.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;4. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see the full list of issues delivered in Snowplow 0.8.1 on &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=16&amp;amp;page=1&amp;amp;state=closed'&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/12/online-catalog-analytics-with-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/12/online-catalog-analytics-with-snowplow"/>
    <title>Measuring product page performance with Snowplow (Catalog Analytics part 1)</title>
    <updated>2013-04-12T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We built Snowplow to enable businesses to execute the widest range of analytics on their web event data. One area of analysis we are particularly excited about is catalog analytics for retailers. Today, we&amp;#8217;ve published the &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;first recipes&lt;/a&gt; in the &lt;a href='/analytics/catalog-analytics/overview.html'&gt;catalog analytics&lt;/a&gt; section of the &lt;a href='/analytics/index.html'&gt;Snowplow Analytics Cookbook&lt;/a&gt;. These cover &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;how to measure and compare the performance of different product pages on an ecommerce site&lt;/a&gt;, using plots like the one below:&lt;/p&gt;

&lt;p&gt;&lt;img alt='Example-catalog-analytics' src='/static/img/analytics/catalog-analytics/product-page-performance/scatter-plot.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we will briefly outline:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#what'&gt;What is catalog analytics?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#today'&gt;What recipes have been published today?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/12/online-catalog-analytics-with-snowplow#tomorrow'&gt;What catalog analytics recipes can we expect published in the next few weeks and months?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;&lt;a name='what'&gt;&lt;h3&gt;What is catalog analytics?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;For very many online businesses, a &amp;#8220;catalog&amp;#8221; is a central part of the user-proposition. For a retailer, for example, a catalog is the collection of products they are selling. For a media site, a catalog is the collection of content items (be they articles or videos) offered. For an affiliate site, a catalog is the collection of links or offers available. For a vertical search site, a catalog is the list of indexed entries presented to the user.&lt;/p&gt;

&lt;p&gt;Understanding how well different items in that catalog &amp;#8220;perform&amp;#8221; is key to enabling these businesses to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Source better catalog items - e.g. by buying more effectively if they are a retailer, or designing new products if they are a manufacturer, or producing better content if they are a media company&lt;/li&gt;

&lt;li&gt;Present catalog items more effectively - e.g. by surfacing more popular items, using search and recommendation to enable users to dive more deeply into the catalog, or personalise the items shown based on user or item data&lt;/li&gt;

&lt;li&gt;Optimize the existing catalog items - e.g. by tweaking product prices, adjusting content copy and so on&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='today'&gt;&lt;h3&gt;Which catalog analytics recipes have been published today?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Today, we published a &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;set of recipes to enable businesses to compare the performance of product pages&lt;/a&gt;. The published analysis is particularly relevant to online retailers - it makes it easy for them to identify:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Which products are good candidates for increased marketing spend: namely, highly converting pages with low traffic levels&lt;/li&gt;

&lt;li&gt;Which product pages are underperforming: perhaps because the products on them are not competitively priced, or because the content or images are weak&lt;/li&gt;

&lt;li&gt;Which products are &amp;#8220;star performers&amp;#8221;: attracting large volumes of traffic and converting those users effectively&lt;/li&gt;

&lt;li&gt;Which products are &amp;#8220;dogs&amp;#8221;: products which do not attract traffic, and do not convert the traffic they do get&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can check out the recipes &lt;a href='/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html'&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;a name='tomorrow'&gt;&lt;h3&gt;Which catalog analytics recipes will be published next?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;These recipes are just the start in what we hope will develop into a long series of recipes for catalog analytics. Some of the other recipes that we plan to add include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Analysing how well individual content pieces drive engagement-on-site&lt;/li&gt;

&lt;li&gt;Analysing how much different catalog items contribute to driving traffic to a site&lt;/li&gt;

&lt;li&gt;Analysing how well different catalog items contribute to basket growth through up-sell and increased time-on-site&lt;/li&gt;

&lt;li&gt;Personalising the items displayed to users based on user data and item data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If there are other examples of catalog analyses you would like us to include - &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;drop us a line&lt;/a&gt;! We&amp;#8217;re always interested to explore new and innovative ways of using Snowplow data to drive business value&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/10/snowplow-event-validation</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/10/snowplow-event-validation"/>
    <title>Towards high-fidelity web analytics - introducing Snowplow's innovative new event validation capabilities</title>
    <updated>2013-04-10T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A key goal of the Snowplow project is enabling &lt;strong&gt;high-fidelity analytics&lt;/strong&gt; for businesses running Snowplow.&lt;/p&gt;

&lt;p&gt;What do we mean by high-fidelity analytics? Simply put, high-fidelity analytics means Snowplow faithfully recording &lt;em&gt;all&lt;/em&gt; customer events in a rich, granular, non-lossy and unopinionated way.&lt;/p&gt;

&lt;p&gt;This data is incredibly valuable: it enables companies to better understand their customers and develop and tailor products and services to them. Ensuring that the data is high fidelity is essential to ensuring that any operational and strategic decision making that&amp;#8217;s made on the basis of that data is sound. Guaranteeing data fidelity is not a sexy topic. But it&amp;#8217;s an important one.&lt;/p&gt;

&lt;p&gt;Surprisingly, ensuring your data is high fidelity is &lt;strong&gt;not&lt;/strong&gt; something that is enforced by other analytics products.&lt;/p&gt;

&lt;p&gt;&lt;img alt='high-fidelity' src='/static/img/blog/2013/04/high-fidelity-2000.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;Why is Snowplow so unusual in aiming for high-fidelity analytics? Most often, analytics vendors sacrifice the goal of high-fidelity data at the altar of these three compromises:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Premature aggregation&lt;/strong&gt; - when the data store gets too large, or the reports take too long to generate, it&amp;#8217;s tempting to perform the aggregation and roll-up of the raw event data earlier, sometimes even at the point of collection. Of course this offers a huge potential performance boost to the tool, but at the cost of a huge degree of customer data fidelity&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Ignoring bad news&lt;/strong&gt; - the nature of event data means that often incomplete, corrupted or plain wrong data is sent in to the analytics tool by the event trackers. Handling bad event data is complicated (let&amp;#8217;s go shopping!). Instead of dealing with the complexity, most analytics packages just throw the bad data away silently; this is why tag audit companies like &lt;a href='http://www.observepoint.com/'&gt;ObservePoint&lt;/a&gt; exist&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Being over-opinionated&lt;/strong&gt; - customer analytics is full of challenging questions which need answering before you can analyse the data: do I track users by their first-party cookie, third-party cookie, business ID and/or IP address? Do I use the server clock, or the user&amp;#8217;s clock to log the event time? When does a user session start and end? Because these questions can be difficult to answer, most analytics tools don&amp;#8217;t ask them: instead they take an opinionated view of the &amp;#8220;right answer&amp;#8221; and silently enforce that view through their event collection, storage and analysis. By the time users realize that the logic enforced is one that does not work for their business, they are already tied to that vendor and the imperfect data set they have created with that vendor to date.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To deliver on the goal of high-fidelity analytics, then, we&amp;#8217;re trying to steer Snowplow around these three common pitfalls as best we can.&lt;/p&gt;

&lt;p&gt;We have talked in detail on our website and wiki about avoiding pitfall #1, Premature aggregation. In short: we do &lt;strong&gt;no&lt;/strong&gt; aggregation - Snowplow users have access to granular, event level data, so that they can work out how best they should aggregate it for each type of analysis they wish to perform.&lt;/p&gt;

&lt;p&gt;We will blog more about our ideas to combat #3, Being over-opinionated, in the future.&lt;/p&gt;

&lt;p&gt;For the rest of this blog post, though, we will look at our solution to pitfall #2, Ignoring bad news: namely, &lt;strong&gt;event validation&lt;/strong&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Our new Scalding-based event enrichment process (introduced in &lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/'&gt;our last blog post&lt;/a&gt;) introduces the concept of &lt;strong&gt;event validation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Instead of &amp;#8220;ignoring bad news&amp;#8221;, the Snowplow enrichment engine now validates that every logged event matches the format that we expect for Snowplow events - be they page views, ecommerce transactions, custom structured events or some other type of event. Events which do not match this format are stored in a new &amp;#8220;Bad Rows&amp;#8221; bucket in Amazon S3, along with the specific data validations which the event failed.&lt;/p&gt;

&lt;p&gt;By way of example, here are a couple of &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#wiki-event'&gt;custom structured events&lt;/a&gt; generated by a ecommerce site running Snowplow; both of these events failed the new validation step in our Scalding ETL process. You will note that the bad rows are logged to the S3 bucket in JSON format - we have &amp;#8220;pretty printed&amp;#8221; the rows to make them easier to read:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='json'&gt;&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2012-11-14\t11:53:07\tDUB2\t3707\t92.237.59.86\tGET\td10wr4jwvp55f9.cloudfront.net\t\/ice.png\t200\thttps:\/\/www.psychicbazaar.com\/shop\/checkout\/?token=EC-6H7658847D893744L\tMozilla\/5.0%20(Windows%20NT%206.0)%20AppleWebKit\/537.11%20(KHTML,%20like%20Gecko)%20Chrome\/23.0.1271.64%20Safari\/537.11\tev_ca=ecomm&amp;amp;ev_ac=checkout&amp;amp;ev_la=id_city&amp;amp;ev_pr=SUCCESS&amp;amp;ev_va=Liverpool&amp;amp;tid=404245&amp;amp;uid=4434aa64ebbefad6&amp;amp;vid=1&amp;amp;lang=en-US&amp;amp;refr=https%253A%252F%252Fwww.paypal.com%252Fuk%252Fcgi-bin%252Fwebscr%253Fcmd%253D_flow%2526SESSION%345DiuJgdNO9t8v06miTqv5EHhhGukkGNH3dfRqrKhe0i-UM9FCbVNg26G10sRC%2526dispatch%253D50a222a57771920b6a3d7b606239e4d529b525e0b7e69bf0224adecfb0124e9b61f737ba21b0819882a9058c69cd92dcdac469a145272506&amp;amp;f_pdf=1&amp;amp;f_qt=0&amp;amp;f_realp=0&amp;amp;f_wma=1&amp;amp;f_dir=1&amp;amp;f_fla=1&amp;amp;f_java=1&amp;amp;f_gears=0&amp;amp;f_ag=1&amp;amp;res=1920x1080&amp;amp;cookie=1&amp;amp;url=https%253A%252F%252Fwww.psychicbazaar.com%252Fshop%252Fcheckout%252F%253Ftoken%253DEC-6H7658847D893744L\t-\tHit\tAN6xpNsbS0JS05bqjmnbJdZDkl-cVkTPQsAJDlIOgAIG4hcPTTlMFA==&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;Field [ev_va]: cannot convert [Liverpool] to Float&amp;quot;&lt;/span&gt;
  &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;line&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;2012-11-14\t11:53:13\tDUB2\t3707\t92.237.59.86\tGET\td10wr4jwvp55f9.cloudfront.net\t\/ice.png\t200\thttps:\/\/www.psychicbazaar.com\/shop\/checkout\/?token=EC-6H7658847D893744L\tMozilla\/5.0%20(Windows%20NT%206.0)%20AppleWebKit\/537.11%20(KHTML,%20like%20Gecko)%20Chrome\/23.0.1271.64%20Safari\/537.11\tev_ca=ecomm&amp;amp;ev_ac=checkout&amp;amp;ev_la=id_state&amp;amp;ev_pr=SUCCESS&amp;amp;ev_va=Merseyside&amp;amp;tid=462879&amp;amp;uid=4434aa64ebbefad6&amp;amp;vid=1&amp;amp;lang=en-US&amp;amp;refr=https%253A%252F%252Fwww.paypal.com%252Fuk%252Fcgi-bin%252Fwebscr%253Fcmd%253D_flow%2526SESSION%345DiuJgdNO9t8v06miTqv5EHhhGukkGNH3dfRqrKhe0i-UM9FCbVNg26G10sRC%2526dispatch%253D50a222a57771920b6a3d7b606239e4d529b525e0b7e69bf0224adecfb0124e9b61f737ba21b0819882a9058c69cd92dcdac469a145272506&amp;amp;f_pdf=1&amp;amp;f_qt=0&amp;amp;f_realp=0&amp;amp;f_wma=1&amp;amp;f_dir=1&amp;amp;f_fla=1&amp;amp;f_java=1&amp;amp;f_gears=0&amp;amp;f_ag=1&amp;amp;res=1920x1080&amp;amp;cookie=1&amp;amp;url=https%253A%252F%252Fwww.psychicbazaar.com%252Fshop%252Fcheckout%252F%253Ftoken%253DEC-6H7658847D893744L\t-\tHit\tXbvEfkx7BvngWyY23OLDvyFi8mXe2E_nhBaJwkzCG3aNxUng1jz4hQ==&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nt'&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;&lt;span class='p'&gt;:&lt;/span&gt; &lt;span class='p'&gt;[&lt;/span&gt;
    &lt;span class='s2'&gt;&amp;quot;Field [ev_va]: cannot convert [Merseyside] to Float&amp;quot;&lt;/span&gt;
  &lt;span class='p'&gt;]&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These validation errors occurred because the ecommerce site incorrectly tried to log customer address information in the &lt;code&gt;value&lt;/code&gt; field of a custom structured event; the &lt;code&gt;value&lt;/code&gt; field only supports numeric values (and is stored in Redshift in a float field). When we saw these validation errors, we notified the site and they corrected their Google Tag Manager implementation.&lt;/p&gt;

&lt;p&gt;Currently these bad rows are simply stored for inspection in the Bad Rows bucket in S3, while Snowplow carries on with the raw event processing. This lets the Snowplow user tackle the tagging/data quality issues offline, without disrupting the loading of all their high-fidelity, now-validated event data into Redshift. It leaves open the possibility that the user can fix and reprocess the bad rows.&lt;/p&gt;

&lt;p&gt;In the future we could look into ways of sending alerts when bad rows are generated, or even look into ways of automatically fixing bad rows and submitting them for re-processing.&lt;/p&gt;

&lt;p&gt;This is straight forward stuff - but compare it with the approach taken by other web analytics vendors. If a Google Analytics user sends incorrectly configured data into GA, for example, one of two things happens:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GA silently ignores the data&lt;/li&gt;

&lt;li&gt;GA accommodates the data, so that it corrupts reports produced in GA&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the GA user, spotting the error is impossible in either case. Not only has a data point been lost, but potentially an erroneous data point has been introduced, one that will be very hard to debug given that users can never inspect the underlying data.&lt;/p&gt;

&lt;p&gt;This becomes more of a problem as we move to a Unviersal Analytics world: one in which companies feed GA with &lt;strong&gt;all&lt;/strong&gt; their customer event data from a variety of systems. Ensuring that the system is fed with perfect data will only get harder, whilst dealing with situations where erroneous data has been pushed in will remain impossible.&lt;/p&gt;

&lt;p&gt;That completes our brief look at event validation. We hope it is clear why this is such an important topic. For us at Snowplow, event validation is a key part of our quest for high-fidelity event analytics - so expect to hear more from us on this topic soon!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment"/>
    <title>Snowplow 0.8.0 released with all-new Scalding-based data enrichment</title>
    <updated>2013-04-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A new month, a new release! We&amp;#8217;re excited to announce the immediate availability of Snowplow version &lt;strong&gt;0.8.0&lt;/strong&gt;. This has been our most complex release to date: we have done a full rewrite our ETL (aka enrichment) process, adding a few nice data quality enhancements along the way.&lt;/p&gt;

&lt;p&gt;This release has been heavily informed by our January blog post, &lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#scalding'&gt;The Snowplow development roadmap for the ETL step - from ETL to enrichment&lt;/a&gt;. In technical terms, we have ported our existing ETL process (which was a combination of HiveQL scripts plus a custom Java deserializer) to a new Hadoop-only ETL process which does not require Hive. The new ETL process is written in Scala, using &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;, a Scala API built on top of &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, the Hadoop ETL framework.&lt;/p&gt;

&lt;p&gt;In the rest of this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#benefits'&gt;The benefits of the new ETL&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#limitations'&gt;Limitations of the new ETL&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#infobright-hive-note'&gt;A note for Infobright/Hive users&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#upgrading-usage'&gt;Upgrading and usage&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='benefits'&gt;1. Benefits of the new ETL&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The new ETL process is essentially a direct re-write of the existing Hive-based ETL process, however we have made some functionality improvements along the way. The benefits of the new Scalding-based ETL process as we see them are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fewer moving parts&lt;/strong&gt; - the new ETL process no longer requires Hive running on top of Hadoop. This should make it simpler to setup and more robust&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Data validation&lt;/strong&gt; - the new ETL process runs a set of validation checks on each raw line of Snowplow log data. If a line does not pass validation, then the line along with its validation errors is written to a new bucket for &amp;#8220;bad rows&amp;#8221;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better handling of unexpected errors&lt;/strong&gt; - if you set your ETL process to continue on unexpected errors, any raw lines which trigger unexpected errors will appear in a new &amp;#8220;errors&amp;#8221; bucket&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Fewer Redshift import errors&lt;/strong&gt; - we now truncate six &amp;#8220;high-risk&amp;#8221; fields (&lt;code&gt;useragent&lt;/code&gt;, &lt;code&gt;page_title&lt;/code&gt; et al) and validate that &lt;code&gt;ev_value&lt;/code&gt; is a float, to prevent the most common Redshift load errors&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Stronger technical foundation for our roadmap&lt;/strong&gt; - the foundations are now in-place for us adding more enrichment of our Snowplow events (e.g. &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=16&amp;amp;state=open'&gt;referer parsing&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues?milestone=17&amp;amp;state=open'&gt;geo-location&lt;/a&gt; - both coming soon), and the &amp;#8220;gang of three&amp;#8221; cross-row ETL processes which we are planning (&lt;a href='https://github.com/snowplow/snowplow/issues/20'&gt;one&lt;/a&gt;, &lt;a href='https://github.com/snowplow/snowplow/issues/169'&gt;two&lt;/a&gt;, &lt;a href='https://github.com/snowplow/snowplow/issues/187'&gt;three&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='limitations'&gt;2. Limitations of the new ETL&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We want to be very clear about the limitations of the new ETL process as it stands today:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Redshift only&lt;/strong&gt; - the new ETL process only supports writing out in Redshift format. We discuss this further in &lt;a href='#infobright-hive-note'&gt;A note for Infobright/Hive users&lt;/a&gt; below.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt; - the new ETL process takes almost twice as long as the old Hive process. This is because it is essentially running twice: once to generate the Redshift output, and once to generate the &amp;#8220;bad rows&amp;#8221;: in a Hadoop world these two outputs are handled sequentially as separate MapReduce jobs&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Small files problem&lt;/strong&gt; - being Hadoop-based, our ETL inherits Hadoop&amp;#8217;s &lt;a href='http://amilaparanawithana.blogspot.co.uk/2012/06/small-file-problem-in-hadoop.html'&gt;&amp;#8220;small files problem&amp;#8221;&lt;/a&gt;. Above around 3,000 raw Snowplow log files, the job can slow down considerably, so aim to keep your runs smaller than this&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Prototype&lt;/strong&gt; - please treat the new ETL process as a prototype. We &lt;strong&gt;strongly recommend&lt;/strong&gt; trying it out away from your existing Snowplow installation rather than upgrading your existing process in-place&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On points 2 and 3: rest assured that improving the performance of the new Hadoop ETL (not least by tackling the small files problem) is a key priority for the Snowplow Analytics team going forwards.&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading-usage'&gt;3. A note for Infobright/Hive users&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you are using Infobright or plain-Hive to store your Snowplow data, we understand that you&amp;#8217;ll be feeling a little left out of this release. Unfortunately, supporting Redshift, Infobright and Hive all in this version 1 simply wasn&amp;#8217;t feasible from a development-effort perspective.&lt;/p&gt;

&lt;p&gt;This does not mean that we are giving up on Hive and Infobright: on the contrary, we have big plans for both data storage targets.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;Hive users&lt;/strong&gt; - we are working on a new Avro-based storage format for Snowplow events. Being based on &lt;a href='http://avro.apache.org/'&gt;Avro&lt;/a&gt;, it should be less fragile than our existing flatfile approach, easily queryable from Hive using the &lt;a href='https://cwiki.apache.org/Hive/avroserde-working-with-avro-from-hive.html'&gt;AvroSerde&lt;/a&gt;, and &lt;em&gt;faster&lt;/em&gt; to query (because data is stored more efficiently in binary format). Evolving the Avro schema to incorporate our &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;event dictionary&lt;/a&gt; will also be much more straightforward. This will be the 0.9.x release series and should come later in Q2 or early Q3.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;Infobright users&lt;/strong&gt; - we will be adding Infobright support into the new Hadoop-based ETL later this year. If you would rather not wait, we recommend switching to Redshift now or switching to PostgreSQL support when this is released in late Q2.&lt;/p&gt;

&lt;p&gt;We should also stress: it is totally safe for Infobright/Hive users to upgrade to 0.8.0: the Hive-based ETL process continues to work as before, and we will be continuing to support the Hive ETL with bug fixes etc for the foreseeable future.&lt;/p&gt;

&lt;p&gt;As always, you can check out upcoming features on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Product roadmap&lt;/a&gt; wiki page.&lt;/p&gt;
&lt;h2&gt;&lt;a name='limitations'&gt;4. Upgrading and usage&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Upgrading is simply a matter of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrading your EmrEtlRunner installation to the latest code on GitHub&lt;/li&gt;

&lt;li&gt;Updating your &lt;code&gt;config.yml&lt;/code&gt; configuration file&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nothing else is changed in this release.&lt;/p&gt;

&lt;p&gt;You can see the template for the new &lt;code&gt;config.yml&lt;/code&gt; file format &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;here&lt;/a&gt;. The new format introduces a few new configuration options:&lt;/p&gt;

&lt;h3 id='updated_and_new_buckets'&gt;Updated and new buckets&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:buckets:&lt;/code&gt; we have changed the path to our hosted assets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:assets: s3://snowplow-hosted-assets&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have also added two new buckets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:out_bad_rows: ADD HERE # Leave blank for Hive ETL.
:out_errors: ADD HERE # Leave blank for Hive ETL.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;out_bad_rows&lt;/code&gt; bucket will contain any raw Snowplow log lines which did not pass the ETL&amp;#8217;s validation. If you set &lt;code&gt;continue_on_unexpected_error&lt;/code&gt; to true, then the &lt;code&gt;out_errors&lt;/code&gt; bucket will contain any raw Snowplow log lines which caused an unexpected error.&lt;/p&gt;

&lt;h3 id='new_etl_configuration'&gt;New ETL configuration&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:etl:&lt;/code&gt; we have added:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:job_name: Snowplow ETL # Give your job a name
:implementation: hadoop # Or &amp;#39;hive&amp;#39; for legacy ETL&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;job_name&lt;/code&gt; should make it easier to identify your ETL job in the Elastic MapReduce console.&lt;/p&gt;

&lt;p&gt;Change &lt;code&gt;implementation&lt;/code&gt; to &amp;#8220;hive&amp;#8221; to use our alternative Hive-based ETL process.&lt;/p&gt;

&lt;h3 id='new_version'&gt;New version&lt;/h3&gt;

&lt;p&gt;Under &lt;code&gt;:snowplow:&lt;/code&gt; we have added a new version to track our new ETL process:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hadoop_etl_version: 0.1.0&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! Once you have made those configuration changes, you should be up-and-running with the new ETL process.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;5. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/25/snowplow-tracker-for-arduino-released-sensor-and-event-analytics-for-the-internet-of-things</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/25/snowplow-tracker-for-arduino-released-sensor-and-event-analytics-for-the-internet-of-things"/>
    <title>Snowplow Arduino Tracker released - sensor and event analytics for the internet of things</title>
    <updated>2013-03-25T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing our first non-Web tracker for Snowplow - an event tracker for the &lt;a href='http://www.arduino.cc/'&gt;Arduino&lt;/a&gt; open-source electronics prototyping platform. The &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;Snowplow Arduino Tracker&lt;/a&gt; lets you track sensor and event-stream information from one or more IP-connected Arduino boards.&lt;/p&gt;

&lt;p&gt;We chose this as our first non-Web tracker because we&amp;#8217;re hugely excited about the potential of sophisticated analytics for the &lt;a href='http://www.forbes.com/sites/ericsavitz/2013/01/14/ces-2013-the-break-out-year-for-the-internet-of-things/'&gt;Internet of Things&lt;/a&gt;, following in the footsteps of great projects like &lt;a href='https://cosm.com/'&gt;Cosm&lt;/a&gt; and &lt;a href='http://exosite.com/'&gt;Exosite&lt;/a&gt;. And of course, Snowplow&amp;#8217;s extremely-scalable architecture is a great fit for the huge volumes of events and sensor readings which machines are able to generate - you could say that we are already &amp;#8220;machine-scale&amp;#8221;!&lt;/p&gt;

&lt;p&gt;&lt;img alt='arduino-photo' src='/static/img/blog/2013/03/arduino-board-photo.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;As far as we know, this is the first time an event analytics platform has released a dedicated tracker for the maker community; we can&amp;#8217;t wait to see what the Arduino and Snowplow communities will use it for! Some ideas we had were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deploying a set of Snowplow-connected Arduinos to monitor the environment (temperature, humidity, light levels etc) in your home&lt;/li&gt;

&lt;li&gt;Tracking the movement of products around your shop/warehouse/factory using Arduino, &lt;a href='http://arduino.cc/blog/category/wireless/rfid/'&gt;RFID readers&lt;/a&gt; and Snowplow&lt;/li&gt;

&lt;li&gt;Sending vehicle fleet information (locations, speeds, fuel levels etc) back to Snowplow using Arduino&amp;#8217;s &lt;a href='http://www.cooking-hacks.com/index.php/documentation/tutorials/arduino-3g-gprs-gsm-gps'&gt;3G and GPS&lt;/a&gt; shields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In fact Alex has gone ahead and written a sample Arduino sketch to track temperatures and log the readings to Snowplow - you can find his project on GitHub at &lt;a href='https://github.com/alexanderdean/arduino-temp-tracker'&gt;alexanderdean/arduino-temp-tracker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Want to find out more? To get started using our event tracker for Arduino, check out:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;GitHub repository&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow/wiki/Arduino-Tracker'&gt;Technical Documentation&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/snowplow/snowplow/wiki/Arduino-Tracker-Setup'&gt;Setup Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Happy making!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/20/rob-slifka-elasticity</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/20/rob-slifka-elasticity"/>
    <title>Inside the Plow - Rob Slifka's Elasticity</title>
    <updated>2013-03-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;The Snowplow platform is built standing on the shoulders of a whole host of different open source frameworks, libraries and tools. Without the amazing ongoing work by these individuals, companies and not-for-profits, the Snowplow project literally could not exist.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;As part of our &amp;#8220;Inside the Plow&amp;#8221; series, we will also be showcasing some of these core components of the Snowplow stack, and talking to their creators. To kick us off, we are delighted to have &lt;a href='https://twitter.com/robslifka'&gt;Rob Slifka&lt;/a&gt;, VP of &lt;a href='http://www.sharethrough.com/engineering'&gt;Engineering&lt;/a&gt; at &lt;a href='http://www.sharethrough.com'&gt;Sharethrough&lt;/a&gt; in San Francisco, talking to us about his &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; project. For those who aren&amp;#8217;t aware: Elasticity is a Ruby library which we use as part of our &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-EmrEtlRunner'&gt;EmrEtlRunner&lt;/a&gt;, to make it easy to automate the Snowplow ETL Job on Amazon Elastic MapReduce. The Elasticity library is a great piece of tech - and indeed was a major factor in us deciding to write EmrEtlRunner in Ruby.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With the introductions done, let&amp;#8217;s hand over to Rob to tell us a bit about himself, Elasticity and what he&amp;#8217;s working on next:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt='rob-slifka-img' src='/static/img/blog/2013/03/rob-slifka.jpeg' /&gt;&lt;/p&gt;

&lt;p&gt;Thanks Alex! Quick bit about me: I&amp;#8217;ve been in software development since the mid 90s working on everything from Java Swing (design automation tools) to embedded Jetty (email encryption) and now a mixture of Ruby and Scala. Since 2010, Ive been responsible for &lt;a href='http://www.sharethrough.com/engineering'&gt;engineering&lt;/a&gt; at &lt;a href='http://www.sharethrough.com'&gt;Sharethrough&lt;/a&gt; - an ad tech company based out of San Francisco. Were building a native advertising platform based on the belief that advertising is no longer sustainable as banners and punch-the-monkey ads and has begun the transition to engaging, non-interruptive choice-based experiences. One thing that a lot of people outside of ad tech dont realize is that online advertising is synonymous with scale and some of the most interesting technology problems are driven from those demands. This is where &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; comes in.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Our ads report a significant amount of information around user behavior which we then use in decisioning, pricing and insight derivation (e.g. Do people share videos before watching them?). In the early days, we were handling what we now consider a small volume of logs (1GB/day) with a correspondingly quick and dirty ETL: a log parser that updated the MySQL instance backing our reporting dashboards. Fast forward to 2013 and our log intake is north of 30GB/day. With this volume of data and with the insights we wanted to derive, that process didnt cut it and we determined that the quickest way for us to begin deriving value from our data was via &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; (hereon referred to as EMR).&lt;/p&gt;

&lt;p&gt;If youre unfamiliar with AWS service interaction and evolution, it often follows this pattern (using EMR as an example):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use the AWS UI to become familiar with EMR. Manually step through the creation of Job Flows, choose your job type, asset location, cluster configuration options, etc.&lt;/li&gt;

&lt;li&gt;(Optionally) Graduate to the AWS CLI once you see a pattern in your interaction. Youre in active development and moving more quickly than before. Youve decided on Pig, your assets and clusters are stored and predicted consistently, etc.&lt;/li&gt;

&lt;li&gt;Use an Amazon or 3rd-party API to integrate the tool into your workflow so you dont have to copy the command line tools around your infrastructure.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Amazons tools are developer services, not meant for absolutely streamlined consumption; some legwork is required. The AWS CLI is a thin wrapper around the EMR REST API meaning there are numerous and frequently mutually exclusive options. If you chose to use the CLI, youll spend a significant amount of time learning how to use the command line tools by reading the developer API guide. Why isnt there a programmatic way to work with EMR that follows the same mental model as that which is exposed via the UI and doesnt require you to understand the EMR REST API?&lt;/p&gt;

&lt;p&gt;Thats where Elasticity comes in.&lt;/p&gt;

&lt;p&gt;As an API author you can choose to represent the EMR model directly or layer your own model on top of it. As a point of reference, this is a partial list of EMR REST API calls: AddInstanceGroups, AddJobFlowSteps, DescribeJobFlows, etc.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;One way to provide access to EMR might be via Ruby methods that wrap each of these calls, something like &lt;a href='https://github.com/rslifka/elasticity/blob/master/lib/elasticity/emr.rb'&gt;this&lt;/a&gt;. And by providing only this, you as a developer would be required to understand the EMR API documentation to use Elasticity - still not much better than using the CLI tools&lt;/li&gt;

&lt;li&gt;Another option might be for Elasticity to say, &amp;#8220;Forget about job flows! I&amp;#8217;m going to give you a &amp;#8216;Session&amp;#8217; and each step of your job flow is a &amp;#8216;Batch Processing Function&amp;#8217;&amp;#8220; and youd be properly confused, having to map between your understanding of EMR and what Elasticity exposes&lt;/li&gt;

&lt;li&gt;Elasticity went with a third option - mirroring what was offered in the AWS EMR UI: &lt;strong&gt;Elasticity is a Ruby gem for working with EMR that requires you only understand the EMR user&amp;#8217;s manual, not the EMR developers manual.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Elasticity v1 split (2) and (3) above, encapsulating an entire job as your unit of interaction with the API. You&amp;#8217;d create and configure a &amp;#8220;HiveJob&amp;#8221; and start it. This was assuming that most interactions with EMR are single-step.&lt;/p&gt;

&lt;p&gt;Elasticity v2 was a major rewrite focusing wholly on option (3) above. You create and configure &amp;#8220;JobFlows&amp;#8221; and add steps to them, just as you do in the UI; a much more comfortable model for those familiar with the EMR UI (which we all were at some point when we learned how to use EMR).&lt;/p&gt;

&lt;p&gt;Elasticity v3&amp;#8230; who knows? First and foremost, I work on features that Sharethrough requires. We&amp;#8217;re in a steady state with EMR at the moment and now I&amp;#8217;m hoping the community has some suggestions :)&lt;/p&gt;

&lt;p&gt;Thanks for making it this far! &lt;strong&gt;And if anything I touched on sounds interesting, Sharethrough is hiring and we&amp;#8217;re relo-friendly! Check us out at &lt;a href='http://www.sharethrough.com/engineering'&gt;Sharethrough Engineering&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support"/>
    <title>Snowplow 0.7.6 released with Redshift data warehouse support</title>
    <updated>2013-03-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the immediate release of Snowplow version &lt;strong&gt;0.7.6&lt;/strong&gt; with support for storing your Snowplow events in &lt;a href='http://aws.amazon.com/redshift/'&gt;Amazon Redshift&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We were very excited when Amazon announced Redshift back in late 2012, and we have been working to integrate Snowplow data since Redshift became generally available two weeks ago. Our tests with Redshift since launch have not disappointed - and we can&amp;#8217;t wait to see what the Snowplow community do with the new platform!&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#why-redshift'&gt;Why Redshift is a great fit for Snowplow data&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#this-version'&gt;Changes in this version&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#snowplow-redshift'&gt;Setting up Snowplow for Redshift&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#upgrading'&gt;Upgrading for Infobright/Hive users&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#roadmap'&gt;Roadmap and next steps&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/03/03/snowplow-0.7.6-released-with-redshift-data-warehouse-support#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='why-redshift'&gt;1. Why Amazon Redshift is a great fit for Snowplow data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Snowplow datasets get big very quickly: we store at least one line of data for every single event that occurs on your website or application. Our largest users are recording 100M+ events every day; these data volumes get very big, very quickly.&lt;/p&gt;

&lt;p&gt;Whereas traditional web analytics packages deal with this by aggregating data to feed pre-cut reports, we built Snowplow specifically to maintain that granularity, because that granularity is critical to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Performing bespoke analyses e.g. analysing conversion rates by product, or segmenting users by behavior&lt;/li&gt;

&lt;li&gt;Joining Snowplow data with third party datasets e.g. from your CMS, CRM, adserver or marketing data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, we built Snowplow on technologies like Hadoop and Hive from the get-go to enable Snowplow users to record and analyse massive volumes of event data.&lt;/p&gt;

&lt;p&gt;The trouble with Hadoop and Hive is that they are not great tools for &lt;a href='http://en.wikipedia.org/wiki/Online_analytical_processing'&gt;OLAP analysis&lt;/a&gt;. As a result, we added support for &lt;a href='http://www.infobright.com/'&gt;Infobright Community Edition&lt;/a&gt;: an open source columnar database you deploy yourself which scales to terabytes.&lt;/p&gt;

&lt;p&gt;With &lt;a href='http://aws.amazon.com/redshift/'&gt;Amazon Redshift&lt;/a&gt;, we now support a columnar database that scales to petabytes. Not only that, but:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Redshift is a fully managed service. Unlike Infobright, you do not need to setup and run your own servers: you simply connect to AWS and ask Amazon to fire up a Redshift cluster for you&lt;/li&gt;

&lt;li&gt;Redshift clusters can easily be scaled up and down over time with your data requirements: it is simple to add and remove nodes; you can even snapshot and hibernate them&lt;/li&gt;

&lt;li&gt;A wide range of analytics tools can be plugged directly into Redshift via well supported PostgreSQL JDBC and ODBC drivers. It already works with &lt;a href='http://chartio.com/'&gt;Chartio&lt;/a&gt;. A dedicated connectors for Tableau is currently in development&lt;/li&gt;

&lt;li&gt;Redshift supports a broader set of SQL functionality than Infobright. In particular, loading data into Redshift is much more straightforward, and debugging errors when they occur much easier&lt;/li&gt;

&lt;li&gt;Data can be loaded directly from S3 into Redshift, making the Snowplow ETL pipeline simpler and more efficient. And support for loading Redshift directly from Elastic MapReduce is in Amazon&amp;#8217;s roadmap&lt;/li&gt;

&lt;li&gt;Redshift is part of the AWS cloud, around which Snowplow has been built&lt;/li&gt;

&lt;li&gt;Redshift is highly cost-effective: costing as little $1,000 per TB per year&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please read on to find out how to get started with Snowplow and Redshift.&lt;/p&gt;
&lt;h2&gt;&lt;a name='this-version'&gt;2. Changes in this version&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This version makes a set of changes to Snowplow to add support for Redshift; it is important to understand these changes even if you are using our Hive or Infobright storage options and are not interested in using Redshift.&lt;/p&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The HiveQL scripts have been renamed to align with the three storage formats they generate: &lt;code&gt;mysql-infobright&lt;/code&gt;, &lt;code&gt;hive&lt;/code&gt; and &lt;code&gt;redshift&lt;/code&gt;.&lt;/li&gt;

&lt;li&gt;EmrEtlRunner and StorageLoader have both been upgraded, to versions 0.0.9 and 0.0.5 respectively&lt;/li&gt;

&lt;li&gt;The configuration file formats for EmrEtlRunner and StorageLoader have been updated to add support for Redshift and reflect the new naming conventions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally we have fixed two bugs in this version:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Our Bash files for automating EmrEtlRunner and/or StorageLoader had a bug where the &lt;code&gt;BUNDLE_GEMFILE&lt;/code&gt; configuration lines did not end in &lt;code&gt;/Gemfile&lt;/code&gt;. This has now been fixed. Many thanks to &lt;a href='https://github.com/EZWrighter'&gt;Eric Zimmerman&lt;/a&gt; for reporting this!&lt;/li&gt;

&lt;li&gt;We have widened the field storing the raw useragent string in Infobright (and Redshift) to 1000 characters: 500 characters wasn&amp;#8217;t enough&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you are a Snowplow Hive/Infobright user with no interest in Redshift, please jump to &lt;a href='FIXME#hive-ice-upgrade'&gt;Upgrading for Infobright/Hive users&lt;/a&gt; for information on how to upgrade.&lt;/p&gt;
&lt;h2&gt;&lt;a name='snowplow-redshift'&gt;3. Setting up Snowplow for Redshift&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a relatively simple process, which is fully documented on our wiki:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-redshift'&gt;Setup a Redshift cluster&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner'&gt;Configure EmrEtlRunner to output Snowplow events in Redshift format&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/1-Installing-the-StorageLoader'&gt;Configure StorageLoader to load Snowplow events into Redshift&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;(Optional) &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data#wiki-redshift'&gt;Connect Chartio to Snowplow data in Redshift&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once you have completed these steps, you should now have a Snowplow eventstream data warehouse setup in Redshift!&lt;/p&gt;
&lt;h2&gt;&lt;a name='upgrading'&gt;4. Upgrading for Infobright/Hive users&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;These are the steps to upgrade Snowplow to version 0.7.6 if you are using the Hive or Infobright output formats:&lt;/p&gt;

&lt;h3 id='41_emretlrunner'&gt;4.1 EmrEtlRunner&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.5
  :hive_hiveql_version: 0.5.7
  :mysql_infobright_hiveql_version: 0.0.8
  :redshift_hiveql_version: 0.0.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you are outputting Snowplow events in Infobright format, you need to update this line too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  ...
  :storage_format: mysql-infobright # Used to be &amp;#39;non-hive&amp;#39;&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='42_infobright_table_definition'&gt;4.2 Infobright table definition&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition, because we have widened the &lt;code&gt;useragent&lt;/code&gt; field.&lt;/p&gt;

&lt;p&gt;To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_008.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_008&lt;/code&gt; (version 0.0.8 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;h3 id='43_storageloader'&gt;4.3 StorageLoader&lt;/h3&gt;

&lt;p&gt;If you are using StorageLoader, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to the new format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type:     infobright
  :host:     # Not used by Infobright
  :database: ADD IN HERE
  :port:     # Not used by Infobright
  :table:    events_008 # NOT &amp;quot;events_007&amp;quot; any more
  :username: ADD IN HERE
  :password: ADD IN HERE&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the &lt;code&gt;table&lt;/code&gt; field now points to the new &lt;code&gt;events_008&lt;/code&gt; table created in section 4.2 above.&lt;/p&gt;

&lt;p&gt;Done!&lt;/p&gt;
&lt;h2&gt;&lt;a name='roadmap'&gt;5. Roadmap and next steps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;re really excited about the opportunities for building web-scale, low-cost data warehouses for marketing and product analytics with Amazon Redshift, and we&amp;#8217;re super-excited about all of the potential uses of Snowplow event data within these data warehouses. If you&amp;#8217;re excited too, do &lt;a href='mailto:sales@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;Separately, this is the last planned release in the 0.7.x series. We&amp;#8217;re already hard at work on the next release, which will see us swap out the current Hive-based ETL process for a more robust, performant and extensible Hadoop (Cascading/Scalding) ETL process.&lt;/p&gt;

&lt;p&gt;To keep track of this new release, please sign up for our &lt;a href='https://groups.google.com/forum/?fromgroups#!forum/snowplow-user'&gt;mailing list&lt;/a&gt; and checkout our &lt;a href='https://github.com/snowplow/snowplow/wiki/Product-roadmap'&gt;Roadmap&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/25/snowplow-0.7.5-released-with-important-javascript-fix</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/25/snowplow-0.7.5-released-with-important-javascript-fix"/>
    <title>Snowplow 0.7.5 released with important JavaScript fix</title>
    <updated>2013-02-25T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are releasing Snowplow version &lt;strong&gt;0.7.5&lt;/strong&gt; - which upgrades the JavaScript tracker to version &lt;strong&gt;0.11.1&lt;/strong&gt;. This is a small but important release - because we are fixing an issue introduced in Snowplow version a month ago: &lt;strong&gt;if you are on versions 0.9.1 to 0.11.0 of the JavaScript tracker, please upgrade!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Essentially, version 0.9.1 of the JavaScript tracker (released in Snowplow 0.7.2) fixed &lt;a href='https://github.com/snowplow/snowplow/pull/147'&gt;an old bug&lt;/a&gt; which we inherited from the Piwik JavaScript tracker when we forked it early last year; this bug was stopping the secure flag from being set on Snowplow&amp;#8217;s first-party cookies when sent via HTTPS pages.&lt;/p&gt;

&lt;p&gt;Unfortunately, fixing this bug caused a larger problem: sending cookies securely from HTTPS pages meant that HTTP pages could no longer read the cookies, causing the cookies to be regenerated. As a result, Snowplow&amp;#8217;s &lt;code&gt;domain_userid&lt;/code&gt; field (the first-party user ID) was being &lt;strong&gt;reset&lt;/strong&gt; when a user browsed from an HTTPS page (e.g. checkout) back to an HTTP page.&lt;/p&gt;

&lt;p&gt;So, this release fixes &lt;a href='https://github.com/snowplow/snowplow/issues/181'&gt;a nasty bug&lt;/a&gt;. Thanks to the Piwik team for their help in brainstorming this problem!&lt;/p&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.11.1. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics"/>
    <title>Snowplow 0.7.4 released for better eventstream analytics</title>
    <updated>2013-02-22T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release! We&amp;#8217;re excited to announce Snowplow version &lt;strong&gt;0.7.4&lt;/strong&gt;. The primary purpose of this release is to clean up and rationalise our event data model, in particular around &lt;strong&gt;user IDs&lt;/strong&gt; and &lt;strong&gt;event timestamps&lt;/strong&gt;. This release should lay the foundations for more sophisticated eventstream analytics (such as funnel analysis), by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Enabling companies to assign custom user IDs (e.g. when a customer logs on)&lt;/li&gt;

&lt;li&gt;Distinguish between IDs set at a domain level (via first-party cookies) and at a network level (via third-party cookies)&lt;/li&gt;

&lt;li&gt;Enable precise ordering of events in a user&amp;#8217;s click stream with accuracy correct to the milli-second&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many thanks to Snowplow users &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; and &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; (APN) for suggesting many of these changes and helping us to design them.&lt;/p&gt;

&lt;p&gt;In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#user-ids'&gt;Our new user IDs&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#event-tstamps'&gt;Our new event timestamps&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#bug-fixes'&gt;Bug fixes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#deprecations'&gt;Breaking changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#upgrading'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/22/snowplow-0.7.4-released-for-better-eventstream-analytics#help'&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Read on below the fold to find out more!&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='user-ids'&gt;1. Our new user IDs&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Historically, Snowplow has supported a single &lt;code&gt;user_id&lt;/code&gt; field. Unfortunately, there were three issues with this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Snowplow was &lt;strong&gt;overloading&lt;/strong&gt; the field with two different meanings - if a user was running the CloudFront collector, the &lt;code&gt;user_id&lt;/code&gt; field contained a user ID from a first-party cookie (set by the JavaScript tracker). If a user was running the Clojure collector, the &lt;code&gt;user_id&lt;/code&gt; field contained a cross-domain user ID as set by in a third-party cookie (and the JavaScript-set first-party cookie was ignored).&lt;/li&gt;

&lt;li&gt;Both meanings of &lt;code&gt;user_id&lt;/code&gt; were &lt;strong&gt;web-specific&lt;/strong&gt; - neither made sense for user tracking in a mobile app or any other platform which does not support cookies&lt;/li&gt;

&lt;li&gt;No support for a &lt;strong&gt;custom&lt;/strong&gt; user ID - Snowplow did not allow you to track a custom &lt;code&gt;user_id&lt;/code&gt; specific to your business, such as your users&amp;#8217; account numbers in your ecommerce package&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this release, we aim to solve these issues by separating out user IDs into three separate fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;user_id&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A custom user ID which you can set. Will be supported by all trackers (except the no-JS tracker)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;domain_userid&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A user ID set by the JavaScript tracker in a first-party cookie; tied to the current domain&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;network_userid&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;A user ID set by the Clojure collector in a third-party cookie; shared across a network of different domains&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;To make use of the new custom user ID, you can use the following new method in the JavaScript tracker:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setUserId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;alex-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Business-defined user ID&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that you must call &lt;code&gt;setUserId()&lt;/code&gt; on every page where you know the user ID - in other words the setting does not survive a pageload.&lt;/p&gt;

&lt;p&gt;Whether or not each type of user ID is available for your analysis depends on the combination of your tracker and collector:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Tracker&lt;/th&gt;&lt;th&gt;Collector&lt;/th&gt;&lt;th&gt;-&amp;gt;&lt;/th&gt;&lt;th&gt;&lt;code&gt;user_id&lt;/code&gt;*&lt;/th&gt;&lt;th&gt;&lt;code&gt;domain_userid&lt;/code&gt;&lt;/th&gt;&lt;th&gt;&lt;code&gt;network_userid&lt;/code&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;CloudFront&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Clojure&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;No-JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;CloudFront&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;N/A&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;No-JS tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Clojure&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;N/A&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Non-web tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;Any&lt;/td&gt;&lt;td style='text-align: left;'&gt;-&amp;gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;Yes&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;&lt;td style='text-align: left;'&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;* Assuming you have added a call to &lt;code&gt;setUserId()&lt;/code&gt; - which isn&amp;#8217;t possible in the no-JS tracker.&lt;/p&gt;
&lt;h2&gt;&lt;a name='event-tstamps'&gt;2. Our new event timestamps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Previously our data model included two fields, &lt;code&gt;dt&lt;/code&gt; and &lt;code&gt;tm&lt;/code&gt;, to track the date and time at which each event occurred. This timestamp was based on when the Snowplow event collector &lt;em&gt;received&lt;/em&gt; the event, &lt;strong&gt;not&lt;/strong&gt; when the tracker &lt;em&gt;sent&lt;/em&gt; the event.&lt;/p&gt;

&lt;p&gt;There are a couple of limitations to using a collector-based timestamp for eventstream analysis:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If two events occur almost simultaneously in the client, there is no guarantee which will be received by the collector first (because of the unpredictability of the HTTP connection)&lt;/li&gt;

&lt;li&gt;If a tracker batches events and then sends them in one batch (e.g. a cellphone out of cell coverage) , then all of the events in that batch will end up with the same collector timestamp, despite occurring at different times&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this reason, in this release we are introducing a tracker-based timestamp, which is set by the tracker when the event occurs, and is stored in our data model alongside the collector timestamp. This means that we now have five timestamp fields:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;collector_dt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date when the collector received the event&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;collector_tm&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Time when the collector received the event&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_dt&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Date on the client device when the event occurred&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_tm&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Time on the client device when the event occurred&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;dvce_epoch&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;bigint&lt;/td&gt;&lt;td style='text-align: left;'&gt;Milliseconds since the epoch (1/1/1970) on the client device when the tracker sent the event&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Note that we include a super-precise &lt;code&gt;dvce_epoch&lt;/code&gt; field because our &lt;code&gt;dvce_tm&lt;/code&gt; field is not accurate to milliseconds; when querying within a given user session, simply order by &lt;code&gt;dvce_epoch&lt;/code&gt; to get the user&amp;#8217;s eventstream accurately ordered to the millisecond.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A word of warning:&lt;/strong&gt; tracker timestamps are great for understanding the correct order of, and elapsed time between, events from a specific user session. However, they are not a safe way of understanding when a given event actually occurred, because you &lt;strong&gt;cannot&lt;/strong&gt; trust the clocks on users&amp;#8217; devices. So, stick to the collector timestamp if you need to understand when in the real-world events occurred across multiple users.&lt;/p&gt;
&lt;a name='bug-fixes'&gt;&lt;h2&gt;3. Bug fixes&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as the new fields introduced above, this release also includes an important bug fix in the JavaScript tracker, related to our newly-named &lt;code&gt;domain_userid&lt;/code&gt;. Many thanks to &lt;a href='https://github.com/ngsmrk'&gt;Angus Mark&lt;/a&gt; at &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; for alerting us to this.&lt;/p&gt;

&lt;p&gt;Previously, the site/app ID as set by &lt;code&gt;setSiteId()&lt;/code&gt; was used as an input into naming the first-party cookie which stores the &lt;code&gt;domain_userid&lt;/code&gt;. This had the unfortunate side effect that, if you used multiple site IDs for different parts of your site, your visitors would end up with different &lt;code&gt;domain_userid&lt;/code&gt;s for the different parts of your site.&lt;/p&gt;

&lt;p&gt;This release fixes this problem - and it does so in a way that should not corrupt or reset any of your existing &lt;code&gt;domain_userids&lt;/code&gt;. Going forwards, you can set different parts of your site to different app IDs without &amp;#8220;fragmenting&amp;#8221; your &lt;code&gt;domain_userid&lt;/code&gt;s.&lt;/p&gt;
&lt;a name='deprecations'&gt;&lt;h2&gt;4. Deprecations&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Making the above changes to clean up our event data model have necessarily involved some deprecations, as set out in the table below. &lt;strong&gt;When upgrading to the new version of the JavaScript tracker (0.11.0), please update your JavaScript tags as per the instructions below to avoid problems:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of change&lt;/th&gt;&lt;th&gt;Component&lt;/th&gt;&lt;th&gt;Change&lt;/th&gt;&lt;th&gt;Comment&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;attachUserId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Remove - this doesn&amp;#8217;t do anything any more&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setSiteId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;setAppId()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;getVisitorId()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getDomainUserId()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;getVisitorInfo()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getDomainUserInfo()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Data change&lt;/td&gt;&lt;td style='text-align: left;'&gt;S3 &amp;amp; Infobright storage&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;visit_id&lt;/code&gt; renamed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Now called &lt;code&gt;domain_sessionidx&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;The first change is because we are no longer overloading the &lt;code&gt;user_id&lt;/code&gt; field with multiple different meanings. The next three changes are simply to bring the JavaScript method names inline with the field names we are using in our data model.&lt;/p&gt;

&lt;p&gt;The final change is to rename the &lt;code&gt;visit_id&lt;/code&gt; field to &lt;code&gt;domain_sessionidx&lt;/code&gt;. The field&amp;#8217;s contents is unchanged, but we have updated the name to reflect that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The field holds the current count (aka index) of visits by this user, not a random ID&lt;/li&gt;

&lt;li&gt;Going forwards we will be tracking different types of sessions (mobile, desktop etc), not just website visits&lt;/li&gt;

&lt;li&gt;The field is generated by the JavaScript tracker, using a first party cookie. The name &lt;code&gt;domain_sessionidx&lt;/code&gt; makes the limited scope of this field clearer&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='upgrading'&gt;&lt;h2&gt;5. Upgrading&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Because we are making some significant changes to the event data model, such as &amp;#8220;unpacking&amp;#8221; the overloaded &lt;code&gt;user_id&lt;/code&gt; field, this upgrade is relatively complex. &lt;strong&gt;Please read this upgrade guide in full first before starting your upgrade&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The upgrade process has multiple steps - we will discuss each step in turn, and then suggest a way of scheduling this upgrade to prevent any data corruption.&lt;/p&gt;

&lt;h3 id='41_javascript_tracker'&gt;4.1 JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version &lt;strong&gt;0.11.0&lt;/strong&gt;. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.11.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t forget to update your Snowplow tags as per the updates in &lt;a href='#deprecations'&gt;Deprecations&lt;/a&gt; above.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='42_clojure_collector'&gt;4.2 Clojure collector&lt;/h3&gt;

&lt;p&gt;If you are using the CloudFront collector, you can skip this step.&lt;/p&gt;

&lt;p&gt;If you are using the Clojure collector, you will need to upgrade it to the latest version, &lt;strong&gt;0.3.0&lt;/strong&gt;. You can find the new version packaged as a complete WAR file on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Hosted-assets'&gt;Hosted assets&lt;/a&gt; page. If you have forgotten how to deploy the Clojure-based collector, you will find full instructions on our Wiki, &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Setting up the Clojure collector&lt;/a&gt; (you can skip most of the setup steps).&lt;/p&gt;

&lt;h3 id='43_etl'&gt;4.3 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.5
  :hive_hiveql_version: 0.5.6
  :non_hive_hiveql_version: 0.0.7&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='44_infobright'&gt;4.4 Infobright&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. To make this easier for you, we have created two scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_006_cf_to_007.sh
4-storage/infobright-storage/migrate_006_clj_to_007.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose the appropriate script depending on which collector you are using: &amp;#8220;cf&amp;#8221; means the CloudFront collector, &amp;#8220;clj&amp;#8221; the Clojure collector.&lt;/p&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_007&lt;/code&gt; (version &lt;strong&gt;0.0.7&lt;/strong&gt; of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_006&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_007&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_007 # NOT &amp;quot;events_006&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='45_scheduling_the_upgrade'&gt;4.5 Scheduling the upgrade&lt;/h3&gt;

&lt;p&gt;This upgrade has to be carefully scheduled because we are changing the meaning of the &lt;code&gt;uid&lt;/code&gt; field in the JavaScript tracker, and we are moving data from the old &lt;code&gt;user_id&lt;/code&gt; field into the new &lt;code&gt;network_userid&lt;/code&gt; or &lt;code&gt;domain_userid&lt;/code&gt; fields.&lt;/p&gt;

&lt;p&gt;Our suggested approach is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setup the new JavaScript tracker version 0.11.0 in your tag manager as per section 4.1 above, but &lt;strong&gt;do not&lt;/strong&gt; publish it live yet&lt;/li&gt;

&lt;li&gt;(If you are using the Clojure collector) Get the Clojure collector version 0.3.0 ready in Elastic Beanstalk as per section 4.2 above, but &lt;strong&gt;do not&lt;/strong&gt; deploy it live yet&lt;/li&gt;

&lt;li&gt;Start a manual run of the EmrEtlRunner for your site&amp;#8230;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As soon as&lt;/strong&gt; the manual run has copied all of your available Snowplow logs into your Processing Bucket, publish the new JavaScript tracker live, and deploy your new Clojure collector live (if you are using it)&lt;/li&gt;

&lt;li&gt;Wait for the EmrEtlRunner operation complete&lt;/li&gt;

&lt;li&gt;If you are using Infobright, run the StorageLoader and wait for it to finish&lt;/li&gt;

&lt;li&gt;Now upgrade the ETL as per section 4.3 above&lt;/li&gt;

&lt;li&gt;Now upgrade Infobright (if you are using it) as per section 4.4 above&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This upgrade approach should prevent any user ID data from ending up in the wrong fields in your Snowplow event store.&lt;/p&gt;
&lt;h2&gt;&lt;a name='help'&gt;6. Getting help&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/20/transferring-data-from-s3-to-redshift-at-the-command-line"/>
    <title>Bulk loading data from Amazon S3 into Redshift at the command line</title>
    <updated>2013-02-20T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;On Friday Amazon launched &lt;a href='http://aws.amazon.com/redshift/'&gt;Redshift&lt;/a&gt;, a fully managed, petabyte-scale data warehouse service. We&amp;#8217;ve been busy since building out Snowplow support for Redshift, so that Snowplow users can use Redshift to store their granular, customer-level and event-level data for OLAP analysis.&lt;/p&gt;

&lt;p&gt;In the course of building out Snowplow support for Redshift, we need to bulk load data stored in S3 into Redshift, programmatically. Unfortunately, the Redshift Java SDK is very slow at inserts, so not suitable bulk loading. We found a simple workaround that might be helpful for anyone who wishes to bulk load data into Redshift from S3, and have documented it below.&lt;/p&gt;

&lt;h2 id='an_overview_of_the_workaround'&gt;An overview of the workaround&lt;/h2&gt;

&lt;p&gt;Amazon enables users to bulk load data from S3 into Redshift by executing queries with the following form:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='postgresql'&gt;&lt;span class='k'&gt;copy&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt; 
&lt;span class='k'&gt;from&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;s3://$MY-BUCKET/PATH/TO/FILES/FOR/UPLOAD&amp;#39;&lt;/span&gt; 
&lt;span class='n'&gt;credentials&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;aws_access_key_id=$ACCESS-KEY;aws_secret_access_key=$SECRET-ACCESS-KEY&amp;#39;&lt;/span&gt; 
&lt;span class='k'&gt;delimiter&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;\t&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, these queries can only be executed in a SQL client running a JDBC or ODBC driver compatible with Redshift. (Links to those drivers can be found &lt;a href='http://docs.aws.amazon.com/redshift/latest/gsg/before-you-begin.html#getting-started-download-tools'&gt;here&lt;/a&gt;. )&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;In order to orchestrate bulk loading programmatically, we used &lt;a href='http://www.xigole.com/software/jisql/jisql.jsp'&gt;JiSQL&lt;/a&gt;, a Java based command-line tool for executing SQL queries that uses a JDBC driver. JiSQL enables us to specify the specific, Redshift-compatible JDBC driver to use to establish the connection. This meant we could upgrade our Ruby &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-alternative-data-stores'&gt;StorageLoader&lt;/a&gt; to execute the relevant command-line syntax to initiate the regular data loads of Snowplow data from S3 into Redshift.&lt;/p&gt;

&lt;h2 id='using_jisql_to_bulk_load_data_from_s3_to_redshift_at_the_commandline_a_step_by_step_guide'&gt;Using JiSQL to bulk load data from S3 to Redshift at the command-line: a step by step guide&lt;/h2&gt;

&lt;h3 id='1_download_and_install_jisql'&gt;1. Download and install JiSQL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Download JiSQL from &lt;a href='http://www.xigole.com/software/jisql/jisql.jsp'&gt;http://www.xigole.com/software/jisql/jisql.jsp&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Unzip the contents of the compressed file to a suitable location&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='2_download_the_redshiftcompatible_jdbc_driver'&gt;2. Download the Redshift-compatible JDBC driver&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Download the driver from &lt;a href='http://jdbc.postgresql.org/download/postgresql-8.4-703.jdbc4.jar'&gt;http://jdbc.postgresql.org/download/postgresql-8.4-703.jdbc4.jar&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We saved the driver to the &lt;code&gt;lib&lt;/code&gt; folder in the jisql subdirectory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='3_identify_the_jdbc_url_for_your_redshift_cluster'&gt;3. Identify the JDBC URL for your Redshift cluster&lt;/h3&gt;

&lt;p&gt;In the AWS Console, go to the Redshift and select the cluster you want to load data into. A window will appear with details about the cluster, including the JDBC URL.&lt;/p&gt;

&lt;p&gt;&lt;img alt='screenshot' src='/static/img/blog/2013/02/redshift-jdbc-url.png' /&gt;&lt;/p&gt;

&lt;h3 id='4_initiate_your_bulk_load_of_data_at_the_command_line'&gt;4. Initiate your bulk load of data at the command line&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;At the command line, navigate to the Ji-SQL folder&lt;/li&gt;

&lt;li&gt;Execute the following command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;java -cp lib/jisql-2.0.11.jar:lib/jopt-simple-3.2.jar:lib/postgresql-8.4-703.jdbc4.jar com.xigole.util.sql.Jisql  -driver postgresql -cstring jdbc:postgresql://snowplow.cjbccnwghslt.us-east-1.redshift.amazonaws.com:5439/snplow -user &lt;span class='nv'&gt;$USERNAME&lt;/span&gt; -password &lt;span class='nv'&gt;$PASSWORD&lt;/span&gt; -c &lt;span class='se'&gt;\;&lt;/span&gt; -query &lt;span class='s2'&gt;&amp;quot;copy events from &amp;#39;s3://$MY_BUCKET/PATH/TO/FILES/FOR/UPLOAD credentials &amp;#39;aws_access_key_id=$ACCESS-KEY;aws_secret_access_key=$SECRET-ACCESS-KEY&amp;#39; delimiter &amp;#39;\t&amp;#39;;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notes about the above query:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You will need to replace e.g. &lt;code&gt;$USERNAME&lt;/code&gt; with your Redshift cluster username, &lt;code&gt;$PASSWORD&lt;/code&gt; with your Redshift password etc.&lt;/li&gt;

&lt;li&gt;You will need to replace the jdbc url in the example &lt;code&gt;jdbc:postgresql://snowplow.cjbccnwghslt.us-east-1.redshift.amazonaws.com:5439/snplow&lt;/code&gt; with the value you fetched from the AWS console in step 2&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;-c \;&lt;/code&gt; flag sets &lt;code&gt;;&lt;/code&gt; as the query delimiter. If this is not specified, the query delimiter is taken to be &lt;code&gt;go&lt;/code&gt; by default. As the query specified is terminated by a &lt;code&gt;;&lt;/code&gt; rather than a &lt;code&gt;go&lt;/code&gt;, leaving out the &lt;code&gt;-c \;&lt;/code&gt; flag would cause the program to hang, as it waits for the terminating characters before executing the query&lt;/li&gt;

&lt;li&gt;If you want to experiment with the tool, you can leave off the &lt;code&gt;-query&lt;/code&gt; parameter, in which case you&amp;#8217;ll invoke an interactive command-line session&lt;/li&gt;

&lt;li&gt;If the query is successful, it will return &lt;code&gt;0 rows affected.&lt;/code&gt; at the command line. This is a bit misleading: if you then look in Redshift you&amp;#8217;ll see the new rows have loaded.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A note about bulk loading data from S3 into Redshift:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon will only let you use the above syntax to load data from S3 into Redshift if the S3 bucket and the Redshift cluster are located in the &lt;strong&gt;same&lt;/strong&gt; region. If they are not (and Redshift is not available in all regions, at the time of writing), you will need to copy your S3 data into a new bucket in the same region as your Redshift cluster, prior to running the bulk upload.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Happy bulk loading from the command line!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp"/>
    <title>Reflections on Saturday's Measurecamp</title>
    <updated>2013-02-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;On Satuday both Alex and I were lucky enough to attend London&amp;#8217;s second &lt;a href='http://www.measurecamp.org/'&gt;Measurecamp&lt;/a&gt;, an unconference dedicated to digital analytics. The venue was packed with smart people sharing some really interesting ideas - we can&amp;#8217;t do justice to all those ideas here, so I&amp;#8217;ve just outlined my favourite two from the day:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#keywords'&gt;Using keywords to segment audience by product and interest match&lt;/a&gt;, courtesy of &lt;a href='https://twitter.com/carmenmardiros'&gt;Carmen Mardiros&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#server-side-datalayer'&gt;Transferring commercially sensitive data into your web analytics platform via a server-side dataLayer&lt;/a&gt;, courtesy of &lt;a href='https://twitter.com/TechPad'&gt;Matt Clarke&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#8217;ve also post the slides I&amp;#8217;d put together on &lt;a href='/blog/2013/02/18/ideas-coming-out-of-februarys-measurecamp#clv'&gt;customer lifetime value&lt;/a&gt; for the event: I didn&amp;#8217;t end up sharing these on the day, because the room where the session took place didn&amp;#8217;t have a projector. That was just as well, as I think we had a much more interesting conversation about customer lifetime value as a result.&lt;/p&gt;
&lt;h2&gt;&lt;a name='keywords'&gt;1. Using keywords to segment audience by product and interest match&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In this rather excellent presentation, &lt;a href='https://twitter.com/carmenmardiros'&gt;Carmen&lt;/a&gt; showed how you can use keywords users enter (either in searches directing them to your website, or on internal searches) to classify audience in especially meaningful buckets e.g.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Are they already a customer?&lt;/li&gt;

&lt;li&gt;Are they brand aware?&lt;/li&gt;

&lt;li&gt;Are they looking to purchase vs looking for support?&lt;/li&gt;

&lt;li&gt;Are they interested broadly or narrowly in your area?&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;And then test whether those audience behaved in significantly different ways to validate your segmentation. (For example, comparing bounce rates or conversion rates between segments.) Once validated, the segmentation enables you to apply segment-specific KPI: for example, it is meaningless analysing purchase conversion rates for users who have already bought, and are returning to the site for support-related queries.&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16581811' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/carmenmardiros/getting-to-the-people-behind-the-keywords-16581811' target='_blank' title='Getting to the People Behind The Keywords'&gt;Getting to the People Behind The Keywords&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/carmenmardiros' target='_blank'&gt;Carmen Mardiros&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;An especially exciting prospect going forwards is to use machine learning to extend the segmentation beyond the subset of users for whom we have keyword data: so we can classify users who have not entered keywords, but appear to behave in a similar way to those who have, into the same buckets. This would be especially powerful with Snowplow data, as we have a user&amp;#8217;s complete click stream to work with when identifying users who &amp;#8220;look-like&amp;#8221; those we have classified on the basis of keywords alone. I hope to explore this in the near future, and blog about it here.&lt;/p&gt;
&lt;h2&gt;&lt;a name='server-side-datalayer'&gt;Transferring commercially-sensitive data into your web analytics platform via a server-side dataLayer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In another excellent presentation, &lt;a href='https://twitter.com/TechPad'&gt;Matt Clarke&lt;/a&gt; talked through his experience implementing &lt;a href='http://support.google.com/analytics/bin/answer.py?hl=en&amp;amp;answer=2790010&amp;amp;topic=2790009&amp;amp;ctx=topic'&gt;Universal Analytics&lt;/a&gt; at an online retailer. The key driver for Matt in choosing to implement Universal Analytics was a desire to deliver more commercially meaningful from Google Analytics, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Profitability of individual marketing campaigns (e.g. AdWords)&lt;/li&gt;

&lt;li&gt;Conversion rates by product and by brand&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to report on the above, it is necessary to capture margin data with every sale (in the case of 1) and record views, add to baskets and transactions by product and brand (in the case of 2). In both instances, Matt needed to pass more data into his web analytics platform than Google Analytics has traditionally allowed, which made Google&amp;#8217;s new Universal Analytics an attractive alternative.&lt;/p&gt;

&lt;p&gt;Passing this commercially sensitive data into Univesal Analytics is not trivial, however. Pushing it through the client-side dataLayer would make it available to any competitor. So instead, Matt passed the data server side into Universal Analytics. Universal Analytics does not have a PHP tracker, but Matt was able to effectively build his own using Google&amp;#8217;s &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;. By syncronising the client ID sent by the Javascript tracker with his own server-side tracker, Matt enables Universal Analtyics to stitch together the data generated client and server side. You can see more details in Matt&amp;#8217;s presentation below:&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16578670' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/MattClarke4/measurecamp-improving-e-commerce-tracking-with-universal-analytics' target='_blank' title='Measurecamp - Improving e commerce tracking with universal analytics'&gt;Measurecamp - Improving e commerce tracking with universal analytics&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/MattClarke4' target='_blank'&gt;Matt Clarke&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;Matt&amp;#8217;s presentation is a must-read for anyone who wants to push web analytics tools into a more powerful business analytics tool. For us at Snowplow, it really highlights the need to enable server side tracking alongside client-side tracking: previously it hadn&amp;#8217;t occurred to me that a Snowplow user might want to use both approaches together, and replicate the dataLayer approach that has become best practice client-side on the server-side.&lt;/p&gt;
&lt;h2&gt;&lt;a name='clv'&gt;Customer lifetime value&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;These are the slides I prepared for my session on customer lifetime value:&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/16598692' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt; &lt;/iframe&gt;&lt;div style='margin-bottom:5px'&gt; &lt;strong&gt; &lt;a href='http://www.slideshare.net/yalisassoon/customer-lifetime-value-16598692' target='_blank' title='Customer lifetime value'&gt;Customer lifetime value&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href='http://www.slideshare.net/yalisassoon' target='_blank'&gt;yalisassoon&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
&lt;p&gt;We had a very interesting discussion about the challenges both of tracking user behavior across user lifetime, and started to explore approaches to developing predictive models for customer lifetime value. In 30 minutes we didn&amp;#8217;t get the chance to develop these very far - I hope we find another forum (maybe G+?) to continue the conversation. Many thanks to all those who attended: I learnt a lot from you.&lt;/p&gt;

&lt;h2 id='thank_you_measurecamp'&gt;Thank you Measurecamp&lt;/h2&gt;

&lt;p&gt;Big thanks to the &lt;a href='http://www.measurecamp.org/attendees/'&gt;Measurecamp Team&lt;/a&gt;. It was a brilliant event, and we&amp;#8217;re looking forward to the next one in 6 months time!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/15/snowplow-0.7.3-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/15/snowplow-0.7.3-released"/>
    <title>Snowplow 0.7.3 released, tracking additional data</title>
    <updated>2013-02-15T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the release of Snowplow version &lt;strong&gt;0.7.3&lt;/strong&gt;. This release adds a set of &lt;strong&gt;16 all-new fields&lt;/strong&gt; to our event model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A new Event Vendor field&lt;/li&gt;

&lt;li&gt;The Page URL split out into its component parts (scheme, host, port, path, querystring, fragment/anchor)&lt;/li&gt;

&lt;li&gt;The web page&amp;#8217;s character set&lt;/li&gt;

&lt;li&gt;The web page&amp;#8217;s width and height&lt;/li&gt;

&lt;li&gt;The browser&amp;#8217;s viewport (i.e. visible width and height)&lt;/li&gt;

&lt;li&gt;For page pings, we are now tracking the user&amp;#8217;s scrolling during the last ping period (four fields)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These fields should make a new set of analyses on Snowplow data, including analysing how deeply users engage with different web pages (e.g. what percentage of a web page have they viewed, and how fast). In addition, it should make some analyses easier, e.g. aggregating (and comparing) metrics by page by page and domain.&lt;/p&gt;

&lt;p&gt;In addition, the new release includes some minor bug fixes. In this post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#new-fields'&gt;The new fields&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#bug-fixes'&gt;Bug fixes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#breaking-changes'&gt;Breaking changes&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/02/15/snowplow-0.7.3-released#upgrade'&gt;Upgrading&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='new-fields'&gt;1. New fields&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are hugely excited to be including 16 new fields in this release - we believe that these fields should unlock a whole host of new analyses on Snowplow data.&lt;/p&gt;

&lt;p&gt;For completeness, we list out all of the new fields below. Note that all of the new fields are available in both the S3 (aka Hive) and Infobright (aka non-Hive) storage outputs:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Field&lt;/th&gt;&lt;th&gt;Datatype&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;event_vendor&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Which company or org. defined this event type&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlscheme&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Scheme aka protocol, e.g. &amp;#8220;https&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlhost&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Host aka domain, e.g. &amp;#8220;www.snowplowanalytics.com&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlport&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;int&lt;/td&gt;&lt;td style='text-align: left;'&gt;Port if specified, 80 if not&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlpath&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Path to page, e.g. &amp;#8220;/product/index.html&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlquery&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Querystring, e.g. &amp;#8220;id=GTM-DLRG&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_urlfragment&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;Fragment aka anchor, e.g. &amp;#8220;4-conclusion&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;br_viewwidth&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The width of the browser&amp;#8217;s viewport in pixels&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;br_viewheight&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The height of the browser&amp;#8217;s viewport in pixels&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_charset&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;string&lt;/td&gt;&lt;td style='text-align: left;'&gt;The page&amp;#8217;s character encoding, e.g. &amp;#8220;UTF-8&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_width&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The total width of the page (incl. non-viewed area)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;doc_height&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;The total height of the page (incl. non-viewed area)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_xoffset_min&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Minimum page x offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_xoffset_max&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Maximum page x offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_yoffset_min&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Minimum page y offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;pp_yoffset_max&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;integer&lt;/td&gt;&lt;td style='text-align: left;'&gt;Maximum page y offset seen in the last ping period&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Don&amp;#8217;t worry if some of these new fields don&amp;#8217;t make immediate sense based on the descriptions above - we will take a look at each of these fields in the sub-sections below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h3 id='11_event_vendor'&gt;1.1 Event vendor&lt;/h3&gt;

&lt;p&gt;As we have &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;previously blogged&lt;/a&gt;, we are in the process of developing the Snowplow event model: the list of first-class events for which we&amp;#8217;ve defined a structured data model. As we stressed in the &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;blog post&lt;/a&gt;, we well understand that different models will be appropriate for different websites and applications, and that model we develop will not be ideal for everyone. In the future, we plan to enable different companies to develop their own first class data model within Snowplow. As a first step in this direction, we have added the &lt;strong&gt;Event vendor&lt;/strong&gt; field to the Snowplow data model: when a company develops its own event data model, it will be identifiable to that vendor using this field. This will open up the possibility of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ingesting proprietary events from third-party systems (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;com.sendgrid&amp;#8221; or &amp;#8220;com.appnexus&amp;#8221;)&lt;/li&gt;

&lt;li&gt;Ingesting clickstream events from other analytics services (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;com.adobe&amp;#8221; or &amp;#8220;com.mixpanel&amp;#8221;)&lt;/li&gt;

&lt;li&gt;Tracking custom events defined by a specific Snowplow user (e.g. &lt;code&gt;event_vendor&lt;/code&gt;=&amp;#8221;au.com.asnowplowuser&amp;#8221;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At the moment, however, all events will have an &lt;code&gt;event_vendor&lt;/code&gt; field that will be &amp;#8220;com.snowplowanalytics&amp;#8221; (using the Java package-style naming convention).&lt;/p&gt;

&lt;h3 id='12_page_url_components'&gt;1.2 Page URL components&lt;/h3&gt;

&lt;p&gt;We have split the &lt;code&gt;page_url&lt;/code&gt; into its six component parts (the unprocessed &lt;code&gt;page_url&lt;/code&gt; field is left unchanged). Having these fields broken out should make it much easier to do page URL-based analyses, such as aggregating data for specific &lt;code&gt;page_url&lt;/code&gt;s (ignoring query strings) or investigating HTTPS traffic.&lt;/p&gt;

&lt;h3 id='13_viewport_fields'&gt;1.3 Viewport fields&lt;/h3&gt;

&lt;p&gt;Each event now tracks the current viewport of the browser - in other words, the viewable area (width x height) current available within the browser.&lt;/p&gt;

&lt;p&gt;This will enable analysts to distinguish browsing behavior based on viewport size, and see if there are specific events on a customer journey that trigger a user resizing his / her browser. (Which is a useful user-experience indicator.)&lt;/p&gt;

&lt;h3 id='14_document_width_and_height'&gt;1.4 Document width and height&lt;/h3&gt;

&lt;p&gt;We are now tracking the complete width and height of the current document (aka web page) on each event. This tells you the total width and height of the current page, as perceived by the browser. This measures the whole document - i.e. including the non-viewable part of the document.&lt;/p&gt;

&lt;p&gt;This can be used in conjunction with the new viewport fields (above) and page ping offsets (below) to analyse what fraction of a document a user has engaged with, and over what time period.&lt;/p&gt;

&lt;h3 id='15_page_ping_offsets'&gt;1.5 Page ping offsets&lt;/h3&gt;

&lt;p&gt;These four new offset fields are perhaps the most complex new additions. First of all: these fields are only set if you &lt;code&gt;enableActivityTracking()&lt;/code&gt; on your site. In a nutshell, activity tracking:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Silently checks for user activity (mouse movements, scrolling, key presses etc) on a page for a specified time period (e.g. 10 seconds)&lt;/li&gt;

&lt;li&gt;If any user activity was detected in those 10 seconds, the tracker sends a &amp;#8220;page ping&amp;#8221; back to Snowplow. (No user activity, no page ping)&lt;/li&gt;

&lt;li&gt;This is then repeated for each new time period, until the user navigates away from the page&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this release we are sending four new offset fields along with each page ping event. These offsets track the &lt;strong&gt;minimum&lt;/strong&gt; and &lt;strong&gt;maximum&lt;/strong&gt; horizontal and vertical page offsets scrolled to by the user in the last page ping period. In other words: these four fields tell you how far left/right and up/down the user scrolled during the last ping period.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simply put: these new offset fields are designed to provide a clear view of how your users scroll around your webpages over time.&lt;/strong&gt; (Especially when combined with the viewport and document width and height fields also listed above.)&lt;/p&gt;

&lt;p&gt;Huge thanks to &lt;a href='https://github.com/kingo55'&gt;Rob Kingston&lt;/a&gt; for providing the original idea and implementation around page ping offsets, and helping us to test our implementation!&lt;/p&gt;

&lt;h3 id='16_document_characterset'&gt;1.6 Document characterset&lt;/h3&gt;

&lt;p&gt;Each event now tracks the document&amp;#8217;s charset where available (not all browsers set this).&lt;/p&gt;
&lt;a name='bug-fixes'&gt;&lt;h2&gt;2. Bug fixes&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;As well as the new fields introduced above, this release also includes a small set of bug fixes in the JavaScript tracker which are worth noting:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Our &lt;code&gt;logImpression()&lt;/code&gt; method was not working (it was using the wrong argument names) - this has now been fixed.&lt;/li&gt;

&lt;li&gt;The activity tracking (page ping) behavior was too fragile: if a single monitoring period elapsed with no activity, then all future monitoring would be cancelled. This could easily lead to on-page activity not being recorded. This has now been fixed&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='breaking-changes'&gt;&lt;h2&gt;3. Breaking changes and deprecations&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The following table tracks the breaking changes and deprecations in this version. &lt;strong&gt;When upgrading to the latest version of the JavaScript tracker (0.10.0), please update your JavaScript tags as per the instructions below to avoid problems:&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of change&lt;/th&gt;&lt;th&gt;Component&lt;/th&gt;&lt;th&gt;Change&lt;/th&gt;&lt;th&gt;Comment&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setAccount()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;setCollectorCf()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setTracker()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;getTrackerCf()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Breaking change&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;setHeartBeatTimer()&lt;/code&gt; removed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;enableActivityTracking()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Deprecation&lt;/td&gt;&lt;td style='text-align: left;'&gt;JavaScript tracker&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackEvent()&lt;/code&gt; deprecated&lt;/td&gt;&lt;td style='text-align: left;'&gt;Use &lt;code&gt;trackStructEvent()&lt;/code&gt; instead&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Data change&lt;/td&gt;&lt;td style='text-align: left;'&gt;S3 &amp;amp; Infobright storage&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;event&lt;/code&gt;=&amp;#8221;custom&amp;#8221; changed&lt;/td&gt;&lt;td style='text-align: left;'&gt;Changed to &lt;code&gt;event&lt;/code&gt;=&amp;#8221;struct&amp;#8221;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;The first three changes are simply cleanup: we are removing tracker methods which we previously deprecated some time ago.&lt;/p&gt;

&lt;p&gt;The last two changes are us starting to re-structure our event tracking - we are making space in our event model to support unstructured events, which will be coming soon. Please check out our previous blog post, &lt;a href='/blog/2013/02/04/help-us-build-out-the-snowplow-event-model/'&gt;Help us build out the Snowplow Event Model&lt;/a&gt; for more background on this.&lt;/p&gt;
&lt;a name='upgrade'&gt;&lt;h2&gt;4. Upgrading&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Upgrading is a three-step process:&lt;/p&gt;

&lt;h3 id='41_javascript_tracker'&gt;4.1 JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.10.0. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.10.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t forget to update your Snowplow tags as per the updates in &lt;a href='#breaking-changes'&gt;breaking changes&lt;/a&gt; and deprecations above.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id='42_etl'&gt;4.2 ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.4
  :hive_hiveql_version: 0.5.5
  :non_hive_hiveql_version: 0.0.6&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='43_infobright'&gt;4.3 Infobright&lt;/h3&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. To make this easier for you, we have created two scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_004_to_006.sh
4-storage/infobright-storage/migrate_005_to_006.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose the appropriate script depending on whether your current events table is &lt;code&gt;events_004&lt;/code&gt; or &lt;code&gt;events_005&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_006&lt;/code&gt; (version 0.0.6 of the Infobright table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_006&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_006 # NOT &amp;quot;events_004&amp;quot; or &amp;quot;events_005&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='5_getting_help'&gt;5. Getting help&lt;/h2&gt;

&lt;p&gt;As always, if you do run into any issues or don&amp;#8217;t understand any of the above changes, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/08/writing-hive-udfs-and-serdes</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/08/writing-hive-udfs-and-serdes"/>
    <title>Writing Hive UDFs - a tutorial</title>
    <updated>2013-02-08T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;Snowplow&amp;#8217;s own &lt;a href='https://github.com/alexanderdean'&gt;Alexander Dean&lt;/a&gt; was recently asked to write an article for the &lt;a href='http://sdjournal.org/apache-hadoop-ecosystem/?a_aid=bartoszmiedeksza&amp;amp;a_bid=45f0d439'&gt;Software Developer&amp;#8217;s Journal edition on Hadoop&lt;/a&gt; The kind folks at the Software Developer&amp;#8217;s Journal have allowed us to reprint his article in full below.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alex started writing Hive UDFs as part of the process to write the &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'&gt;Snowplow log deserializer&lt;/a&gt; - the custom SerDe used to parse Snowplow logs generated by the Cloudfront and Clojure collectors so they can be processed in the Snowplow ETL step.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id='article_synopsis'&gt;Article Synopsis&lt;/h2&gt;

&lt;p&gt;In this article you will learn how to write a user-defined function (&amp;#8220;UDF&amp;#8221;) to work with the Apache Hive platform. We will start gently with an introduction to Hive, then move on to developing the UDF and writing tests for it. We will write our UDF in Java, but use Scala&amp;#8217;s SBT as our build tool and write our tests in Scala with Specs2.&lt;/p&gt;

&lt;p&gt;In order to get the most out of this article, you should be comfortable programming in Java. You do not need to have any experience with Apache Hive, HiveQL (the Hive query language) or indeed Hive UDFs - I will introduce all of these concepts from first principles. Experience with Scala is advantageous, but not necessary.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='introduction'&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Before we start: my name is Alex Dean, and I am the co-founder of Snowplow (http://snowplowanalytics.com/), an open-source web analytics platform built on top of Apache Hadoop and Apache Hive. My experience writing Java code to extend the Hive platform comes from Snowplow, where we built a core piece of our launch platform using a Hive deserializer (https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers).&lt;/p&gt;

&lt;p&gt;So, what is Apache Hive, and what would you want a Hive UDF for? Hive is a data warehouse system built on top of Hadoop for ad-hoc queries and processing of large datasets. Now an Apache Software Foundation project, Hive was originally developed at Facebook, where analysts and data scientists wanted a SQL-like abstraction over traditional Hadoop MapReduce. As such, the key distinguishing feature of Hive is the SQL-like query language HiveQL. An example HiveQL query might look like this:&lt;/p&gt;

&lt;p&gt;Listing 1: An example HiveQL query&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt; 
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is actually a standard Snowplow query to calculate the number of unique visitors to a website by day. So what happens when an analyst runs this query in Hive? Simply this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Hive converts the query into the simplest possible set of MapReduce jobs&lt;/li&gt;

&lt;li&gt;The MapReduce job or jobs is run on the Hadoop platform&lt;/li&gt;

&lt;li&gt;The generated result set is then returned to the user&amp;#8217;s console&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Certainly this is a powerful abstraction over MapReduce jobs, which can be tedious and difficult to write by hand. And HiveQL has a lot of power - there is very little in the ANSI SQL standard which is not available in HiveQL. Nonetheless, sometimes the Hive user will need more power, and for these occasions Hive has three main extension points:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;User-defined functions (&amp;#8220;UDFs&amp;#8221;), which provide a way of extending the functionality of Hive with a function (written in Java) that can be evaluated in HiveQL statements&lt;/li&gt;

&lt;li&gt;Custom serializers and/or deserializers (&amp;#8220;serdes&amp;#8221;), which provide a way of either deserializing a custom file format stored on HDFS to a POJO (plain old Java object), or serializing a POJO to a custom file format (or both)&lt;/li&gt;

&lt;li&gt;Custom mappers/reducers, which allow you to add custom map or reduce steps into your Hive query. These map/reduce steps can be written in any programming language - not just Java&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will not consider serdes or custom mappers/reducers further in this article - we hope to write further articles on each of these in the future.&lt;/p&gt;

&lt;p&gt;Now that we understand why you might write a UDF for Hive, let&amp;#8217;s crack on and start writing one!&lt;/p&gt;

&lt;h2 id='setting_up_our_project'&gt;Setting up our project&lt;/h2&gt;

&lt;p&gt;We will be writing a relatively simple UDF - one which generates a converts a string in Hive to upper-case. Note that a version of this function is actually built into Hive as the UPPER function - for a full list of built-in UDFs in Hive, please see: https://cwiki.apache.org/Hive/languagemanual-udf.html&lt;/p&gt;

&lt;p&gt;As mentioned previously, we will write our UDF in Java - but we will wrap our Java core in a Scala project (with Scala tests), because at Snowplow we much prefer writing Scala to Java. We will use SBT, the Scala build tool, to configure our project - this is an alternative to Maven or similar; SBT handles mixed Java and Scala projects perfectly well.&lt;/p&gt;

&lt;p&gt;First, let&amp;#8217;s create a directory for our project, and add a file, project.sbt into the project root, which contains:&lt;/p&gt;

&lt;p&gt;Listing 2: Our project.sbt build file&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='n'&gt;name&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;hive-example-udf&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;version&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.0.1&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;organization&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;com.snowplowanalytics&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;scalaVersion&lt;/span&gt; &lt;span class='o'&gt;:=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;2.9.2&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;scalacOptions&lt;/span&gt; &lt;span class='o'&gt;++=&lt;/span&gt; &lt;span class='nc'&gt;Seq&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;-unchecked&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;-deprecation&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='n'&gt;resolvers&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;CDH4&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;at&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;https://repository.cloudera.com/artifactory/cloudera-repos/&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.apache.hadoop&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;%&lt;/span&gt;  &lt;span class='s'&gt;&amp;quot;hadoop-core&amp;quot;&lt;/span&gt;        &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.20.2&amp;quot;&lt;/span&gt;      &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.apache.hive&amp;quot;&lt;/span&gt;   &lt;span class='o'&gt;%&lt;/span&gt;  &lt;span class='s'&gt;&amp;quot;hive-exec&amp;quot;&lt;/span&gt;          &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;0.8.1&amp;quot;&lt;/span&gt;       &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;provided&amp;quot;&lt;/span&gt;
&lt;span class='n'&gt;libraryDependencies&lt;/span&gt; &lt;span class='o'&gt;+=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;org.specs2&amp;quot;&lt;/span&gt;        &lt;span class='o'&gt;%%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;specs2&amp;quot;&lt;/span&gt;             &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;1.12.1&amp;quot;&lt;/span&gt;      &lt;span class='o'&gt;%&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;test&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a simple project configuration which names and versions our project, and also adds Hadoop, Hive and Specs2 (our testing library) as dependencies. If you do not have SBT already installed, you can find instructions here http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html&lt;/p&gt;

&lt;h2 id='writing_our_udf'&gt;Writing our UDF&lt;/h2&gt;

&lt;p&gt;Done? Onto the code. First let&amp;#8217;s create a folder for it:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;mkdir -p src/main/java/com/snowplowanalytics/hive/udf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;#8217;s add a file into our udf folder called ToUpper.java, containing:&lt;/p&gt;

&lt;p&gt;Listing 3: Our ToUpper.java UDF definition&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;package&lt;/span&gt; &lt;span class='n'&gt;com&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;snowplowanalytics&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;hive&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;udf&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.hive.ql.exec.UDF&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.hive.ql.exec.Description&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.io.Text&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='nd'&gt;@Description&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;
	&lt;span class='n'&gt;name&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;toupper&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;_FUNC_(str) - Converts a string to uppercase&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;extended&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;Example:\n&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt;
	&lt;span class='s'&gt;&amp;quot;  &amp;gt; SELECT toupper(author_name) FROM authors a;\n&amp;quot;&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt;
	&lt;span class='s'&gt;&amp;quot;  STEPHEN KING&amp;quot;&lt;/span&gt;
	&lt;span class='o'&gt;)&lt;/span&gt;
&lt;span class='kd'&gt;public&lt;/span&gt; &lt;span class='kd'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ToUpper&lt;/span&gt; &lt;span class='kd'&gt;extends&lt;/span&gt; &lt;span class='n'&gt;UDF&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;

    &lt;span class='kd'&gt;public&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='nf'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
		&lt;span class='n'&gt;Text&lt;/span&gt; &lt;span class='n'&gt;to_value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;
		&lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='o'&gt;!=&lt;/span&gt; &lt;span class='kc'&gt;null&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
		    &lt;span class='k'&gt;try&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; 
				&lt;span class='n'&gt;to_value&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;set&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt;&lt;span class='o'&gt;().&lt;/span&gt;&lt;span class='na'&gt;toUpperCase&lt;/span&gt;&lt;span class='o'&gt;());&lt;/span&gt;
		    &lt;span class='o'&gt;}&lt;/span&gt; &lt;span class='k'&gt;catch&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;Exception&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt; &lt;span class='c1'&gt;// Should never happen&lt;/span&gt;
				&lt;span class='n'&gt;to_value&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;
		    &lt;span class='o'&gt;}&lt;/span&gt;
		&lt;span class='o'&gt;}&lt;/span&gt;
		&lt;span class='k'&gt;return&lt;/span&gt; &lt;span class='n'&gt;to_value&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This file defines our UDF, ToUpper. The package definition and imports should be self-explanatory; the &lt;code&gt;@Description&lt;/code&gt; annotation is a useful Hive-specific annotation to provide usage information for our UDF in the Hive console.&lt;/p&gt;

&lt;p&gt;All user-defined functions extend the Hive UDF class; a UDF sub-class must then implement one or more methods named &amp;#8220;evaluate&amp;#8221; which will be called by Hive. We implement an evaluate method which takes one Hadoop Text (which stores text using UTF8) and returns the same Hadoop Text, but now in upper-case. The only complexity is some exception handling, which we include for safety&amp;#8217;s sake.&lt;/p&gt;

&lt;p&gt;Now let&amp;#8217;s check that this compiles. In the root folder, run SBT like so:&lt;/p&gt;

&lt;p&gt;Listing 4: Compiling in SBT&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;sbt
&amp;gt; compile
&lt;span class='o'&gt;[&lt;/span&gt;success&lt;span class='o'&gt;]&lt;/span&gt; Total &lt;span class='nb'&gt;time&lt;/span&gt;: 0 s, completed 28-Jan-2013 16:41:53
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='testing_our_udf'&gt;Testing our UDF&lt;/h2&gt;

&lt;p&gt;Okay great, now time to write a test to make sure this is doing what we expect! First we create a folder for it:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='bash'&gt;&lt;span class='nv'&gt;$ &lt;/span&gt;mkdir -p src/test/scala/com/snowplowanalytics/hive/udf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now let&amp;#8217;s add a file into our udf test folder called ToUpperTest.scala, containing:&lt;/p&gt;

&lt;p&gt;Listing 5: Our ToUpperTest.scala Specs2 test&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;package&lt;/span&gt; &lt;span class='n'&gt;com&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;snowplowanalytics&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;hive&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;udf&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.apache.hadoop.io.Text&lt;/span&gt;

&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;org.specs2._&lt;/span&gt;

&lt;span class='kd'&gt;class&lt;/span&gt; &lt;span class='nc'&gt;ToUpperSpec&lt;/span&gt; &lt;span class='kd'&gt;extends&lt;/span&gt; &lt;span class='n'&gt;mutable&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;Specification&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;val&lt;/span&gt; &lt;span class='n'&gt;toUpper&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;ToUpper&lt;/span&gt;

  &lt;span class='s'&gt;&amp;quot;ToUpper#evaluate&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;should&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='s'&gt;&amp;quot;return an empty string if passed a null value&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;in&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;toUpper&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='kc'&gt;null&lt;/span&gt;&lt;span class='o'&gt;).&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt; &lt;span class='n'&gt;mustEqual&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;

    &lt;span class='s'&gt;&amp;quot;return a capitalised string if passed a mixed-case string&amp;quot;&lt;/span&gt; &lt;span class='n'&gt;in&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
      &lt;span class='n'&gt;toUpper&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;evaluate&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Text&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;Stephen King&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;)).&lt;/span&gt;&lt;span class='na'&gt;toString&lt;/span&gt; &lt;span class='n'&gt;mustEqual&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;STEPHEN KING&amp;quot;&lt;/span&gt;
    &lt;span class='o'&gt;}&lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a Specs2 unit test (http://etorreborre.github.com/specs2/), written in Scala, which checks that ToUpper is performing correctly: we test that an empty string is handled correctly, and then we test that a mixed-case string (&amp;#8220;Stephen King&amp;#8221;) is successfully converted to &amp;#8220;STEPHEN KING&amp;#8221;.&lt;/p&gt;

&lt;p&gt;So let&amp;#8217;s run this next from SBT:&lt;/p&gt;

&lt;p&gt;Listing 6: Testing in SBT&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='text'&gt;&amp;gt; test
[info] Compiling 1 Scala source to /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/test-classes...
[info] ToUpperSpec
[info] 
[info] ToUpper#evaluate should
[info] + return an empty string if passed a null value
[info] + return a capitalised string if passed a mixed-case string
[info]  
[info]  
[info] Total for specification ToUpperSpec
[info] Finished in 742 ms
[info] 2 examples, 0 failure, 0 error
[info] 
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 8 s, completed 28-Jan-2013 17:11:45
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='building_and_using_our_udf'&gt;Building and using our UDF&lt;/h2&gt;

&lt;p&gt;Our tests passed! Now we&amp;#8217;re ready to use our function &amp;#8220;in anger&amp;#8221; from Hive. First, still from SBT, let&amp;#8217;s build our jarfile:&lt;/p&gt;

&lt;p&gt;Listing 7: Packaging our jar&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='text'&gt;&amp;gt; package
[info] Packaging /home/alex/Development/Snowplow/hive-example-udf/target/scala-2.9.2/hive-example-udf_2.9.2-0.0.1.jar ...
[info] Done packaging.
[success] Total time: 1 s, completed 28-Jan-2013 17:21:02
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now take the jarfile (hive-example-udf_2.9.2-0.0.1.jar) and upload it to our Hive cluster - on Amazon&amp;#8217;s Elastic MapReduce, for example, you could upload it to S3.&lt;/p&gt;

&lt;p&gt;From your Hive console, you can now add our new UDF like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;add&lt;/span&gt; &lt;span class='n'&gt;jar&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;path&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='k'&gt;to&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='n'&gt;HiveSwarm&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;jar&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;create&lt;/span&gt; &lt;span class='n'&gt;temporary&lt;/span&gt; &lt;span class='n'&gt;function&lt;/span&gt; &lt;span class='n'&gt;to_upper&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;com.snowplowanalytics.hive.udf.ToUpper&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then finally you can use our new UDF in your HiveQL queries, something like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='k'&gt;SELECT&lt;/span&gt; &lt;span class='nf'&gt;toupper&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;author_name&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;authors&lt;/span&gt; &lt;span class='n'&gt;a&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;STEPHEN&lt;/span&gt; &lt;span class='n'&gt;KING&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That completes our article. If you would like to download the example code above as a working project, you can find it on GitHub here: https://github.com/snowplow/hive-example-udf&lt;/p&gt;

&lt;p&gt;I hope to return with further articles about Hive and Hadoop in the future - potentially one on writing a custom serde - an area where we have a lot of experience at Snowplow Analytics.&lt;/p&gt;

&lt;h2 id='looking_for_help_performing_analytics_or_developing_data_pipelines_using_hive_and_other_hadooppowered_tools'&gt;Looking for help performing analytics or developing data pipelines using Hive and other Hadoop-powered tools&lt;/h2&gt;

&lt;p&gt;&lt;a href='/services/pipelines.html'&gt;Learn more&lt;/a&gt; about services offered by the &lt;a href='/services/pipelines.html'&gt;Snowplow Professional Services team&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/02/04/help-us-build-out-the-snowplow-event-model"/>
    <title>Help us build out the Snowplow Event Model</title>
    <updated>2013-02-04T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At its beating heart, Snowplow is a platform for capturing, storing and analysing event-data, with a real focus on web event data.&lt;/p&gt;

&lt;p&gt;Working out how best to structure the Snowplow event data is key to making Snowplow a success. One of the things that has surprised us, since we started working on Snowplow, is the extent to which our view of the best way to structure that data has changed over time.&lt;/p&gt;

&lt;p&gt;In this blog post, we outline our vision for the Snowplow event data model. We do so to elicit feedback and invite collaboration with the wider Snowplow community. Developing a data model that will work well for &lt;strong&gt;everyone&lt;/strong&gt; will only be possible with input from a broad set of people from a wide range of companies performing analytics on a wide range of businesses. We&amp;#8217;ve been fortunate to work with a wide range of businesses who&amp;#8217;ve helped shape our thinking so far. We hope to talk to many more to help us on our journey to develop the Snowplow Event Model.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='from_page_views_to_events__a_quick_look_back_at_the_development_of_the_web'&gt;From page views to events - a quick look back at the development of the web&lt;/h2&gt;

&lt;p&gt;Before we get started thinking about the Snowplow event data model, it is helpful to put it in the context of how other web analytics tools model web data, and that in the context of the development of the web.&lt;/p&gt;

&lt;p&gt;When the web started out, it was a network of largely static documents that were hyperlinked to one-another. Over time the documents started updating more rapidly, so looked less static. In addition, the development of Flash and then Javascript meant that web pages became more interactive: websites started to look more like interactive applications and less like documents.&lt;/p&gt;

&lt;p&gt;The web analytics industry was born in the 1990s, when the web was still a network of hyperlinked documents. The primary &amp;#8220;event&amp;#8221; that web analytics programmes were interested a &lt;em&gt;hit&lt;/em&gt;, which referred to a request being made to a web server. (As such, it was more of an &amp;#8220;event&amp;#8221; for the sysadmin than the user navigating the website.) Over time this evolved into the &lt;em&gt;page view&lt;/em&gt; - as loading a web pages with multiple elements (e.g. different images) would result in multiple hits. Web analytics packages excelled at tracking &lt;em&gt;page views&lt;/em&gt;. As online retail took off, they extended to capturing transactions, and most recently, social events (e.g. &lt;em&gt;liking a product&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;As a user, there are millions of things you can do on the web: from checking your bank balance, to messaging a friend, to researching a holiday, to sharing photos of your children. Web analytics packages, however, still only recognise a very small subset of events. To go beyond tracking &lt;em&gt;page views&lt;/em&gt; and &lt;em&gt;transactions&lt;/em&gt;, web analysts have to use custom event tracking (in Google Analytics), or an unholy combination of eVars and sProps (in SiteCatalyst).&lt;/p&gt;

&lt;p&gt;We want to do better with Snowplow. We want to identfy a broad set of events that are useful to a wide range of web analysts across different companies and products, and recognise these in the Snowplow Event Model as first class citizens. We want to design the data structures for these events so that there are named fields to capture the dimensions and metrics for those events that meet the needs of 80% of Snowplow users, and a set of configurable dimensions and metrics to meet the needs of the remaining 20%. Similarly, we recognise that those &amp;#8220;1st class&amp;#8221; events might only meet the needs of 80% of the events that people need to track online, and so we will still need generic &amp;#8220;custom events&amp;#8221; for users to configure to track the rest.&lt;/p&gt;

&lt;p&gt;By writing this blog post, we hope to entice readers like you to contribute to that Event Model.&lt;/p&gt;

&lt;h2 id='why_bother_with_an_event_model_at_all'&gt;Why bother with an Event Model at all?&lt;/h2&gt;

&lt;p&gt;Some of the people we have talked to about the Snowplow Event Model have not been convinced of the need to develop one. These people, who are typically very familiar with NoSQL datastores like Mongo, Riak and Cassandra, sometimes argue that we can do away with a formal model all together and simply stuff a JSON with whatever dimensions and metrics suit, when we come to store the data associated with a specific event.&lt;/p&gt;

&lt;p&gt;NoSQL data stores are attractive because they enable users to store data without worrying about a schema. However, that does not mean we can forget about schemas all together: we still need a schema when it comes to querying the data, in order to drive our analysis. Performing even simple OLAP analysis on data in NoSQL stores is significantly harder than on structured data in columnar databases, because we have to work out a schema as part of the analysis. Not only that: but we have to check individual event-level data to test if the dimensions and metrics we&amp;#8217;re exploring using our OLAP analysis are correctly stored for every event we want to explore, and potentially map different fields together to include all the events that we would like. (If this is even possible.) This makes analysis much more involved and complex.&lt;/p&gt;

&lt;p&gt;Sometimes, this complexity is worth it: if our data structures are evolving so fast that any schema we develop today will be redundant tomorrow - then better to collect the data that&amp;#8217;s available today and work out how to query it another day, then over complicate our data collection by forcing the data into a schema it doesn&amp;#8217;t really fit.&lt;/p&gt;

&lt;p&gt;That is not the situation that we are in when it comes to web data, however. With a bit of thought, it is not too difficult to identify a set of events that are meaningful for a wide range of people, and a set of dimensions and metrics that are relevant for each event type. By standardising these in a Event Model, we can develop a standard set of analyses that anyone collecting data which adheres to the model can apply. As an open source community committed to driving innovation in web event analytics, this will make it easier to work collaboratively to develop new approaches to mining web data to learn new and valuable insights.&lt;/p&gt;

&lt;h2 id='the_snowplow_event_model_first_class_events_identified_so_far'&gt;The Snowplow Event Model: first class events identified so far&lt;/h2&gt;

&lt;p&gt;So far, we have identified the following events as ones we wish to identify as first class citizens. (Some of these are already incorporated in Snowplow as first class citizens, others need to be added.)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Page views&lt;/li&gt;

&lt;li&gt;Page pings (i.e. a user reading through content on a page)&lt;/li&gt;

&lt;li&gt;Link clicks&lt;/li&gt;

&lt;li&gt;Ad impressions&lt;/li&gt;

&lt;li&gt;Online transactions&lt;/li&gt;

&lt;li&gt;Social events&lt;/li&gt;

&lt;li&gt;Item views (e.g. viewing a product on a retailer site, or viewing an article on a news site)&lt;/li&gt;

&lt;li&gt;Errors (e.g. an application returning an error)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each of these events, we expect there to be some specific dimensions and metrics that are likely to be captured. In addition, we need to make it possible for users with particular needs to record their own custom dimensions and metrics associated with those specific events. We have detailed the event-specific fields for each of the above 1st class events on the &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;Canonical Event Model&lt;/a&gt; page on the &lt;a href='https://github.com/snowplow/snowplow/wiki'&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Are there other events that we should add to the above list? Are their fields we should add to any of the specific events listed above? Let us know if so :-).&lt;/p&gt;

&lt;p&gt;In addition to the events explicitly recognised by the Event Model, there are likely to be many events that need to be tracked that are not included in the above list. For most of these, we hope that Snowplow users will store them as &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customstruct'&gt;custom structured events&lt;/a&gt;. Where this is not possible, we plan to enable capturing of &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model#wiki-customunstruct'&gt;custom unstructured events&lt;/a&gt;, so our friends who like their NoSQL technologies can create whatever JSONs they like to store their event data in.&lt;/p&gt;

&lt;h2 id='technical_implications_of_expanding_out_the_event_model'&gt;Technical implications of expanding out the event model&lt;/h2&gt;

&lt;p&gt;Currently, Snowplow events data is stored in a single &amp;#8216;fat&amp;#8217; table in either S3 or Infobright. As we build out the number of events that are explicitly included in the event model, along with their associated fields, the table will have to get wider to accommodate those new fields. Clearly, there are implications to doing so - especially as a single row of data, which represents a single event, will only have a subset of those fields populated. (Those that are relevant for the specific event.)&lt;/p&gt;

&lt;p&gt;This is one of the reasons we plan to the storage format of data stored in S3 from the current flat-file structure into &lt;a href='http://avro.apache.org/'&gt;Avro&lt;/a&gt;. There are a number of other benefits associated with migrating to Avro - these will be explored in a forthcoming blog post.&lt;/p&gt;

&lt;p&gt;We also plan to make the StorageLoader that loads data into Infobright configurable, so that it only loads fields related to events that the particular business is interested in. If, for example, you do not serve ads to your users, than you will not track ad impressions served. It therefore makes no sense to devote 5 or 6 columns in Infobright to fields which only relate to ad impression tracking like &lt;code&gt;campaign_id&lt;/code&gt;, &lt;code&gt;advertiser_id&lt;/code&gt; etc. Upgrading the StorageLoader so that it understands that is a priority moving forwards.&lt;/p&gt;

&lt;h2 id='help_us_build_out_the_event_model'&gt;Help us build out the Event Model&lt;/h2&gt;

&lt;p&gt;A lot about big data is sexy. Unfortunately, data modelling is not. Nonetheless, getting it right will be a huge benefit to the whole Snowplow community and by extension the wider web analytics community. Help us to get it right - by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Posting ideas and feedback on this blog post&lt;/li&gt;

&lt;li&gt;Raising ideas / issues on &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;Github&lt;/a&gt;. (Like the &lt;a href='https://github.com/snowplow/snowplow/issues/113'&gt;original suggestion&lt;/a&gt; from &lt;a href='https://github.com/kingo55'&gt;Robert Kingston&lt;/a&gt; to track &lt;a href='https://github.com/snowplow/snowplow/issues/113'&gt;item views&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;&lt;a href='/about/index.html'&gt;Get in touch with us directly&lt;/a&gt; to share your thoughts and ideas&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/29/snowplow-0.7.2-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/29/snowplow-0.7.2-released"/>
    <title>Snowplow 0.7.2 released, with the new no-JavaScript tracker</title>
    <updated>2013-01-29T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce the release of Snowplow version &lt;strong&gt;0.7.2&lt;/strong&gt;. As well as a couple of bug fixes, this release includes our second Snowplow tracker - the &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt;, to be used in web environments where a JavaScript-based tracker is not an option.&lt;/p&gt;

&lt;p&gt;One of the bug fixes is particularly important: we are recommending that &lt;strong&gt;all users of the Clojure-based Collector upgrade&lt;/strong&gt; to the new version (0.2.0) due to a serious bug in the way that event timestamps were recorded.&lt;/p&gt;

&lt;p&gt;But first let&amp;#8217;s look at the No-JS Tracker, and then talk about the other fixes:&lt;/p&gt;

&lt;h2 id='introducing_the_nojavascript_tracker'&gt;Introducing the No-JavaScript Tracker&lt;/h2&gt;

&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt; (or &amp;#8216;pixel tracker&amp;#8217;) can be used to log web events in environments that do not support Javascript. Examples of events include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Views an HTML email&lt;/li&gt;

&lt;li&gt;Views of product listing on a 3rd party marketplace&lt;/li&gt;

&lt;li&gt;Views a page on a 3rd party hosting site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our &lt;a href='/no-js-tracker.html'&gt;No-JS Tracking tag wizard&lt;/a&gt; makes it easier to generate pure-HTML tracking tags. Were you to embed these in email marketing messages, for example, you would be able to compare the behavior of users on your website who had opened specific messages with those who had not. The &lt;a href='/no-js-tracker.html'&gt;No-JS Tracker&lt;/a&gt; enables you to track a broader set of user events in Snowplow, so providing greater coverage of your users&amp;#8217; journeys. For more information on the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; see the &lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker/'&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='important_bug_fixes'&gt;Important bug fixes&lt;/h2&gt;

&lt;p&gt;We have fixed an important issue with the logging on the Clojure Collector (&lt;a href='https://github.com/snowplow/snowplow/issues/146'&gt;issue 146&lt;/a&gt;). The previous version was logging all event dates using a 12-hour clock - meaning that it was impossible to tell if an event happened in the morning or evening.&lt;/p&gt;

&lt;p&gt;As a result of this, we &lt;strong&gt;strongly recommend&lt;/strong&gt; that &lt;strong&gt;all&lt;/strong&gt; users of the Clojure-based Collector upgrade to the new version, 0.2.0. As always, you can find this version available from our &lt;a href='https://github.com/snowplow/snowplow/wiki/Hosted-assets'&gt;Hosted assets page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our second bug (&lt;a href='https://github.com/snowplow/snowplow/pull/147'&gt;issue 147&lt;/a&gt;) was spotted and fixed by &lt;a href='https://github.com/ngsmrk'&gt;Angus Mark&lt;/a&gt; from &lt;a href='http://www.simplybusiness.co.uk/'&gt;Simply Business&lt;/a&gt; - many thanks Angus! There was a bug in the JavaScript tracker where the secure flag was not being correctly set on Snowplow cookies transmitted over HTTPS.&lt;/p&gt;

&lt;p&gt;We don&amp;#8217;t believe that this bug was breaking any functionality, but you can upgrade to the new version 0.9.1 of the tracker at this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.9.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;#8217;s it! As always, if you do run into any issues, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/29/introducing-the-no-js-tracker</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/29/introducing-the-no-js-tracker"/>
    <title>Introducing the No-Javascript pixel tracker</title>
    <updated>2013-01-29T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) enables companies running Snowplow to track users in environments that do not support Javascript. In this blog post we will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#why'&gt;The purpose of the No-Javascript tracker (pixel tracker)&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#mechanics'&gt;How it works&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#collector-considerations'&gt;Considerations when using the No-JS tracker (pixel tracker) with the Clojure collector in particular&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/29/introducing-the-no-js-tracker#roadmap'&gt;Next steps on the Snowplow tracker roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='why'&gt;&lt;h2&gt;What is the purpose of the No-Javascript tracker (pixel tracker)?&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;Our aim with Snowplow has been to enables companies to track user events across &lt;strong&gt;all&lt;/strong&gt; platforms and devices. That means enabling tracking offline events, as well as online events, and mobile events, as well as web events.&lt;/p&gt;

&lt;p&gt;There is a whole class of web event that Snowplow users may want to capture, but which are not possible to track using our standard Javascript tracker because they are environments that do not support Javascript. This includes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Views of HTML emails&lt;/li&gt;

&lt;li&gt;Views of ecommerce products on 3rd party marketplaces&lt;/li&gt;

&lt;li&gt;Views of pages on 3rd party hosting sites e.g. Github&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In these cases, you can use the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) to track events directly into your Snowplow stack. Doing so enables you to analyse complete customer journeys: tying together data on the emails a user has opened with their subsequent web browsing behavior, for example.&lt;/p&gt;
&lt;!--more--&gt;&lt;a name='mechanics'&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The standard Javascript tracker&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Uses a set of Javascript functions to determine key data elements about an event e.g. the &lt;code&gt;page_url&lt;/code&gt; that the event occurs on or the &lt;code&gt;page_title&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;Appends those data points as key value parameters on a query string&lt;/li&gt;

&lt;li&gt;Makes a GET request to your collector including the above querystring, so that the data relevant data is passed into Snowplow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key difference with the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; (pixel tracker) is that it is not possible ot use Javascript functions to determine data points like &lt;code&gt;page_url&lt;/code&gt;. Instead, you have to hardcode those values into the request string, and append those values onto an image request for the Snowplow tracking pixel.&lt;/p&gt;

&lt;p&gt;As a result, the range of data captured by the No-Javascript tracker is smaller than the Javascript tracker. (For example, no browser features are identified and passed into Snowplow.) Nevertheless, it is still a useful data set, and can be used to return data on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The number of unique visitors to a web page&lt;/li&gt;

&lt;li&gt;The number of events / page views (e.g. the number of times an email was opened)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='anatomy_of_a_nojs_tracking_tag'&gt;Anatomy of a No-JS tracking tag&lt;/h3&gt;

&lt;p&gt;The standard No-JS tracking tag looks something like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='html'&gt;&lt;span class='c'&gt;&amp;lt;!--Snowplow start plowing--&amp;gt;&lt;/span&gt;
&lt;span class='nt'&gt;&amp;lt;img&lt;/span&gt; &lt;span class='na'&gt;src=&lt;/span&gt;&lt;span class='s'&gt;&amp;quot;http://collector.snplow.com/i?&amp;amp;e=pv&amp;amp;page=Root%20README&amp;amp;url=http%3A%2F%2Fgithub.com%2Fsnowplow%2Fsnowplow&amp;amp;aid=snowplow&amp;amp;p=web&amp;amp;tv=no-js-0.1.0&amp;quot;&lt;/span&gt; &lt;span class='nt'&gt;/&amp;gt;&lt;/span&gt;
&lt;span class='c'&gt;&amp;lt;!--Snowplow stop plowing--&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There are several things to note about the tag:&lt;/p&gt;

&lt;h4 id='1_it_is_a_plain_html_image_tag'&gt;1. It is a plain HTML image tag&lt;/h4&gt;

&lt;p&gt;Given the tag uses no Javascript, we should not be surprised that it is just a simple HTML &lt;code&gt;&amp;lt;img src...&amp;gt;&lt;/code&gt; tag.&lt;/p&gt;

&lt;h4 id='2_only_a_handful_of_parameters_is_passed_into_the_collector'&gt;2. Only a handful of parameters is passed into the collector&lt;/h4&gt;

&lt;p&gt;The data points passed are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Event&lt;/code&gt; = &lt;code&gt;Pageview&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Page title&lt;/code&gt; = &lt;code&gt;Root README&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Page URL&lt;/code&gt; = &lt;code&gt;https://github.com/snowplow/snowplow&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;code&gt;Application ID&lt;/code&gt; = &lt;code&gt;snowplow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id='3_the_parameters_are_hardcoded'&gt;3. The parameters are hard-coded&lt;/h4&gt;

&lt;p&gt;As a result, is it necessary to generate a unique tag for each individual web page / email newsletter that you want to track. To make it easier to generate the tag, we have &lt;a href='/no-js-tracker.html'&gt;created a wizard&lt;/a&gt;&lt;/p&gt;
&lt;a name='collector-considerations'&gt;&lt;h2&gt;Considerations when using the No-JS tracker with the Clojure collector in particular&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; works with &lt;strong&gt;both&lt;/strong&gt; the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-cloudfront-collector'&gt;Cloudfront collector&lt;/a&gt; and the cross-domain &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Clojure collector&lt;/a&gt;. However, there is an important difference between the way it works with each collector, that has implications for:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What user data is captured&lt;/li&gt;

&lt;li&gt;Which services you should use the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; with&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; is used with the Cloudfront collector, the &lt;strong&gt;only&lt;/strong&gt; data captured is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The name / value pairs stored on the query string i.e. the &lt;code&gt;event type&lt;/code&gt;, &lt;code&gt;page_url&lt;/code&gt; and &lt;code&gt;page_title&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The data captured as standard by the Cloudfront collector i.e. the &lt;code&gt;useragent&lt;/code&gt; string and the &lt;code&gt;date&lt;/code&gt; / &lt;code&gt;time&lt;/code&gt; of the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This limits the scope of the analysis that can be performed with the data: if for example we&amp;#8217;re using the No-JS tracker to track views of a README page on a Github repo, we can see how many times the page was viewed but &lt;strong&gt;not&lt;/strong&gt; how many unique users viewed the page, because no &lt;code&gt;user_id&lt;/code&gt; has been set or stored.&lt;/p&gt;

&lt;p&gt;In contrast, when using the Clojure collector with the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt;, a &lt;code&gt;user_id&lt;/code&gt; is set server-side, and saved to a cookie on the user browser. This provides better data for analytics: you can now analyse at the number of unique visitors to a web page.&lt;/p&gt;

&lt;p&gt;However, you need to make sure that you are allowed to drop a cookie on a user, on a web page owned and managed by a partner or 3rd party service provider. It is &lt;strong&gt;your&lt;/strong&gt; responsibility to ensure that you only drop cookies on web pages where the owners of the web page / service provider are happy for you to do so. There are many examples of providers who do not: for example &lt;a href='http://pages.ebay.com/help/policies/listing-javascript.html'&gt;eBay explicitly does not allow you to drop cookies on your listings pages&lt;/a&gt;. Snowplow takes &lt;strong&gt;no&lt;/strong&gt; responsibility for your use of the &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt;. It is your responsibility to ensure that you abide by the terms and conditions of any 3rd party services and hosting companies you employ this tracking technology on, and we urge extreme caution when deploying the &lt;a href='/no-js-tracker.html'&gt;No-Javascript tracker&lt;/a&gt; in conjunction wiht the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Clojure collector&lt;/a&gt; on sites owned and operated by 3rd parties.&lt;/p&gt;
&lt;a name='roadmap'&gt;&lt;h2&gt;Next steps on the Snowplow tracker roadmap&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The &lt;a href='/no-js-tracker.html'&gt;No-JS tracker&lt;/a&gt; is only our second tracker: to fulfil our vision of supporting event-data collection across many more platforms, we need to launch a wide range of new trackers.&lt;/p&gt;

&lt;p&gt;We are getting close to launching an &lt;a href='https://github.com/snowplow/snowplow-arduino-tracker'&gt;Arduino tracker&lt;/a&gt; for Snowplow, which will enable data collection from physical events into Snowplow. As you might expect, mobile trackers (especially for Android and iOS) are high priorities oadmap, alongside other software trackers (e.g. Windows 8). It will take a lot of work (and trackers) to fulfil our vision of enabling data collection across any platform in Snowplow, but we are getting there steadily.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/22/snowplow-0.7.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/22/snowplow-0.7.1-released"/>
    <title>Snowplow 0.7.1 released, with easier-to-run Ruby apps</title>
    <updated>2013-01-22T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce the release of Snowplow version &lt;strong&gt;0.7.1&lt;/strong&gt;. This release is designed to make it much easier to install and run the two Snowplow Ruby applications:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;EmrEtlRunner&lt;/a&gt; - which runs the Snowplow ETL job&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/storage-loader'&gt;StorageLoader&lt;/a&gt; - which loads Snowplow events into Infobright&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the feedback we received, setting up and running these two Ruby apps was the most challenging (and error-prone) part of the Snowplow experience. Many thanks to all of those in the community who reported bugs with our original approach and suggested fixes!&lt;/p&gt;

&lt;p&gt;To streamline this process and reduce the chances of problems occurring, we have updated both Ruby apps to work in a &lt;a href='https://rvm.io/integration/bundler/'&gt;RVM+Bundler&lt;/a&gt; environment, inline with Ruby community best practice. Specifically, we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Created a simple &lt;a href='https://github.com/snowplow/snowplow/wiki/Ruby-and-RVM-setup'&gt;guide to setting up Ruby and RVM&lt;/a&gt; ready for Snowplow&lt;/li&gt;

&lt;li&gt;Updated both of our Ruby apps to use RVM and Bundler&lt;/li&gt;

&lt;li&gt;Updated our cronjob shell scripts to work with RVM and Bundler&lt;/li&gt;

&lt;li&gt;Updated the setup guides (&lt;a href='https://github.com/snowplow/snowplow/wiki/EmrEtlRunner-setup'&gt;EmrEtlRunner&lt;/a&gt;; &lt;a href='https://github.com/snowplow/snowplow/wiki/StorageLoader-setup'&gt;StorageLoader&lt;/a&gt;) for both apps to follow RVM and Bundler best practice&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release bumps EmrEtlRunner to version 0.0.8 and StorageLoader to version 0.0.4.&lt;/p&gt;

&lt;p&gt;And that&amp;#8217;s it! Hopefully this release fixes all of the Ruby-related issues encountered by Snowplow users - but of course there might still be a couple of teething issues. If you spot anything that still doesn&amp;#8217;t seem right, do please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/21/working-out-what-data-to-pass-into-your-tag-manager</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/21/working-out-what-data-to-pass-into-your-tag-manager"/>
    <title>What data should you be passing into your tag manager?</title>
    <updated>2013-01-21T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Since the launch of &lt;a href='https://www.google.com/tagmanager/'&gt;Google Tag Manager&lt;/a&gt;, a plethora of blog posts have been written on the value of tag management solutions. What has been left out of the discussion is practical advice on how to setup your tag management solution (be it &lt;a href='https://www.google.com/tagmanager/'&gt;GTM&lt;/a&gt; or &lt;a href='http://www.opentag.qubitproducts.com/'&gt;OpenTag&lt;/a&gt; or one of the paid solutions), and, crucially, what data you should be passing into your tag manager. In this post, we will outline a methodology for identifying all the relevant data you should be passing in, and bringing that methodology to life with a real-world example.&lt;/p&gt;
&lt;img src='/static/img/tag-management/tag-management-schematic.gif' width='320' /&gt;
&lt;h2 id='why_is_it_important_to_define_at_implementation_time_what_data_to_pass_to_your_tag_manager'&gt;Why is it important to define, at implementation time, what data to pass to your tag manager?&lt;/h2&gt;

&lt;p&gt;One of the things we hear a lot from proponents of tag management (especially from web analyts) is that they make it easy to capture data from web pages. Indeed, the &lt;em&gt;best&lt;/em&gt; solutions enable analysts with no development knowledge to identify and capture new data points, to pass onto their web analytics program, without any programming knowledge, using snazzy drag-and-drop UIs.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;We think this is rather dangerous. We&amp;#8217;re much more excited about the way that tag management solutions enable webmasters to explicitly pass data into their tag management solutions using constructs like the &lt;a href='https://developers.google.com/tag-manager/reference'&gt;&lt;code&gt;dataLayer&lt;/code&gt;&lt;/a&gt; in GTM and the &lt;a href='https://github.com/QubitProducts/UniversalVariable'&gt;&lt;code&gt;Universal Variable&lt;/code&gt;&lt;/a&gt; in OpenTag. The nice thing about this approach is that the infrastructure for managing the flow of data into your analytics infrastructure is decoupled from the infrastructure delivering the end-user experience. It means that web masters are free to improve websites, able to modify elements of web pages without breaing any data transfer processes. It forces companies to think through what data they should be capturing, and document it. It makes it easy for analysts and data scientists, down the line, to audit what data is being collected and how.&lt;/p&gt;

&lt;p&gt;The trouble with insisting on formally passing data to your tag management system using things like the &lt;a href='https://developers.google.com/tag-manager/reference'&gt;&lt;code&gt;dataLayer&lt;/code&gt;&lt;/a&gt; or &lt;a href='https://github.com/QubitProducts/UniversalVariable'&gt;&lt;code&gt;Universal Variable&lt;/code&gt;&lt;/a&gt; is that it makes the process of implementing a tag management system more complicated: because you have to identify all the data points you want to pass to your web analytics (and advertising) systems and often develop a data model for transferring them to your tag management system, so that it can pass them on via the tags it fires.&lt;/p&gt;

&lt;p&gt;For Snowplow users, the challenge is more acute. Whereas other analytics systems recommend that you only pass data into them that you know how to use / evaluate, we recommend that Snowplow users pass in &lt;em&gt;all&lt;/em&gt; the data associated with the events on a user journey so that analysts have a &lt;strong&gt;complete&lt;/strong&gt; picture of a user&amp;#8217;s journey. Then it is up to the analyst to decide whether or not specific bits of data are valuable based on what he / she does with that data (rather than prejudging it). So for Snowplow users who are setting up a tag management system, the challenge is to identify, upfront &lt;strong&gt;all&lt;/strong&gt; the data points to pass into the tag management system, so that they can be passed on to Snowplow via the Snowplow tracking tags. Simple, right?&lt;/p&gt;

&lt;h2 id='what_data_do_we_want_to_pass_into_snowplow'&gt;What data do we want to pass into Snowplow?&lt;/h2&gt;

&lt;p&gt;Broadly speaking, there are types of data that we want to process in Snowplow: event data and page-level data.&lt;/p&gt;

&lt;h3 id='event_data'&gt;Event data&lt;/h3&gt;

&lt;p&gt;At its heart, Snowplow is a tool for capturing, storing and analysing event-stream data, with a focus on web event data. We aim to capture all events that occur on an individual&amp;#8217;s customer journey. To give a random assortment of examples of the types of events we might include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add item to basket&lt;/li&gt;

&lt;li&gt;Like a post&lt;/li&gt;

&lt;li&gt;Invite someone to be friends&lt;/li&gt;

&lt;li&gt;Watch a video&lt;/li&gt;

&lt;li&gt;Review a product&lt;/li&gt;

&lt;li&gt;Log in to an application&lt;/li&gt;

&lt;li&gt;Create and display a graph&lt;/li&gt;

&lt;li&gt;Send a message&lt;/li&gt;

&lt;li&gt;Create a listing&lt;/li&gt;

&lt;li&gt;Update a status&lt;/li&gt;

&lt;li&gt;Ask a question&lt;/li&gt;

&lt;li&gt;Book a flight&lt;/li&gt;

&lt;li&gt;Donate to a charity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As part of setting up a tag management solution, it is important to identify all the possible events that can occur to a user on his / her journey through our website, and what data we want to capture for each of those event types.&lt;/p&gt;

&lt;h3 id='web_page_entity_data'&gt;Web page entity data&lt;/h3&gt;

&lt;p&gt;As the web evolves, websites look less-and-less like hyperlinked documents and more-and-more like interactive applications. A larger fraction of interesting events on a customer journeys are powered by AJAX events, and fewer are enabled by web page loads.&lt;/p&gt;

&lt;p&gt;In spite of this evolution, web page loads are still very important events in a user journey. Broadly speaking, we capture data that occur thanks to AJAX events using the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-events'&gt;Snowplow event tracking method&lt;/a&gt; except for specific events that have their own specific methods e.g. &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-ecommerce'&gt;tracking ecommerce transaction&lt;/a&gt; or &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-adimps'&gt;ad impression tracking&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To capture the broad swathe of events that result in a web page load, we use the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-page'&gt;page tracker method&lt;/a&gt;. However, performing an analysis on the journey a user has taken based on the URLs and page titles of the pages they have visited is not that informative: we really want to store what entities were displayed on those web pages, so we can analyse what the user was shown, what entities they engaged with and which they did not. To take a simple example, we might want to compare conversion rate for a retailer by product, to see which products &amp;#8216;convert best&amp;#8217; and why. In order to do this, we need to pass onto Snowplow exactly what products were displayed on the web pages they visited, and potentially pass in additional information like what type of listing they were shown.&lt;/p&gt;

&lt;p&gt;To give an example of the types of entities we might identify:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Products&lt;/li&gt;

&lt;li&gt;Articles / blog posts&lt;/li&gt;

&lt;li&gt;Videos&lt;/li&gt;

&lt;li&gt;Adverts&lt;/li&gt;

&lt;li&gt;People / connections&lt;/li&gt;

&lt;li&gt;Jobs&lt;/li&gt;

&lt;li&gt;Flights&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As well as identifying all the events that can occur on a user journey, then, we &lt;em&gt;also&lt;/em&gt; need to identify &lt;strong&gt;all&lt;/strong&gt; the key elements that make up each web page, and pass them to our tag manager on page load, as part of a tag management implementation.&lt;/p&gt;

&lt;h2 id='summarising_our_method_for_identifying_all_the_data_points_to_pass_into_the_tag_manager'&gt;Summarising our method for identifying all the data points to pass into the tag manager&lt;/h2&gt;

&lt;p&gt;We are now in a position to pull the above information together and summarise our suggested approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Identify all the relevant events that can occur to a user navigating your website app. Catalogue each.&lt;/li&gt;

&lt;li&gt;For each event type, document what data should/could be captured&lt;/li&gt;

&lt;li&gt;Comb through each web page that makes up your website, and identify the relevant entities that that make up web pages.&lt;/li&gt;

&lt;li&gt;For each entity, document what data you want to capture&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='bringing_the_method_to_life_a_realworld_example'&gt;Bringing the method to life: a real-world example&lt;/h2&gt;

&lt;p&gt;We applied the above methodology to &lt;a href='http://www.psychicbazaar.com/index.php'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer in the esoteric space. The good folks at Psychic Bazaar have kindly allowed us to share the implementation guide, so you can see the approach in action. Psychic Bazaar is in the process of implementing Google Tag Manager. However, the approach outlined is tag manager agnostic: if they implemented OpenTag instead, then references to the &lt;code&gt;dataLayer&lt;/code&gt; would be replaced to references to the &lt;code&gt;Universal Variable&lt;/code&gt; - the actual data and structure of the data would remain unchanged.&lt;/p&gt;
&lt;a href='/static/pdf/google-tag-manager-implementation-specification-for-psychic-bazaar.pdf'&gt;&lt;img src='/static/img/tag-management/gtm-spec-title-page.JPG' /&gt;&lt;/a&gt;
&lt;h2 id='want_help_implementing_a_tag_management_solution'&gt;Want help implementing a tag management solution?&lt;/h2&gt;

&lt;p&gt;The Snowplow &lt;a href='/services/index.html'&gt;Professional Services team&lt;/a&gt; can produce implementation guides like &lt;a href='/static/pdf/google-tag-manager-implementation-specification-for-psychic-bazaar.pdf'&gt;the one for Psychic Bazaar&lt;/a&gt;. If you are implementing a tag management solution, either as part of a Snowplow implementation or not, and and would like assistance, then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/20/snowplow-hits-202-stars</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/20/snowplow-hits-202-stars"/>
    <title>Snowplow reaches 202 stars on GitHub</title>
    <updated>2013-01-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;As of this weekend, the Snowplow repository on GitHub now has over 200 stars! We&amp;#8217;re hugely excited to reach this milestone - this makes us:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The 3rd most-watched analytics project on GitHub, after &lt;a href='https://github.com/mnutt/hummingbird'&gt;Hummingbird&lt;/a&gt; (real-time analytics) and &lt;a href='https://github.com/Countly/countly-server'&gt;Countly&lt;/a&gt; (mobile analytics)&lt;/li&gt;

&lt;li&gt;The &lt;a href='https://github.com/languages/Scala/most_watched?page=3'&gt;58th most-watched&lt;/a&gt; Scala project on GitHub&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many thanks to everyone in the Snowplow community and on GitHub for their support and interest!&lt;/p&gt;

&lt;p&gt;We thought it might be interesting to share the &lt;a href='http://jrvis.com/red-dwarf/?user=snowplow&amp;amp;repo=snowplow'&gt;Red Dwarf heatmap&lt;/a&gt; of where our 202 GitHub stars are located across the world:&lt;/p&gt;

&lt;p&gt;&lt;img alt='heatmap' src='/static/img/blog/2013/01/snowplow-stars-at-202.png' /&gt;&lt;/p&gt;

&lt;p&gt;Beyond the &amp;#8220;hot spots&amp;#8221; in the US and Europe, it&amp;#8217;s encouraging to see growing interest in Snowplow from South America, South/East Asia and Australia/New Zealand. We look forward to checking back on the heatmap when we have some more stars!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/18/using-snowplow-with-qubit-opentag</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/18/using-snowplow-with-qubit-opentag"/>
    <title>Implementing Snowplow with QuBit's OpenTag</title>
    <updated>2013-01-18T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;This is a short blog post to highlight a new section on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Snowplow-setup-guide'&gt;Snowplow setup guide&lt;/a&gt; covering &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;how to integrate Snowplow with QuBit&amp;#8217;s OpenTag tag management system&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In November last year, we started playing with tag management systems: testing Snowplow with Google Tag Manager, and documented how to setup Snowplow with GTM on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager'&gt;Snowplow setup guide&lt;/a&gt;. We were impressed on a number of fronts, but thought that the much more thought need to be put into what data was passed into the tag management system than people typically admit. (We documented our thoughts, at the time, on &lt;a href='/blog/2012/11/16/integrating-snowplow-with-google-tag-manager/'&gt;this blog post&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Since then, we&amp;#8217;ve recommended that &lt;strong&gt;all&lt;/strong&gt; new Snowplow users setup a tag management system, prior to integrating Snowplow on their website, if they have not already done so. The benefits of doing so are well documented elsewhere. For Snowplow users, there are two big benefits in particular, that we will flag:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Going through the exercise of implementing a tag management solution forces companies to take a rigorous look at the data they pass from their website into the tag manager, especially when declaring the data explicitly using things like the &lt;code&gt;dataLayer&lt;/code&gt; (in GTM) or the &lt;code&gt;Universal Variable&lt;/code&gt; in OpenTag. This makes it easier for analysts, down the line, to understand &lt;em&gt;what&lt;/em&gt; data has been passed into their web analytics system, and how that data has been generated: key bits of information that can often get lost months after web analytics platforms like Snowplow have been implemented. In addition, it makes the analytics as a whole more robust, as the generation of data is decoupled from the generation of other elements of web pages, which means web developers can continue to improve site functionality, safe in the knowledge they wont break anything on the analytics side.&lt;/li&gt;

&lt;li&gt;A selfish reason, perhaps, but having our customers use a tag management platforms gives us the freedom to improve Snowplow tracking tags where we see the opportunity, safe in the knowledge that we&amp;#8217;re not causing our clients too much difficulty to upgrade their tags.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='qubit-opentag-logo' src='/static/img/blog/2013/01/qubit-opentag.png' /&gt;&lt;/p&gt;

&lt;p&gt;We were eager, having integrated Snowplow with GTM, to integrate it with an open source tag management system. After all, a big selling point of Snowplow is that it is open source: enabling companies to setup, own and manage their own data infrastructure, without relying on a third party to mediate their access to their own data. We were therefore delighted that QuBit has developed an open source tag management system, &lt;a href='http://www.opentag.qubitproducts.com/'&gt;OpenTag&lt;/a&gt;, and that we have finally documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;how to integrate it with Snowplow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a lot we like about QuBit&amp;#8217;s OpenTag:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt;. You can view the &lt;a href='https://github.com/QubitProducts/OpenTag/blob/master/OpenTag.js'&gt;OpenTag.js&lt;/a&gt; on Github to get a handle on exactly how OpenTag works&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Low cost hosted service&lt;/strong&gt;. QuBit offers free hosting for sites with less than 1M page views per month, and $99 per 10M page views thereafter&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Easy-to-implement host-yourself option&lt;/strong&gt;. You can use OpenTag&amp;#8217;s web UI to configure all your tags for free and publish the results to a Javascript file that you can then host on your own CDN (e.g. Amazon Cloudfront). By not using Qubit to host the configured javascript file with all your different tags, you are not locked into QuBit as a vendor. In addition, the cost of managing your tags across large sites, content networks and ad networks is kept at a bare minimum.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to make it easier for new Snowplow users to implement OpenTag alongside Snowplow, and existing OpenTag users to implement Snowplow, we&amp;#8217;ve documented how to setup Snowplow in OpenTag on our &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20Javascript%20tags%20with%20QuBit%20OpenTag'&gt;setup guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Keep plowing!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/16/scala-maxmind-geoip-released"/>
    <title>Scala MaxMind GeoIP library released</title>
    <updated>2013-01-16T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A short blog post this, to announce the release of &lt;a href='https://github.com/snowplow/scala-maxmind-geoip'&gt;&lt;strong&gt;Scala MaxMind GeoIP&lt;/strong&gt;&lt;/a&gt;, our Scala wrapper for the MaxMind &lt;a href='http://www.maxmind.com/download/geoip/api/java/'&gt;Java Geo-IP&lt;/a&gt; library.&lt;/p&gt;

&lt;p&gt;We have extracted Scala MaxMind GeoIP from our current (ongoing) work porting our ETL process from Apache Hive to &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. We extracted this as a separate library for two main reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Being good open-source citizens&lt;/strong&gt; - as with our &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt; library, we believe this library willl be useful to the wider community of software developers, not just Snowplow users&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Keeping Snowplow&amp;#8217;s footprint small&lt;/strong&gt; - at Snowplow we believe very strongly in building modular, loosely-coupled software. Massive monolithic systems that &amp;#8216;do everything&amp;#8217; are a nightmare to test, maintain and extend - so we prefer to build small, standalone components and libraries which we (and the community) can then compose into larger pipelines and processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On to the library: for Scala developers, the main benefits of using &lt;a href='https://github.com/snowplow/scala-maxmind-geoip'&gt;scala-maxmind-geoip&lt;/a&gt; over the MaxMind Java library are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easier to setup/test&lt;/strong&gt; - the SBT project definition automatically pulls down the latest MaxMind Java code and &lt;code&gt;GeoLiteCity.dat&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better type safety&lt;/strong&gt; - the MaxMind Java library is somewhat null-happy. This library uses Option boxing wherever possible&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - as well as or instead of using MaxMind&amp;#8217;s own caching (&lt;code&gt;GEOIP_MEMORY_CACHE&lt;/code&gt;), you can also configure an LRU (Least Recently Used) cache of variable size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;#8217;s it! And if you have any problems with this Scala library for MaxMind GeoIP lookups, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/09/from-etl-to-enrichment"/>
    <title>The Snowplow development roadmap for the ETL step - from ETL to enrichment</title>
    <updated>2013-01-09T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In this blog post, we outline our plans to develop the &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;ETL&lt;/a&gt; (&amp;#8220;extract, transform and load&amp;#8221;) part of the Snowplow stack. Although in many respects the least sexy element of the stack, it is critical to Snowplow, and we intend to re-architect the ETL step in quite significant ways. In this post, we discuss our plans and the rationale behind them, in the hope to get:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feedback from the community on them&lt;/li&gt;

&lt;li&gt;Ideas for alternative approaches or new features&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#purpose'&gt;Recap: the point of the ETL step&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#limitations'&gt;Limitations with the current, Hive-based ETL process&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#enrichment'&gt;From ETL to enrichment&lt;/a&gt;: what we want the ETL step to achieve&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#speed'&gt;Towards a real-time ETL&lt;/a&gt;: speeding things up&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#scalding'&gt;Moving to Cascading / Scalding&lt;/a&gt;: what we plan to do&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/09/from-etl-to-enrichment/#benefits'&gt;Benefits of this approach&lt;/a&gt;: both in the short and long term&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To get the conversation started, a conceptual map of the new ETL process is shown below. You will probably want to click on it to see a blown-up PDF version, as it is rather large:&lt;/p&gt;
&lt;p&gt;&lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;&lt;img src='/static/img/blog/2013/01/scalding-etl-spec.gif' /&gt;&lt;/a&gt;&lt;/p&gt;&lt;!--more--&gt;&lt;a name='purpose'&gt;&lt;h2&gt;Recap: the point of the ETL step&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The primary purpose of the ETL step is to parse the logs generated by the Snowplow collector(s) and push the data stored into one or more storage facilities (e.g. S3, Infobright) where it can be accessed by analytic tools. However, there are two complexities that have to be dealt with:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Checking data quality and resolving any issues&lt;/strong&gt;. Sometimes, the Snowplow tracker has not been correctly configured; sometimes, there may even be a bug in a tracker or collector, which means that the log files contain errors. In an ideal world, the ETL step should validate the lines of data in the logs, push data through to storage when the data is good quality, and initiate a process for handling malformed data in the unfortunate cases when it is not. (Note: most web analytics programmes do not support this, so if you haven&amp;#8217;t set your tracking up properly and haven&amp;#8217;t been logging data correctly for a couple of months - tough - there&amp;#8217;s no way of fixing it.) By flagging malformed data quickly, the ETL step should also provide the ops team with a good guide to review the tracker and collector setup, and correct any mistakes.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Supporting multiple storage options&lt;/strong&gt;. We want Snowplow to support the widest range of analytics: encompassing &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP style aggregations&lt;/a&gt; slicing and dicing of data, &lt;a href='http://mahout.apache.org/'&gt;Mahout-like machine learning&lt;/a&gt; and &lt;a href='https://github.com/skydb'&gt;Sky-like&lt;/a&gt; event stream analytics. The ETL step has to be powerful enough to push data into multiple locations in an efficient manner, and support pushing different cuts and structures of the data into each of those different storage options as required.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='limitations'&gt;&lt;h2&gt;Limitations with the current, Hive-based ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The current ETL process is based on Hive, which processes Cloudfront-formatted log files containing querystrings matching the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow tracker protocol&lt;/a&gt; using a &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers'&gt;custom deserializer&lt;/a&gt;. This was a good option to build an initial prototype of the ETL step: it enabled us to query data in the raw logs directly, and made it relatively straightforward to transfer the data from the Snowplow log format into a more standard format suitable for faster querying in Hive or importing into Infobright.&lt;/p&gt;

&lt;p&gt;However, there are a number of limitations to the Hive-based ETL process:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;It makes error handling very difficult&lt;/strong&gt;. Either a row is processed, or it is not. There&amp;#8217;s no option to build more sophisticated data processing pipelines including flows to divert malformed data, spot the source of the data quality problem and address it.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is a tightly-coupled process&lt;/strong&gt;: all the parsing on the entire row is performed by the custom deserializer. If something goes wrong, it is hard to debug what went wrong. If we want to extend part of the ETL process, we have to go in and upgrade the deserializer or the HiveQL wrapper scripts. As the conceptual map of our proposed ETL shown at the top of this post demonstrates, our ideal ETL process consists of multiple steps. These should be decoupled for robustness and ease of extension.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;It is hard to extend the ETL process to build enrichments of the data&lt;/strong&gt;. (See the &lt;a href='#enrichments'&gt;next section&lt;/a&gt;.)&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='enrichment'&gt;&lt;h2&gt;From ETL to enrichment: what we want to achieve&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;The initial purpose of the ETL step was quite narrow: to move data generated by the collectors into the different storage options for analytics. Since then, we have realised that there are a number of important enrichments that can be performed on the data, that are best done as part of the ETL step, so that they are available when the data comes to be analysed. Examples include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Inferring location from &lt;code&gt;user_ipaddress&lt;/code&gt; e.g. using &lt;a href='http://www.maxmind.com/en/geolocation_landing'&gt;Maxmind&lt;/a&gt; or &lt;a href='http://www.digitalelement.com/our_technology/our_technology.html'&gt;Digital Element&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;Inferring marketing parameters (source, medium, keywords) by processing referrer url and query strings using &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;. This would include identifying search engine originated traffic and social network originated traffic, for example&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, decomposing some of the fields into constituent elements can make analysis easier: for example, breaking up &lt;code&gt;page_url&lt;/code&gt; and &lt;code&gt;referrer_url&lt;/code&gt; into host, domain, path and query string, can enable us to easily group visits by referer domain or path, depending on granularity of analysis we&amp;#8217;re performing.&lt;/p&gt;
&lt;a name='speed'&gt;&lt;h2&gt;Towards a real-time ETL process: speeding things up&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The majority of Snowplow users run their ETL process daily, so that yesterday&amp;#8217;s data is available today.&lt;/p&gt;

&lt;p&gt;We need to move the whole Snowplow stack so that data is available for analytics faster. Doing so will be welcomed by analysts crunching Snowplow data, but perhaps more significantly, it will open up the possibility of building real-time response engines based on Snowplow data: these might include things like retargeting users who&amp;#8217;ve performed specific actions with display ads or emails, or personalising the content shown to a user based on their recent browsing history, on the fly.&lt;/p&gt;

&lt;p&gt;There is limited scope to speed up the current Hive-based ETL process. However, there are lots of interesting opportunities that arise if we consider an alternative archtiecture, especially one that moves us closer to a stream-based data processing model.&lt;/p&gt;
&lt;a name='scalding'&gt;&lt;h2&gt;Moving to Cascading / Scalding: how we plan to rearchitect the ETL process&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;We intend to replace the current Hive-based ETL process with one based on the Scala library that runs on top of &lt;a href='http://www.cascading.org/'&gt;Cascading&lt;/a&gt;, known as &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cascading is an application framework specifically designed to build robust data pipelines using Hadoop. We intend to use it to build the pipeline &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;sketched above&lt;/a&gt;.&lt;/p&gt;
&lt;a name='benefits'&gt;&lt;h2&gt;Benefits of this approach: both in the short and long term&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;By rearchitecting the ETL using Scalding / Cascading, we hope to realise the following benefits in the short-term:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deliver enrichments on the data: in particular, classify visits based on referer, and locate users via geo-ip&lt;/li&gt;

&lt;li&gt;Improved handling of malformed data: making it easier to spot bugs in Snowplow, mistakes in tracker or collector setup, and the ability to fix and reprocess malformed data&lt;/li&gt;

&lt;li&gt;Make it easier to run the ETL process more frequently, so that Snowplow data is more up-to-date&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the long term there are a number of important benefits we hope moving to Scalding will help us realise:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Expand the ETL to output data in a format suitable for OLAP reporting&lt;/strong&gt;. Currently, users who want to use OLAP tools e.g. Tableau, Pentaho or Microstrategy, to report on Snowplow data, need to &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;transform that data&lt;/a&gt; prior to running those tools on top of it. We want to build out the ETL process to output two versions of the data: the raw event field (as it currently does) and a cube-formatted version that can be used directly with these tools. Delivering this with the current Hive-based process would be incredibly difficult.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Move towards a real-time engine&lt;/strong&gt;. In order to deliver data in real-time, Snowplow ETL would need to move from a Hadoop, batch-based process into a stream-based process, likely using &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. Porting the data pipeline from Cascading to Storm should be significantly easier than porting it from Hive to Storm: as such, Cascading provides a useful stepping stone on our journey to deliver real time event-level analytics.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a wider range of collector log formats&lt;/strong&gt;. Because the ETL process is decoupled, handling a different log file format means only updating the first processing step in the data pipeline that parses the raw collector logs. That means building out the ETL to support other collectors (e.g. &lt;a href='/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow/'&gt;SnowCannon&lt;/a&gt;) should be much simpler.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Make it easier to support a growing range of event types&lt;/strong&gt;. As should be clear from the &lt;a href='/static/pdf/snowplow-scalding-etl-specification.pdf'&gt;data pipeline flowchart&lt;/a&gt;, seven event types are currently supported, each with their own set of fields. (Page views, page pings, link clicks, custom events, ad impressions, transaction events and transaction items.) That list is only likely to grow over time. By clearly differentiating each of them in the data pipeline, a Scalding-based ETL process should be easier to extend to support a greater range of events.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id='we_want_your_feedback'&gt;We want your feedback&lt;/h2&gt;

&lt;p&gt;We&amp;#8217;ve been very lucky to have community members contribute an enormous number of fantastic ideas and code that we&amp;#8217;ve been able to incorporate into Snowplow. We&amp;#8217;ve shared our roadmap for the ETL step and our rationale for that roadmap to see what you think. Does our approach sound sensible? What should we do differently? What can we add to it to make it more robust and valuable?&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data"/>
    <title>Using ChartIO to visualise and interrogate Snowplow data</title>
    <updated>2013-01-08T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;In the last couple of weeks, we have been experimenting with &lt;a href='http://chartio.com/'&gt;ChartIO&lt;/a&gt; - a hosted BI tool for visualising data and creating dashboards. So far, we are very impressed - ChartIO is an excellent analytics tool to use to interrogate and visualise Snowplow data. Given the number of requests we get from Snowplow users to recommend tools to assist with analytics on Snowplow data, we thought it well worth sharing why ChartIO is so good, and give some examples of analyses on Snowplow data using ChartIO.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-0' src='/static/img/blog/2013/01/chartio-0.png' /&gt;&lt;/p&gt;

&lt;p&gt;In this post we cover:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#setup'&gt;Setting up ChartIO to work with Snowplow&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/08/using-chartio-to-visualise-and-interrogate-snowplow-data#engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a name='why'&gt;Why is ChartIO so good?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ChartIO is great for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;. ChartIO is quick to setup. (Because it is a hosted product, with a very nice script for establishing an SSH connection between your database and the ChartIO web application.) At the same time, it is very quick, once a data connection is established, to create new graphs and charts and embed them in dashboards.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt;. ChartIO is easy to use. This is partly because the UI is really nice. (Lots of drag and drop, easy-to-follow workflow.) But it is also because ChartIO is very simple: it lacks a lot of the complexity of more traditional BI tools like Microstrategy and Pentaho. It is a lot simpler even than more recent innovations in the space like Tableau. Whilst this means it is a bit less powerful, the upside is the tool is a lot easier to use than comparable tools.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ChartIO has one enormous advantage that makes it especially well suited to querying Snowplow data: it does not require the data to be in a specific format before it will let users chart / graph it. That compares with the vast majority of tools (including Tableau, Qlikview, Pentaho and Microstrategy) that all require that any data is structured in a format suitable for &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;OLAP analysis&lt;/a&gt; before they can be used. (We covered how to convert Snowplow data into that format in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;.) ChartIO &lt;strong&gt;does&lt;/strong&gt; work better with data that is formatted in this way, but it still works beautifully with the data as is. As a result, &lt;strong&gt;ChartIO is, we believe, the easiest way to build graphs and dashboards on top of Snowplow data&lt;/strong&gt;.&lt;/p&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='setup'&gt;Setting up ChartIO to work with Snowplow&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You can get started with ChartIO by signing up to a free 30 day trial. Connecting it to Snowplow data is straightforward: full instructions can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data'&gt;on the setup guide&lt;/a&gt;, including how to create your first graph using Snowplow data in ChartIO.&lt;/p&gt;
&lt;h2&gt;&lt;a name='engagement'&gt;Tutorial: using ChartIO to unpick the drivers of engagement with a site&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id='before_we_get_started_how_will_we_measure_engagement'&gt;Before we get started: how will we measure engagement?&lt;/h3&gt;

&lt;p&gt;As we discuss in detail in the &lt;a href='/analytics/customer-analytics/user-engagement.html'&gt;analytics cookbook&lt;/a&gt;, there are many possible ways to measure engagement, and Snowplow supports all of them. We need to pick one or two to use in this tutorial, although it would be possible to perform the analyses described with any measure that suits your business.&lt;/p&gt;

&lt;p&gt;For this tutorial we&amp;#8217;re going to use data from &lt;a href='http://www.psychicbazaar.com/'&gt;Psychic Bazaar&lt;/a&gt;, an online retailer of esoteric products. For an online retailer, whether a visitors makes a purchase is generally more interesting than whether they &amp;#8216;engage&amp;#8217; in vaguer terms. So we will use conversion rate as our first measure of engagement. However, to keep our tutorial interesting to people who want to perform the analysis on non-retail sites, we will also look at number of page views over a period of time as a measure of engagement.&lt;/p&gt;

&lt;h3 id='establishing_the_baseline_measuring_engagement_over_time'&gt;Establishing the baseline: measuring engagement over time&lt;/h3&gt;

&lt;p&gt;Lets start by looking out how engagement has changed over time on Psychic Bazaar. Let&amp;#8217;s create a new dashboard to explore this issue in particular. Log into ChartIO and click on the &lt;strong&gt;+Dashboard&lt;/strong&gt; link on the left hand menu to create a new dashboard.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-1' src='/static/img/blog/2013/01/chartio-1.png' /&gt;&lt;/p&gt;

&lt;p&gt;Give the dashboard a suitable name and description and then click the relevant button to craete it. Now we need to add a chart to it. Click on the &lt;strong&gt;+Chart&lt;/strong&gt; link on the right hand menu. The Chart Creator opens in &lt;strong&gt;interactive mode&lt;/strong&gt;, with your database on the top left, a list of tables under it (including the Snowplow events table) and under the table, a list of fields split by which ChartIO believes is a measure and dimension.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-2' src='/static/img/blog/2013/01/chartio-2.png' /&gt;&lt;/p&gt;

&lt;p&gt;In interactive mode, ChartIO lets you drag and drop measures into the &lt;strong&gt;Measures&lt;/strong&gt;, &lt;strong&gt;Dimensions&lt;/strong&gt; and &lt;strong&gt;Filters&lt;/strong&gt; dialogue box to generate graphs. We&amp;#8217;re not going to do that, though, because we want to be explicit about how ChartIO uses Snowplow data. So we&amp;#8217;re going to use &lt;strong&gt;Query mode&lt;/strong&gt; by clicking on the &lt;strong&gt;Query mode&lt;/strong&gt; hyperlink on the top left of the &lt;strong&gt;Layer 1&lt;/strong&gt; box. This enables us to enter a SQL query directly. ChartIO will graph the results:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-3' src='/static/img/blog/2013/01/chartio-3.png' /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;#8217;re ready to graph engagement levels over time. Let&amp;#8217;s start with our first measure of engagement: conversion levels. We want to look at what % of users who visit our site each month that perform a transaction. To do this, we first need to identify users who have performed a transaction each month, using the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;),&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we join this table with the events table to list all the users who have visited each month and identify which of them has bought:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can aggregate over the results of the above query, calculating the conversion rate by dividing the number of buyers by the total number of visitors:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='ss'&gt;`converted_visitors`&lt;/span&gt; &lt;span class='o'&gt;/&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;
	&lt;span class='k'&gt;AND&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Pop the above query in the ChartIO query box:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-4' src='/static/img/blog/2013/01/chartio-4.png' /&gt;&lt;/p&gt;

&lt;p&gt;and click the &lt;strong&gt;Chart Query&lt;/strong&gt; button below. ChartIO will respond with a table of data. We can graph the data by clicking on any of the graph icons above the data table. Choosing the line graph, I get:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-6' src='/static/img/blog/2013/01/chartio-6.png' /&gt;&lt;/p&gt;

&lt;p&gt;We can then rename the graph (by clicking the &lt;strong&gt;edit&lt;/strong&gt; hyperlink that appears when you hover over &lt;strong&gt;Chart Title&lt;/strong&gt;) and save the graph to our dashboard by clicking &lt;strong&gt;Save to Exploring engagement&lt;/strong&gt; button. ChartIO lets us resize and position the graph on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-7' src='/static/img/blog/2013/01/chartio-7.png' /&gt;&lt;/p&gt;

&lt;p&gt;Great! We can see conversion rates were reasonably stable between September and November of the year, but peaked at the end of the year at a height they were previously in June. The figure for September seems suspiciously high - we&amp;#8217;ll drill into this in more detail in a bit. Next we will plot our alternative measure of engagement over time: the number of pageviews per user per month, and see how that has changed over time.&lt;/p&gt;

&lt;p&gt;Calculating the number of pageviews per user per month is straightforward in Snowplow - we can use the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we want to aggregate users by the number of pageviews each has done by month:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Lastly we want to bucket values of page views e.g. into 1, 2-5, 6-10, 11-25 and 25+. We can introduce a bucketing into our previous query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;CASE&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;6-10&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;2-5&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then aggregate by bucket in the next query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;uniques&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='n'&gt;page_views&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='k'&gt;CASE&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;25&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;25+&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;11-25&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;5&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;06-10&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;WHEN&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='o'&gt;&amp;gt;&lt;/span&gt;  &lt;span class='mi'&gt;1&lt;/span&gt; &lt;span class='k'&gt;THEN&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;02-05&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;ELSE&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;01&amp;#39;&lt;/span&gt;
	&lt;span class='n'&gt;END&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
	&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;distinct&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='nf'&gt;count&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
	&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt; &lt;span class='k'&gt;ASC&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;page_views&lt;/span&gt; &lt;span class='k'&gt;DESC&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;u&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;bucket&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create a new chart in ChartIO using the above query and graph it using the first bar chart icon. Give the graph a suitable name:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-8' src='/static/img/blog/2013/01/chartio-8.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph tells an interesting story. Overall, the number of unique visitors per month has grown pretty dramatically over time, peaking at about 1700 uniques in November. It is not so easy to tell how the distribution of users by engagement level has changed over time: this is easier if we change the graph to be a &amp;#8220;percent bar&amp;#8221;:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-9' src='/static/img/blog/2013/01/chartio-9.png' /&gt;&lt;/p&gt;

&lt;p&gt;This graph suggests that engagement levels dropped in October, but climbed dramatically from then to December. Curiously, there was no drop in overall engagement level as user numbers increased on the site between August and October: that means that the new users acquired were &amp;#8220;high quality&amp;#8221; or &amp;#8220;highly engaged&amp;#8221;. This is a useful graph: let&amp;#8217;s add it to our dashboard alongside the first graph we created:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-10' src='/static/img/blog/2013/01/chartio-10.png' /&gt;&lt;/p&gt;

&lt;p&gt;Just to put the two baseline graphs in context, let&amp;#8217;s add a third graph the tracks the number of unique users per month to our dashboard. Add a new chart using the following simple query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;as&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_004&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;span class='k'&gt;ORDER&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And pop it on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-11' src='/static/img/blog/2013/01/chartio-11.png' /&gt;&lt;/p&gt;

&lt;p&gt;Our baseline data tells us an interesting story, which from the dashboard, we&amp;#8217;re in a position to summarise:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Overall, visitor numbers have increased pretty dramatically between June and September&lt;/li&gt;

&lt;li&gt;Over that same period, there was no corresponding drop in engagement, in terms of numbers of page views by visit. If anything, there was a slight increase&lt;/li&gt;

&lt;li&gt;Looking at conversion rates over the same time, the picture is much more hairy. (With a surprising spike in July.) In fact, we know this was to do with a bug on the website, which prevented data being collected from any page apart from the checkout page. Hence user numbers are underreported, but conversion rates are overstated, for July&lt;/li&gt;

&lt;li&gt;Conversion rates and average page views per visit rise in December&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='unpicking_the_drivers_of_changing_engagement_levels'&gt;Unpicking the drivers of changing engagement levels&lt;/h3&gt;

&lt;p&gt;For our sample data set there appears to be a rather interesting rise in engagement level (as measured by both conversion rates and page views by month) between November and December. What&amp;#8217;s driving that increase? What clues can our Snowplow data give us?&lt;/p&gt;

&lt;p&gt;We can divide drivers into two groups: those that effect all users on our website, and those that only effect some of them. If, for example, we performed a measure rearchitecture of our entire site, that is likely to effect &lt;strong&gt;all&lt;/strong&gt; users&amp;#8217; behavior. But if we upgraded the site for mobile, then we would &lt;strong&gt;only&lt;/strong&gt; expect that to impact user behavior for people browsing from mobile sites.&lt;/p&gt;

&lt;p&gt;A good approach, then, to unpick what&amp;#8217;s driving growth in engagement levels is to see if this growth is consistent across all users, or just some of them. One easy way to do this is to compare engagement rates between different types of users, to see if we can spot a difference. It makes sense to start off with factors we have a hunch might be driving those changes (e.g. because we&amp;#8217;re familiar with what has changed at those business over the months in question.) To give a specific examples:&lt;/p&gt;

&lt;h4 id='comparing_engagement_levels_between_users_from_paid_search_campaigns_and_notpaid_search_campaigns'&gt;Comparing engagement levels between users from paid search campaigns and not-paid search campaigns&lt;/h4&gt;

&lt;p&gt;Psychic Bazaar&amp;#8217;s only direct marketing spend is on paid search campaigns on Google and Bing. We might therefore wonder whether a change to those campaigns drove the uplift in engagement we see on the site in September. To do this, first we need to identify all the users acquired via paid search:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Compare this with our data on which users have converted:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And our list of &lt;strong&gt;all&lt;/strong&gt; users:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We join the three data sets:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then aggregate over the result set to compare conversion rates:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;&lt;span class='o'&gt;/&lt;/span&gt;&lt;span class='nf'&gt;sum&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;visitor&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;conversion_rate&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;buyer&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;visitor&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;buyer&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;transaction&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;buyers&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
		&lt;span class='k'&gt;SELECT&lt;/span&gt;
		&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
		&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
		&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
		&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
		&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
		&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;visitors&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
	&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;t&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the data in ChartIO we can see that users acquired from paid campaigns are much more likely to convert:&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-12' src='/static/img/blog/2013/01/chartio-12.png' /&gt;&lt;/p&gt;

&lt;p&gt;This naturally leads to the question: has the number of users acquired from paid search increased over the time period? (Especially between November and December, when our increase in conversion rates is most noticeable?) We can find out by graphing the following query, which looks at the number of uniques by month divided by whether they were acquired by paid search or not:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='mysql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='nf'&gt;MONTH&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='nf'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;DISTINCT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;uniques&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;
&lt;span class='k'&gt;LEFT&lt;/span&gt; &lt;span class='k'&gt;JOIN&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;
	&lt;span class='k'&gt;SELECT&lt;/span&gt;
	&lt;span class='n'&gt;user_id&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
	&lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt; &lt;span class='k'&gt;AS&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
	&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events_005&lt;/span&gt;
	&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;mkt_medium&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;cpc&amp;#39;&lt;/span&gt;
	&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;span class='k'&gt;ON&lt;/span&gt; &lt;span class='n'&gt;e&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;user_id&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='ss'&gt;`month`&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;paid_search&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Plotting the above graph shows that growth in paid search traffic accounts for some of the growth in traffic volumes between July and September. However, there was &lt;strong&gt;no&lt;/strong&gt; increase in traffic from paid search terms between November and December, so this does &lt;strong&gt;not&lt;/strong&gt; account for the rising conversion rate in December.&lt;/p&gt;

&lt;p&gt;&lt;img alt='chartio-pic-13' src='/static/img/blog/2013/01/chartio-13.png' /&gt;&lt;/p&gt;

&lt;h4 id='other_factors_that_might_account_for_the_rise'&gt;Other factors that might account for the rise&lt;/h4&gt;

&lt;p&gt;There is a wealth of other factors that we can explore using Snowplow data, to see if they account for the rise in engagement levels / conversion rates in December. Doing so is beyond the scope of this blog post. However, we can outline them:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Factor&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;How we would test it&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvement to the site structure (e.g. homepage)&lt;/td&gt;&lt;td style='text-align: left;'&gt;Investigate how engagement levels vary by visit by landing page, and see if those changes by landing page on dates when those web pages were updated&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Improvements to the checkout flow&lt;/td&gt;&lt;td style='text-align: left;'&gt;Compare the conversion funnel between November and December - see if there&amp;#8217;s a specific point in the funnel where conversion rates change, and see if that change can be attributed to a development change&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;A change in the makeup of the users e.g. so that in December, a bigger portion of the userbase are repeat visitors&lt;/td&gt;&lt;td style='text-align: left;'&gt;Explore whether there is a change in makeup (e.g. more repeat visitors as a proportion of uniques) and see if there&amp;#8217;s a corresponding difference in conversion rates by different types of users (e.g. new vs returning). Note: this is the same approach as described above for user acquired from &lt;em&gt;paid search&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Christmas&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hard to prove definitively - but if no other factor can be identified, and the engagement level drops back in January, then the December bump might be season.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Snowplow makes it possible to drill into all of the above, and other factors we can think of, to see which is responsible for driving changing engagement levels.&lt;/p&gt;

&lt;h2 id='summarising_our_thoughts_on_chartio'&gt;Summarising our thoughts on ChartIO&lt;/h2&gt;

&lt;p&gt;From our experience with it in the last couple of weeks, we believe that ChartIO is an excellent tool for visualising Snowplow data. We highly recommend Snowplow users give it a try,: ChartIO&amp;#8217;s simplicitly, speed, and lack of assumptions about the way data is structured make it an ideal analytics tool to run directly on top of Snowplow data stored in Infobright.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re going to continue to use ChartIO (and blog about the results). We&amp;#8217;d love to hear from other Snowplow users who are using it.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/07/the-clojure-collector-in-detail"/>
    <title>Understanding the thinking behind the Clojure Collector, and mapping out its development going forwards</title>
    <updated>2013-01-07T00:00:00+00:00</updated>
    <author>
      <name>yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last week we released &lt;a href='/blog/2013/01/03/snowplow-0.7.0-released/'&gt;Snowplow 0.7.0&lt;/a&gt;: which included a new Clojure Collector, with some significant new functionality for content networks and ad networks in particular. In this post we explain a lot of the thinking behind the Clojure Collector architecture, before taking a look ahead at the short and long-term development roadmap for the collector.&lt;/p&gt;

&lt;p&gt;This is the first in a series of posts we write where describe in some detail the thinking behind the architecture and design of Snowplow components, and discuss how we plan to develop those components over time. The purpose of doing so is to engage people like yourself: developers and analysts in the Snowplow community, in a discussion about how best to evolve Snowplow. The reasoning is simple: we have had many fantastic ideas and contributions from community members that have proved invaluable in driving Snowplow development, and we want to encourage more of these conversations and contributions, to help make Snowplow great.&lt;/p&gt;

&lt;p&gt;&lt;img alt='engine' src='/static/img/blog/2013/01/engine.jpg' /&gt;&lt;/p&gt;

&lt;h2 id='contents'&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#biz-case'&gt;The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#under-the-hood'&gt;Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#short-term-roadmap'&gt;Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2013/01/07/the-clojure-collector-in-detail#long-term-roadmap'&gt;Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;h2&gt;&lt;a name='biz-case'&gt;1. The business case for a new collector: understanding the limitations of the Cloudfront Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We launched Snowplow with the &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-cloudfront-collector'&gt;Cloudfront Collector&lt;/a&gt;. The Cloudfront Collector is simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Snowplow tracking pixel is served from Amazon Cloudfront&lt;/li&gt;

&lt;li&gt;Cloudfront logging is switched on (so that every time the pixel is fetched by a Snowplow tracking tag, the request is logged).&lt;/li&gt;

&lt;li&gt;Events and associated data points we want to capture are stored as name / value pairs and appended to the query string for the tracking pixel, so that they are automatically logged.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Cloudfront Collector is so simple that many people express surprise that there are so few files in the appropriate section of the &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector'&gt;Snowplow repo&lt;/a&gt;. (It&amp;#8217;s just a &lt;code&gt;readme&lt;/code&gt; and the tracking pixel.) In spite of that simplicity, however, the Cloudfront Collector boasts two key strengths:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;. Simplicity is a strength: because it has no moving parts, the Cloudfront Collector is incredibly robust. It makes no decisions. All it does is faithfully log requests made to the tracking pixel. There is very little that can go wrong with it. (Nothing if Cloudfront stays live.)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;. By using Amazon&amp;#8217;s content distribution network (CDN) to serve the tracking pixel and log requests for the tracking pixel, we can be confident that businesses using the Cloudfront Collector will be able to comfortably track millions of requests per hour. Amazon&amp;#8217;s CDN has been designed to be elastic: it responds automatically to spikes in demand, so you can be confident that even in during peak demand periods, all your data will successfully be captured and stored.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nonetheless, there are two major limitations to the Cloudfront Collector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Unable to track users across domains&lt;/strong&gt;. Because Snowplow has been designed to be scalable, we&amp;#8217;ve had a lot of interest in it from media groups, content networks and ad networks. All of these companies want to track individual users across multiple websites. This is not directly supported by the Cloudfront Collector: because it has no moving parts, user identification has to be performed client side, by the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker-setup'&gt;Javascript tracker&lt;/a&gt; using first party cookies. As a result, &lt;code&gt;user_id&lt;/code&gt;s that are set on one domain cannot be accessed on another domain, even if both domains are owned and operated by the same company.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Not real-time&lt;/strong&gt;. Cloudfront log files typically appear in S3 3-4 hours after the requests logged were made. As a result, if you rely on the Cloudfront cCollector for your web analytics data, you will always be looking at data that is at least 3-4 hours old.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The Clojure Collector explicitly addresses the first issue identified above: it has a single moving part, which checks if a &lt;code&gt;user_id&lt;/code&gt; has been set for this user: if so, it logs that &lt;code&gt;user_id&lt;/code&gt;. If not, it sets a &lt;code&gt;user_id&lt;/code&gt; (server side), and stores that &lt;code&gt;user_id&lt;/code&gt; in a cookie on the collectors own domain, accessible from any website running Snowplow that uses the same collector.&lt;/p&gt;

&lt;p&gt;The Clojure Collector does not explicitly address the second issue related to the speed at which data is logged for analysis. Although it logs data faster than the Cloudfront Collector (logs are rotated to S3 hourly), this is still not fast enough for real time analysis. However, it is fast enough that the processing bottleneck shifts from the collector to the ETL step: this is something we plan on addressing in the near future. (More on this &lt;a href='#long-term-roadmap'&gt;later&lt;/a&gt;.)&lt;/p&gt;
&lt;h2&gt;&lt;a name='under-the-hood'&gt;2. Under the hood: the design decisions behind the Clojure Collector&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We made three important design decisions when building the Clojure Collector:&lt;/p&gt;

&lt;h3 id='1_built_for_elastic_beanstalk'&gt;1. Built for Elastic Beanstalk&lt;/h3&gt;

&lt;p&gt;We built the Clojure Collector specifically for Elastic Beanstalk. This has a number of important advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comfortably scales to handle spikes in demand&lt;/strong&gt;. Elastic Beanstalk is &lt;strong&gt;elastic&lt;/strong&gt; in the same way as Cloudfront is &lt;strong&gt;elastic&lt;/strong&gt;. It makes it easy to scale services to handle spikes in demand, which is crucial if we&amp;#8217;re going to continue to track events data during spikes in service usage.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Automatic logging to S3&lt;/strong&gt;. Elastic Beanstalk supports a configuration option that automatically rotates Tomcat access logs to Elastic Beanstalk hourly. By using this feature, we were able to save ourselves having to build build a process to manage that log rotation, and save our users the hassle of installing and maintaining the process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Amazon Elastic Beanstalk supports open source applications built for the JVM, or PHP, Python and Ruby web apps. Of the four, it was clear that JVM was the most performative platform to build a Collector in.&lt;/p&gt;

&lt;h3 id='2_minimal_moving_parts'&gt;2. Minimal moving parts&lt;/h3&gt;

&lt;p&gt;The Clojure collector &lt;em&gt;only&lt;/em&gt; sets &lt;code&gt;user_id&lt;/code&gt;s and expiry dates on those &lt;code&gt;user_id&lt;/code&gt;s. It does &lt;em&gt;nothing&lt;/em&gt; else: keeping it as simple as possible.&lt;/p&gt;

&lt;h3 id='3_log_files_formats_match_those_produced_by_cloudfront'&gt;3. Log files formats match those produced by Cloudfront&lt;/h3&gt;

&lt;p&gt;The least wieldy part of the Snowplow stack today is the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL step&lt;/a&gt;. This parses the log files produced by the collector, extracts the relevant data points and loads them into S3 for processing by Hadoop/Hive and/or Infobright for processing in a wide range of tools e.g. &lt;a href='http://chartio.com'&gt;ChartIO&lt;/a&gt; or &lt;a href='http://www.r-project.org/'&gt;R&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have plans to replace the current &lt;a href='https://github.com/snowplow/snowplow/wiki/hive-etl-setup'&gt;Hive-based ETL process&lt;/a&gt; with an all new process based on &lt;a href='https://github.com/twitter/scalding'&gt;Scalding&lt;/a&gt;. (More on this in the next blog post in this series.) In the meantime, however, we did not want to have to write a new Hive deserializer to parse log files that match a new format: instead, we customised Tomcat in the Clojure Collector to output log files that matched the Cloudfront logging format. (This involved writing a custom &lt;a href='[tomcat-cf-access-valve]'&gt;Tomact Access Valve&lt;/a&gt; and tailoring &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml'&gt;Tomcat&amp;#8217;s server.xml&lt;/a&gt;.) As a result, the new Clojure Collector plays well with the existing ETL process.&lt;/p&gt;
&lt;h2&gt;&lt;a name='short-term-roadmap'&gt;3. Moving forwards: short term Clojure Collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is the initial release of the Clojure Collector. If it will be deployed by large media companies, content networks and ad networks, it is important that we learn how to configure it to function well at scale. To this end, we are looking for help, from members of the Snowplow community (particularly those with an interest in tracking users across domains), to help with the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Load testing the collector. Test how fast the collector responds to increasing number of requests per second, and how this varies by the size of instance offered by Amazon. (E.g. how does the curve differ for an m1.small instance than an m1.large instance?) It should be possible to use a tool like &lt;a href='http://www.joedog.org/siege-home/'&gt;Siege&lt;/a&gt; or &lt;a href='http://httpd.apache.org/docs/2.2/programs/ab.html'&gt;Apache Bench&lt;/a&gt; to test response levels and response times at increasing levels of request concurrency, and plot one against the other.&lt;/li&gt;

&lt;li&gt;On the basis of the above, working out the optimal way of setting up the Clojure Collector on Elastic Beanstalk. It would be good to answer two questions in particular: what size instance is it most cost effective to use, and what should trigger the starting up of an additional instance to cope with a spike in traffic? Amazon makes it possible to specify custom KPI to use to trigger scaling of services on Elastic Beanstalk, and it may be that doing so results in much improved performance and reliability from the Collector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because we haven&amp;#8217;t been able to perform the above tests to date, we&amp;#8217;re still calling the Clojure Collector an experimental release, adn recommend that companies using it in production run it alongside the Cloudfront Collector.&lt;/p&gt;
&lt;h2&gt;&lt;a name='long-term-roadmap'&gt;4. Looking ahead: long term collector roadmap&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Long term we need to move the whole Snowplow so that it&amp;#8217;s processing data faster, closer to real-time. This primarily means moving the &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-an-etl-module'&gt;ETL&lt;/a&gt; process from a Hadoop, batch-based process that is run at regular intervals to a stream-based, always on process, using a technology like &lt;a href='http://storm-project.net/'&gt;Storm&lt;/a&gt;. In the next post in this blog post series, we will elaborate further on our proposed developments for this part of the Snowplow stack. When the time comes, however, we will need to build a new collector, or modify an existing collector, to work in a stream-based system. (So that rather than rely on the processing of logs, each new event logged generates a message in a queue that kicks of a set of analytic processing tasks that end with the data being stored in S3 / Infobright.)&lt;/p&gt;

&lt;h2 id='want_to_get_involved'&gt;Want to get involved?&lt;/h2&gt;

&lt;p&gt;Want to help us develop the Clojure Collector, or some other part of the Snowplow stack? Have an idea about what we should be doing better, or differently? Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt;. We&amp;#8217;d love to hear from you.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/03/snowplow-0.7.0-released"/>
    <title>Snowplow 0.7.0 released, with new Clojure-based collector</title>
    <updated>2013-01-03T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are hugely excited to announce the release of Snowplow version &lt;strong&gt;0.7.0&lt;/strong&gt;, which includes an experimental new &lt;a href='https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector'&gt;Clojure-based collector&lt;/a&gt; designed to run on &lt;a href='http://aws.amazon.com/elasticbeanstalk/'&gt;Amazon Elastic Beanstalk&lt;/a&gt;. This release allows you to use Snowplow to uniquely identify and track users across multiple domains - even across a whole content or advertising network.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/shermozle'&gt;Simon Rumble&lt;/a&gt; for developing many of the ideas underpinning the new collector in &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, his node.js-based collector for Snowplow.&lt;/p&gt;

&lt;p&gt;To date, the primary collector for Snowplow events has been our CloudFront-based collector. The CloudFront-based collector has been easy to setup and very reliable, but has one main drawback: it does not support user tracking across multiple domains.&lt;/p&gt;

&lt;p&gt;The Clojure-based collector changes this: it sets a unique user ID server-side and returns it to the browser as a third-party cookie; this user ID is then stored with your Snowplow events, instead of the first-party cookie set by the JavaScript tracker. This means that user=123 on, say, &lt;a href='http://maven.snplow.com'&gt;maven.snplow.com&lt;/a&gt; will be the same as user=123 on &lt;a href='http://snowplowanalytics.com'&gt;snowplowanalytics.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And the other good news is that our Clojure collector automatically logs the raw Snowplow events to Amazon S3 - and it logs in the exact same format as the CloudFront-based collector, so we can use the same ETL process for both collectors!&lt;/p&gt;

&lt;p&gt;Read on below the fold for installation instructions and some additional information on this release.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='installation_instructions'&gt;Installation instructions&lt;/h2&gt;

&lt;h3 id='clojurebased_collector'&gt;Clojure-based collector&lt;/h3&gt;

&lt;p&gt;You will find full instructions on setting up the new Clojure-based collector on our Wiki, &lt;a href='https://github.com/snowplow/snowplow/wiki/setting-up-the-clojure-collector'&gt;Setting up the Clojure collector&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update to the latest version, which is 0.0.7 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You will also need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  # ...
  :hive_hiveql_version: 0.5.4
  :non_hive_hiveql_version: 0.0.5&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;p&gt;If you are using StorageLoader, you need to update to the latest version, which is 0.0.3 - this is available by checking out the master branch of the &lt;a href='https://github.com/snowplow/snowplow'&gt;Snowplow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition, you will need to update your table definition. This is because the &lt;code&gt;user_id&lt;/code&gt; field was not wide enough to store the new user IDs (UUIDs) set by the Clojure collector. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_005.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_005&lt;/code&gt; (version 0.0.5 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events_004&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_005&lt;/code&gt; table, not your old &lt;code&gt;events_004&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  # ...
  :table:    events_005 # NOT &amp;quot;events_004&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! Your Clojure collector should be ready to run now. However, please read on for an important note about its experimental nature.&lt;/p&gt;

&lt;h2 id='warning_experimental'&gt;Warning: Experimental!&lt;/h2&gt;

&lt;p&gt;We want to stress that the new Clojure-based collector is a piece of experimental technology - we are looking to the community to try it out and feedback to us on how it&amp;#8217;s working for you, especially at scale.&lt;/p&gt;

&lt;p&gt;In particular, we would recommend running the Clojure-based collector alongside the CloudFront collector to be confident that it is performing under load and that no events are being dropped. We have run both collectors alongside each other for the &lt;a href='http://snowplowanalytics.com'&gt;Snowplow Analytics&lt;/a&gt; website for four complete days, and total event counts are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;CloudFront&lt;/th&gt;&lt;th&gt;Clojure&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-02&lt;/td&gt;&lt;td style='text-align: left;'&gt;275&lt;/td&gt;&lt;td style='text-align: left;'&gt;274&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2013-01-01&lt;/td&gt;&lt;td style='text-align: left;'&gt;116&lt;/td&gt;&lt;td style='text-align: left;'&gt;108&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-31&lt;/td&gt;&lt;td style='text-align: left;'&gt;107&lt;/td&gt;&lt;td style='text-align: left;'&gt;109&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;2012-12-30&lt;/td&gt;&lt;td style='text-align: left;'&gt;142&lt;/td&gt;&lt;td style='text-align: left;'&gt;141&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Overall for the result set, the absolute percentage difference between results for the Cloudfront and Clojure collectors is less than 2% (1.9%). Possible reasons for this discrepancy include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Differences in datestamps - possibly an event fell on either side of a date boundary for each collector&lt;/li&gt;

&lt;li&gt;Duplicate rows - the two collectors may be occassionally duplicating different rows (see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;Browsing behavior - it may be that the user navigates away from the page before one or other collector can register the event&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We plan on testing all of this further with larger datasets; we also intend to explore the Clojure collector&amp;#8217;s duplicate rows to check there are no particular issues there.&lt;/p&gt;

&lt;h2 id='other_features_in_this_release'&gt;Other features in this release&lt;/h2&gt;

&lt;p&gt;There are two minor changes in this release not related to the Clojure-based collector:&lt;/p&gt;

&lt;p&gt;Both EmrEtlRunner and StorageLoader now print &amp;#8220;Completed successfully&amp;#8221; to &lt;code&gt;stdout&lt;/code&gt; on completion. This should help to make it clearer (e.g. in logs) that these Ruby programs have completed successfully.&lt;/p&gt;

&lt;p&gt;StorageLoader has been updated so that its &lt;code&gt;--skip&lt;/code&gt; argument works the same way as it does in EmrEtlRunner:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Specific options:
    ...
    -s, --skip download,load,archive   skip work step(s)&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.7.0, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2013/01/02/referer-parser-ported-to-3-more-languages"/>
    <title>referer-parser now with Java, Scala and Python support</title>
    <updated>2013-01-02T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Happy New Year all! It&amp;#8217;s been three months since we &lt;a href='/blog/2012/10/11/attlib-0.0.1-released/'&gt;introduced our Attlib project&lt;/a&gt;, now renamed to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, and we are pleased to announce that referer-parser is now available in three additional languages: Java, Scala and Python.&lt;/p&gt;

&lt;p&gt;To recap: referer-parser is a simple library for extracting seach marketing attribution data from referer &lt;em&gt;(sic)&lt;/em&gt; URLs. You supply referer-parser with a referer URL; it then tells you whether the URL is from a search engine - and if so, which search engine it is, and what keywords the user supplied to arrive at your page.&lt;/p&gt;

&lt;p&gt;Huge thanks to &lt;a href='https://github.com/donspaulding'&gt;Don Spaulding&lt;/a&gt; @ &lt;a href='http://mirusresearch.com/'&gt;Mirus Research&lt;/a&gt; for contributing the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/python'&gt;Python port&lt;/a&gt; of referer-parser; the &lt;a href='https://github.com/snowplow/referer-parser/tree/master/java-scala'&gt;Java/Scala port&lt;/a&gt; was developed by us in-house and it will be a key addition to our &lt;a href='https://github.com/snowplow/snowplow/wiki/etl'&gt;Snowplow ETL&lt;/a&gt; process in the coming months.&lt;/p&gt;

&lt;p&gt;You can checkout the code on GitHub, in the &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser repository&lt;/a&gt;, or read on below the fold for some code examples in the new languages:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='python'&gt;Python&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Python script:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='python'&gt;&lt;span class='kn'&gt;from&lt;/span&gt; &lt;span class='nn'&gt;referer_parser&lt;/span&gt; &lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;

&lt;span class='n'&gt;referer_url&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;#39;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;Referer&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;referer_url&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;known&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;              &lt;span class='c'&gt;# True&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;            &lt;span class='c'&gt;# &amp;#39;Google&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_parameter&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;   &lt;span class='c'&gt;# &amp;#39;q&amp;#39;     &lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_term&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;        &lt;span class='c'&gt;# &amp;#39;gateway oracle cards denise linn&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;print&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;uri&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;                &lt;span class='c'&gt;# ParseResult(scheme=&amp;#39;http&amp;#39;, netloc=&amp;#39;www.google.com&amp;#39;, path=&amp;#39;/search&amp;#39;, params=&amp;#39;&amp;#39;, query=&amp;#39;q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;#39;, fragment=&amp;#39;&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Python &lt;a href='https://github.com/snowplow/referer-parser/blob/master/python/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='scala'&gt;Scala&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Scala app:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='scala'&gt;&lt;span class='k'&gt;val&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='k'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;

&lt;span class='k'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.scala.Parser&lt;/span&gt;
&lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='nc'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;))&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
  &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;name&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;      &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='k'&gt;for&lt;/span&gt; &lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt; &lt;span class='k'&gt;&amp;lt;-&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt; &lt;span class='o'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;term&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;            &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
    &lt;span class='n'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;s&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;)&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='o'&gt;}&lt;/span&gt;
&lt;span class='o'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='usage_java'&gt;Usage: Java&lt;/h2&gt;

&lt;p&gt;To use referer-parser from a Java program:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='java'&gt;&lt;span class='kn'&gt;import&lt;/span&gt; &lt;span class='nn'&gt;com.snowplowanalytics.refererparser.Parser&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

&lt;span class='o'&gt;...&lt;/span&gt;

  &lt;span class='n'&gt;String&lt;/span&gt; &lt;span class='n'&gt;refererUrl&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s'&gt;&amp;quot;http://www.google.com/search?q=gateway+oracle+cards+denise+linn&amp;amp;hl=en&amp;amp;client=safari&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;;&lt;/span&gt;

  &lt;span class='n'&gt;Parser&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='k'&gt;new&lt;/span&gt; &lt;span class='n'&gt;Parser&lt;/span&gt;&lt;span class='o'&gt;();&lt;/span&gt;
  &lt;span class='n'&gt;Referal&lt;/span&gt; &lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;refererParser&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parse&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;refererUrl&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;

  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;referer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;name&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;       &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;Google&amp;quot;&lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;parameter&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;   &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;q&amp;quot;    &lt;/span&gt;
  &lt;span class='n'&gt;System&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;out&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;println&lt;/span&gt;&lt;span class='o'&gt;(&lt;/span&gt;&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;search&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='na'&gt;term&lt;/span&gt;&lt;span class='o'&gt;);&lt;/span&gt;        &lt;span class='c1'&gt;// =&amp;gt; &amp;quot;gateway oracle cards denise linn&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more information, please see the Java/Scala &lt;a href='https://github.com/snowplow/referer-parser/blob/master/java-scala/README.md'&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with the new versions of referer-parser, please &lt;a href='https://github.com/snowplow/referer-parser/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if you find referer-parser useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/26/snowplow-0.6.5-released"/>
    <title>Snowplow 0.6.5 released, with improved event tracking</title>
    <updated>2012-12-26T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re excited to announce our next Snowplow release - version &lt;strong&gt;0.6.5&lt;/strong&gt;, a Boxing Day release for Snowplow!&lt;/p&gt;

&lt;p&gt;This is a big release for us, as it introduces the idea of &lt;strong&gt;event types&lt;/strong&gt; - every event sent by the JavaScript tracker to the collector now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. This should be really helpful for a couple of things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It should make querying Snowplow events much easier&lt;/li&gt;

&lt;li&gt;It should make Snowplow event data a better fit for JSON-oriented datastores such as MongoDB and Riak&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As well as event types, in this release we are also introducing &lt;strong&gt;event IDs&lt;/strong&gt;. With this, the ETL phase adds an &lt;code&gt;event_id&lt;/code&gt; UUID (universally unique ID) to each event row, which should help with subsequent querying.&lt;/p&gt;

&lt;p&gt;Here is a taster of how Snowplow event data looks with the new event types and event IDs:&lt;/p&gt;

&lt;p&gt;&lt;img alt='events-screenshot' src='/static/img/blog/2012/event_and_event_id_fields.png' /&gt;&lt;/p&gt;

&lt;p&gt;These are not the only improvements in this version - here are the rest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We have cleaned up the code for on-page activity tracking (&amp;#8220;page pings&amp;#8221;)&lt;/li&gt;

&lt;li&gt;We have fixed a bug that affected ad impression tracking - thanks &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt;!&lt;/li&gt;

&lt;li&gt;The ETL no longer dies if a raw event has a corrupted querystring (e.g. the &lt;code&gt;&amp;amp;refr=&lt;/code&gt; parameter was not escaped)&lt;/li&gt;

&lt;li&gt;The JavaScript tracker&amp;#8217;s build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, now has a combine-only option (no minification), which is helpful for testing purposes&lt;/li&gt;

&lt;li&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter&lt;/li&gt;

&lt;li&gt;We have added the ability to override the IP address by passing in an &lt;code&gt;&amp;amp;ip=&lt;/code&gt; parameter on the querystring&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, we first explain how to upgrade, before taking a brief tour through these updates:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='0_upgrading'&gt;0. Upgrading&lt;/h2&gt;

&lt;p&gt;Upgrading is a two-step process:&lt;/p&gt;

&lt;h3 id='javascript_tracker'&gt;JavaScript tracker&lt;/h3&gt;

&lt;p&gt;Please update your website(s) to use the latest version of the JavaScript tracker, which is version 0.9.0. As always, the updated minified tracker is available here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.9.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;If you are using EmrEtlRunner, you need to update your configuration file, &lt;code&gt;config.yml&lt;/code&gt;, to use the latest versions of the Hive serde and HiveQL scripts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.3
  :hive_hiveql_version: 0.5.3
  :non_hive_hiveql_version: 0.0.4&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;#8217;s it! You don&amp;#8217;t need to make any changes to your Infobright setup, assuming you are up-to-date with previous releases.&lt;/p&gt;

&lt;h2 id='1_event_types'&gt;1. Event types&lt;/h2&gt;

&lt;p&gt;To recap, every event sent by the JavaScript tracker now has an &lt;code&gt;event&lt;/code&gt; field which specifies what type of event it is. Currently we have six different types of events, which are set out in the table below:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of event&lt;/th&gt;&lt;th&gt;JavaScript tracker function&lt;/th&gt;&lt;th&gt;Value of Snowplow &lt;code&gt;event&lt;/code&gt; field&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page view&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackPageView()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_view&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Page ping&lt;/td&gt;&lt;td style='text-align: left;'&gt;None (automatic)*&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;page_ping&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Custom event&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackEvent()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;custom&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Ad impression&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;trackImpression()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;ad_impression&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addTrans&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Transaction item&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;addItem&lt;/code&gt; &amp;amp; &lt;code&gt;trackTrans()&lt;/code&gt;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&lt;code&gt;transaction_item&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;em&gt;* for more information on on-page activity tracking, please see the relevant section later in this blog post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This new event field should make it much easier to query Snowplow data by the type of event. For example, to retrieve the number of e-commerce transactions per day:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;transaction&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use the &lt;code&gt;event&lt;/code&gt; field to simplify queries where possible.&lt;/p&gt;

&lt;h2 id='2_event_ids'&gt;2. Event IDs&lt;/h2&gt;

&lt;p&gt;As stated above, the Snowplow ETL now attaches a unique ID to each event - specifically a &lt;a href='http://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_.28random.29'&gt;type 4 UUID&lt;/a&gt;. This new &lt;code&gt;event_id&lt;/code&gt; is much more unique than the existing &lt;code&gt;txn_id&lt;/code&gt; field, which is a short random number set in the JavaScript tracker (&lt;code&gt;txn_id&lt;/code&gt; is currently unused, but we may eventually use it to check for duplicate events &lt;code&gt;txn_id&lt;/code&gt; introduced prior to the ETL, see &lt;a href='https://github.com/snowplow/snowplow/issues/24'&gt;issue 24&lt;/a&gt; for more details).&lt;/p&gt;

&lt;p&gt;You can use the new &lt;code&gt;event_id&lt;/code&gt; field to uniquely identify individual events in your event store, and of course to count distinct events. For example, to count the number of page views by day, we simply execute the following query:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='sql'&gt;&lt;span class='k'&gt;SELECT&lt;/span&gt;
&lt;span class='n'&gt;dt&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
&lt;span class='k'&gt;COUNT&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;event_id&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='k'&gt;FROM&lt;/span&gt; &lt;span class='n'&gt;events&lt;/span&gt;
&lt;span class='k'&gt;WHERE&lt;/span&gt; &lt;span class='n'&gt;event&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;page_view&amp;#39;&lt;/span&gt;
&lt;span class='k'&gt;GROUP&lt;/span&gt; &lt;span class='k'&gt;BY&lt;/span&gt; &lt;span class='n'&gt;dt&lt;/span&gt; &lt;span class='p'&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We will be updating our &lt;a href='/analytics/index.html'&gt;Analytics Cookbook&lt;/a&gt; to use &lt;code&gt;event_id&lt;/code&gt; in any examples which currently (erroneously) use &lt;code&gt;txn_id&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='3_onpage_activity_tracking'&gt;3. On-page activity tracking&lt;/h2&gt;

&lt;p&gt;In this release of the JavaScript tracker we have deprecated the old (undocumented) &lt;code&gt;setHeartBeatTimer()&lt;/code&gt; inherited from &lt;code&gt;piwik.js&lt;/code&gt;, and introduced a new function, &lt;code&gt;enableActivityTracking(minimumVisitLength, heartBeatDelay)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With activity tracking enabled, &amp;#8220;page pings&amp;#8221; are sent to Snowplow every &lt;code&gt;heartBeatDelay&lt;/code&gt; seconds, as long as the visitor remains active (moving the mouse, clicking etc) on the page. Page pings are not sent until the &lt;code&gt;minimumVisitLength&lt;/code&gt; seconds have elapsed.&lt;/p&gt;

&lt;p&gt;Here is an example configuration:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;enableActivityTracking&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Ping every 10 seconds after 10 seconds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is still an experimental feature - but it should provide some interesting data to start to explore page residency, true bounce rates and so on.&lt;/p&gt;

&lt;p&gt;Please note that enabling activity tracking can &lt;strong&gt;significantly&lt;/strong&gt; increase the number of Snowplow events generated, especially with a short &lt;code&gt;heartBeatDelay&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id='4_and_the_rest'&gt;4. And the rest&lt;/h2&gt;

&lt;p&gt;The rest of the changes in this release are much smaller, being either bug fixes or small preparatory features for future releases:&lt;/p&gt;

&lt;h3 id='ad_impression_tracking_bug_fix'&gt;Ad impression tracking bug fix&lt;/h3&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/talkspoon'&gt;Alan Z&lt;/a&gt; @ &lt;a href='http://www.verycd.com'&gt;VeryCD&lt;/a&gt; for spotting a bug in the &lt;code&gt;trackImpression()&lt;/code&gt; method, which was stopping ad impressions from being logged. This is now fixed.&lt;/p&gt;

&lt;h3 id='etl_resilient_against_corrupted_querystrings'&gt;ETL resilient against corrupted querystrings&lt;/h3&gt;

&lt;p&gt;We had a problem with two historic versions of the JavaScript tracker, 0.8.0 and 0.8.1, where querystrings were being transmitted to the Snowplow collector unescaped. These &amp;#8220;corrupted&amp;#8221; querystrings caused the ETL process to error and die.&lt;/p&gt;

&lt;p&gt;We have updated the ETL process so that events with corrupted querystrings can be processed without error: these rows are stored as Snowplow events, but of course with most of the standard fields empty.&lt;/p&gt;

&lt;h3 id='snowpaksh_combineonly_option'&gt;snowpak.sh combine-only option&lt;/h3&gt;

&lt;p&gt;To make it easier to hack on the JavaScript tracker, we have updated the build script, &lt;code&gt;snowpak.sh&lt;/code&gt;, so that it has a &amp;#8220;combine-only&amp;#8221; option. If you set the combine-only flag on the command line, then the minification step will &lt;strong&gt;not&lt;/strong&gt; be run, and debug code will left in. This is helpful for local testing when you want to debug the JavaScript in its pre-minified form.&lt;/p&gt;

&lt;p&gt;Here are the updated usage options for the &lt;code&gt;snowpak.sh&lt;/code&gt; script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage: ./snowpak.sh [options]

Specific options:
  -y PATH             path to YUICompressor 2.4.2 *
  -c                  combine only (no minification or removing debug)

* or set env variable YUI_COMPRESSOR_PATH instead

Common options:
  -h                  Show this message&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='attachuseridboolean'&gt;attachUserId(boolean)&lt;/h3&gt;

&lt;p&gt;The JavaScript tracker has a new method, &lt;code&gt;attachUserId(boolean)&lt;/code&gt;, which can be used to stop the tracker from sending a &lt;code&gt;&amp;amp;uid=xxx&lt;/code&gt; parameter. By default, the JavaScript tracker sends the user ID to the collector via this &lt;code&gt;uid&lt;/code&gt; parameter; you can now disable this by calling &lt;code&gt;attachUserId()&lt;/code&gt; like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;attachUserId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kc'&gt;false&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt; &lt;span class='c1'&gt;// Don&amp;#39;t attach &amp;amp;uid=xxx to the querystring&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This function is not of immediate use - but it will be an important part of the setup for using the new Clojure Collector, which we are currently working on.&lt;/p&gt;

&lt;h3 id='ip_address_override'&gt;IP address override&lt;/h3&gt;

&lt;p&gt;Finally, we have added the ability to override the IP address by passing in an &lt;code&gt;ip=&lt;/code&gt; parameter on the querystring.&lt;/p&gt;

&lt;p&gt;This one is a little confusing, as there is no capability in the JavaScript tracker to attach an IP address to the querystring (because JavaScript cannot know the user&amp;#8217;s IP address). Rather, we have added this IP address override ability to cater for future server-side trackers and collectors which &lt;em&gt;do&lt;/em&gt; know the user&amp;#8217;s IP address and want to append it to the querystring themselves.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.6.5, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/20/snowplow-0.6.4-released"/>
    <title>Snowplow 0.6.4 released, with Infobright improvements</title>
    <updated>2012-12-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next Snowplow release - version &lt;strong&gt;0.6.4&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An upgraded Infobright table definition which scales to millions of pageviews easily&lt;/li&gt;

&lt;li&gt;Clarified Hive table definitions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; worked closely with us on the updated Infobright table definition&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt; for flagging the missing Hive table definition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both updates below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='upgraded_infobright_table_definition'&gt;Upgraded Infobright table definition&lt;/h2&gt;

&lt;p&gt;With help from &lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; we have upgraded the Infobright table definition so that it can easily scale to loading millions of new Snowplow events per day. It also supports much longer &lt;code&gt;br_lang&lt;/code&gt; and &lt;code&gt;page_url&lt;/code&gt; fields, which should prevent you from occasional load errors.&lt;/p&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_004.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_004&lt;/code&gt; (version 0.0.4 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_004&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_004 # NOT &amp;quot;events_003&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='clarified_hive_table_definitions'&gt;Clarified Hive table definitions&lt;/h2&gt;

&lt;p&gt;We have clarified the two different Hive table definitions, available in this folder:&lt;/p&gt;

&lt;p&gt;4-storage/hive-storage&lt;/p&gt;

&lt;p&gt;Which format your Snowplow event files are in will depend on how your EmrEtlRunner is configured. If your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your Snowplow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/non-hive-format-table-def.q'&gt;&lt;code&gt;non-hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whereas if your &lt;code&gt;config.yml&lt;/code&gt; contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then your Snowplow events will be stored in the format shown in &lt;a href='https://github.com/snowplow/snowplow/blob/master/4-storage/hive-storage/hive-format-table-def.q'&gt;&lt;code&gt;hive-format-table-def.q&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;That&amp;#8217;s it! If you have any problems with Snowplow version 0.6.4, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/18/snowplow-0.6.3-released"/>
    <title>Snowplow 0.6.3 released, with JavaScript and HiveQL bug fixes</title>
    <updated>2012-12-18T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing Snowplow version &lt;strong&gt;0.6.3&lt;/strong&gt; - another clean-up release following on from the 0.6.2 release. This release bumps the JavaScript Tracker to version 0.8.2, and the Hive-data-format HiveQL file to version 0.5.2.&lt;/p&gt;

&lt;p&gt;Many thanks to the community members who contributed bug fixes to this release: &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; @ &lt;a href='http://meltmedia.com/'&gt;meltmedia&lt;/a&gt;, &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; and &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both fixes below:&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_fixes'&gt;JavaScript tracker fixes&lt;/h2&gt;

&lt;p&gt;This release fixes the issues in the JavaScript tracker raised in &lt;a href='https://github.com/snowplow/snowplow/issues/103'&gt;issue 103&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These issues stemmed from the splitting of the JavaScript into multiple files in Snowplow version 0.6.1 (JavaScript tracker version 0.8.0). We do not believe these bugs affected Snowplow data collection, but they are well worth fixing.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for their help in squashing these bugs!&lt;/p&gt;

&lt;p&gt;With these fixes we have bumped the JavaScript Tracker to version 0.8.2; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.2/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='hiveql_script_fix'&gt;HiveQL script fix&lt;/h2&gt;

&lt;p&gt;This release also fixes a bug (&lt;a href='https://github.com/snowplow/snowplow/pull/112'&gt;issue 112&lt;/a&gt;) in the HiveQL file used to generate the &lt;strong&gt;Hive-format&lt;/strong&gt; Snowplow event files.&lt;/p&gt;

&lt;p&gt;This bug prevented the ETL from running if you were using EmrEtlRunner to generate Hive-format Snowplow event files - in other words if you had set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mmoulton'&gt;Mike Moulton&lt;/a&gt; for spotting and fixing this one!&lt;/p&gt;

&lt;p&gt;With this fix we have bumped the Hive-format HiveQL file to version 0.5.2. To start using the new file, all you need to do is update your EmrEtlRunner&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; file by changing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.1&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:hive_hiveql_version: 0.5.2&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with Snowplow version 0.6.3, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/12/17/transforming-snowplow-data-so-it-can-be-interrogated-by-olap-tools-like-tableau"/>
    <title>Transforming Snowplow data so that it can be interrogataed in BI / OLAP tools like Tableau, Qlikview and Pentaho</title>
    <updated>2012-12-17T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Because Snowplow does not ship with any sort of user interface, we get many enquiries from current and prospective users who would like to interrogate Snowplow data with popular BI tools like Tableau or Qlikview.&lt;/p&gt;
&lt;p style='text-align:center;'&gt;&lt;img alt='olap cube' src='/static/img/olap/example-cube-2.png' width='250' /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, it is not possible to run a tool like Tableau directly on top of the Snowplow events table. That is because these tools require the data to be in a particular format: one in which each line of data is made up of a combination of dimension and metrics fields, such that it is straightforward for the reporting tool to understand how to aggregate metrics by different combinations of dimensions. To give a very simple example of a data set that would play nicely with a reporting tool like Tableau:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Country&lt;/th&gt;&lt;th&gt;Date&lt;/th&gt;&lt;th&gt;Product&lt;/th&gt;&lt;th&gt;Number Sold&lt;/th&gt;&lt;th&gt;Revenue&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Sept 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;137&lt;/td&gt;&lt;td style='text-align: right;'&gt;1779.63&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;193&lt;/td&gt;&lt;td style='text-align: right;'&gt;2507.07&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;UK&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Shoes&lt;/td&gt;&lt;td style='text-align: left;'&gt;15&lt;/td&gt;&lt;td style='text-align: right;'&gt;1125.00&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;France&lt;/td&gt;&lt;td style='text-align: left;'&gt;Oct 2012&lt;/td&gt;&lt;td style='text-align: left;'&gt;Hats&lt;/td&gt;&lt;td style='text-align: left;'&gt;288&lt;/td&gt;&lt;td style='text-align: right;'&gt;2877.12&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: left;'&gt;&amp;#8230;&lt;/td&gt;&lt;td style='text-align: right;'&gt;&amp;#8230;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Reporting tools like Tableau recognise that the two right hand columns (number sold and revenue) are metrics, and that analysts will want to examine how those metrics vary by country, time and product (all of which are dimensions). They will give analysts easy-to-use tools to enable them to slice, dice, drill up and drill down those metrics by different combinations of those dimensions. Enabling those operations is straightforward for the reporting tool, because it knows it knows if the analyst wants to report product sales in the UK over time, to filter all results by country (so only lines of data from the UK are included), and display sales of each type of product by month.&lt;/p&gt;

&lt;p&gt;This type of dimensional analysis is called &lt;a href='http://en.wikipedia.org/wiki/Online_analytical_processing'&gt;OLAP&lt;/a&gt;, and has a long and venerable history in business intelligence. Although the term &amp;#8216;OLAP&amp;#8217; is no longer fashionable, this type of analysis is still the predominant one used by anyone who relies on PivotTables in Excel or any mainstream BI tool including Tableau, Qlikview, Microstrategy, Pentaho etc.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Snowplow data is in a log file format. Whilst each line does include metrics (e.g. revenue) and dimensions (e.g. browser features or operating system details), there are a large number of dimensions that we might want to slice and dice the data on that are not included in each line: these data points have to be inferred by reading across several lines of data. To give just one example: the source of traffic i.e. &lt;code&gt;mkt_source&lt;/code&gt; for a particular visit is only given on the &lt;strong&gt;first line&lt;/strong&gt; of data for that visit. Hence, in order to enable users to analyse page views, customer lifetime value and other metrics by different marketing sources (e.g. to do attribution analysis), we need to work out which source of traffic to attribute that visit to, and label every line of data associated with that visit with that source of traffic.&lt;/p&gt;

&lt;p&gt;So in order to use a BI tool like Tableau or Qlikview to interrogate Snowplow data, you need to transform the data first. We&amp;#8217;ve documented how to perform the transformation in the &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;analytics cookbook&lt;/a&gt;. We hope it provides a useful guide for anyone interested in interrogating or visualising Snowplow data using BI tools.&lt;/p&gt;

&lt;p&gt;We also wonder whether the documentation will be of interest to the wider community of analysts and data scientists interested in using tools like Tableau to query log data. One of the things that surprised us, going in to this exercise, was the lack of material we could find on the internet that covered transforming log data so that it could be analysed using BI tools. We expected this to be well-covered territory: especially given the fact that the vast majority of data processed by Hadoop is log files, and nearly all the BI vendors are working hard to integrate their products with Hadoop. (For example, Tableau now integrates with MapR Hadoop distributions, so you can analyse Hive tables directly in Tableau.) Without the transformation step covered in our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; however, this type of integration is useless.&lt;/p&gt;

&lt;p&gt;If we&amp;#8217;ve missed any online literature on the topic, or if there are other people who&amp;#8217;ve looked at this particular problem and come up with different approaches - we&amp;#8217;d love to debate them here. Get in touch! For everyone else, we hope you find our &lt;a href='/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html'&gt;guide&lt;/a&gt; helpful&amp;#8230;&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Snowplow data &lt;a href='https://github.com/snowplow/snowplow/wiki/canonical-event-model'&gt;canonical data structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/28/snowplow-0.6.2-released"/>
    <title>Snowplow 0.6.2 released, with JavaScript tracker bug fixes</title>
    <updated>2012-11-28T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today we are releasing Snowplow version &lt;strong&gt;0.6.2&lt;/strong&gt; - a clean-up release after yesterday&amp;#8217;s 0.6.1 release. This release bumps the JavaScript Tracker to version 0.8.1; the updated minified tracker is available as always here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.1/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This release fixes two bugs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/pull/101'&gt;Issue #101&lt;/a&gt; - we had left in a &lt;code&gt;console.log()&lt;/code&gt; in the production version, which should only have been printed in debug mode. Harmless but worth taking out. Many thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting this so quickly and fixing&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/102'&gt;Issue #102&lt;/a&gt; - there was a trailing space in our initialization code, &lt;a href='https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/init.js'&gt;&lt;code&gt;init.js&lt;/code&gt;&lt;/a&gt;, which could cause some issues in Internet Explorer. Many thanks to community member Alan Z for raising&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The JavaScript Tracker&amp;#8217;s API and the Tracker Protocol are unchanged. We recommend upgrading to using the new JavaScript Tracker version 0.8.1 over the previous 0.8.0.&lt;/p&gt;

&lt;p&gt;Finally, if you haven&amp;#8217;t yet read our tutorial on &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating-javascript-tags-with-Google-Tag-Manager'&gt;Integrating the JavaScript Tracker with Google Tag Manager&lt;/a&gt;, we would recommend taking a look - it makes these sorts of upgrades much easier.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/27/snowplow-0.6.1-released"/>
    <title>Snowplow 0.6.1 released, with lots of small improvements</title>
    <updated>2012-11-27T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re happy to announce our next Snowplow release - version &lt;strong&gt;0.6.1&lt;/strong&gt;. This release includes updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Additional data collection&lt;/strong&gt;. The Javascript tracker has been updated to capture additional data points, including a user fingerprint (which can be used as a &lt;code&gt;user_id&lt;/code&gt; for companies tracking users across domains), the tracker version, browser timezone and color depth&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Javascript tracker updates&lt;/strong&gt;. A number of updates have been made to make the Javascript tracker more robust&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Updates to the ETL flow&lt;/strong&gt; so that the &lt;code&gt;user_agent&lt;/code&gt; string and &lt;code&gt;platform&lt;/code&gt; captured and stored in Hive / Infobright&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Improved EmrEtlRunner command line options&lt;/strong&gt; now provide more flexibility when writing your data to storage&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; related to loading Snowplow data into Infobright&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Before we start - a big thanks to the community members who helped out on this release in a big way:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; @ &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; substantially re-factored the JavaScript tracker, splitting it into multiple smaller files, which made our work significantly easier :-)&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/moncaubeig'&gt;Gilles Moncaubeig&lt;/a&gt; @ &lt;a href='http://en.overblog.com/'&gt;OverBlog&lt;/a&gt; contributed the user fingerprinting code - thanks Gilles!&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; @ &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; continued his great work on EmrEtlRunner with improved command line options&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h2 id='javascript_tracker_updates'&gt;JavaScript tracker updates&lt;/h2&gt;

&lt;p&gt;We have released a new version of the JavaScript tracker, &lt;strong&gt;0.8.0&lt;/strong&gt;. As always, we are hosting this new version on CloudFront if you don&amp;#8217;t want to host it yourself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http(s)://d1fc8wv8zag5ca.cloudfront.net/0.8.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But before you update your tags, we need to share a few important things about this new version, including a &lt;strong&gt;breaking change&lt;/strong&gt;:&lt;/p&gt;

&lt;h3 id='changes_to_the_javascript_api'&gt;Changes to the JavaScript API&lt;/h3&gt;

&lt;p&gt;Three main changes have been made to the JavaScript tracker&amp;#8217;s API - please note that the first is a &lt;strong&gt;breaking change&lt;/strong&gt;, while the other two deprecate some existing functions (and introduce new ones):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The tracker now GETs &lt;code&gt;i&lt;/code&gt; not &lt;code&gt;ice.png&lt;/code&gt; &lt;strong&gt;(BREAKING CHANGE)&lt;/strong&gt; - version 0.8.0 of the JavaScript tracker now GETs a transparent 1x1 GIF called &lt;code&gt;i&lt;/code&gt;, no longer &lt;code&gt;ice.png&lt;/code&gt;. If you are using the CloudFront Collector, you &lt;strong&gt;must&lt;/strong&gt; upload our &lt;a href='https://github.com/snowplow/snowplow/blob/master/2-collectors/cloudfront-collector/static/i?raw=true'&gt;&lt;code&gt;i&lt;/code&gt;&lt;/a&gt; pixel to sit in your S3 bucket alongside &lt;code&gt;ice.png&lt;/code&gt;. Don&amp;#8217;t forget to make it publically readable. &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, already supports &lt;code&gt;i&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;setAccount()&lt;/code&gt; function was badly named. We have added a new function, &lt;code&gt;setCollectorCf()&lt;/code&gt;, which does the exact same thing, and we will remove &lt;code&gt;setAccount()&lt;/code&gt; in a future version. If you continue to use &lt;code&gt;setAccount()&lt;/code&gt;, then a warning message will be printed to &lt;code&gt;console.log&lt;/code&gt;, but it will still work&lt;/li&gt;

&lt;li&gt;The &lt;code&gt;getTracker()&lt;/code&gt; function (which not many people need to use) was badly named. As wth point 2 above: we have added &lt;code&gt;getTrackerCf()&lt;/code&gt; and &lt;code&gt;getTrackerUrl()&lt;/code&gt;, and deprecated &lt;code&gt;getTracker()&lt;/code&gt; for now&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As always, our JavaScript tracker&amp;#8217;s current API is fully documented on our Wiki, on the &lt;a href='https://github.com/snowplow/snowplow/wiki/javascript-tracker'&gt;JavaScript Tracker&lt;/a&gt; page.&lt;/p&gt;

&lt;h3 id='new_tracking_data'&gt;New tracking data&lt;/h3&gt;

&lt;p&gt;Version 0.8.0 of the JavaScript tracker now passes additional data along to the Snowplow collector. This additional data is as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tracker version&lt;/strong&gt; - so you will always know which version of the JavaScript tracker generated a given Snowplow event&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;User fingerprint&lt;/strong&gt; - we are still working on a new collector which supports cross-domain user tracking. In the meantime, we are releasing an experimental feature: a &amp;#8216;user fingerprint&amp;#8217; based on various (hopefully unique) attributes of the user&amp;#8217;s browser. Many thanks to &lt;a href='https://github.com/moncaubeig'&gt;Gilles&lt;/a&gt; for contributing this feature; to read more about this, please take a look at &lt;a href='https://github.com/snowplow/snowplow/issues/70'&gt;issue #70&lt;/a&gt; in GitHub&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Timezone&lt;/strong&gt; - tells you what timezone the user in, recording the &lt;a href='http://en.wikipedia.org/wiki/Tz_database'&gt;Olsen key&lt;/a&gt; for the user&amp;#8217;s timezone&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Color depth&lt;/strong&gt; - the bit depth of the browser&amp;#8217;s color palette for displaying images (in bits per pixel)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have updated the ETL and storage systems (e.g. Hive and Infobright table definitions) to extract and store these new fields.&lt;/p&gt;

&lt;p&gt;We have updated the &lt;a href='https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol'&gt;Snowplow Tracker Protocol&lt;/a&gt; page on our Wiki with these additions.&lt;/p&gt;

&lt;h2 id='etl_and_storage_improvements'&gt;ETL and storage improvements&lt;/h2&gt;

&lt;p&gt;This release includes various improvements to Snowplow ETL and storage which are unrelated to the JavaScript tracker changes above. To break these down:&lt;/p&gt;

&lt;h3 id='additional_data_being_saved'&gt;Additional data being saved&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Useragent&lt;/strong&gt; - the raw browser useragent is now being logged in a new &lt;code&gt;useragent&lt;/code&gt; field. Previously we were throwing this useful data away&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Platform&lt;/strong&gt; - the tracker&amp;#8217;s &lt;code&gt;platform&lt;/code&gt; type is now extracted and stored. The JavaScript tracker always sets this field to &lt;code&gt;web&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='bug_fixes'&gt;Bug fixes&lt;/h3&gt;

&lt;p&gt;Just one bug fix - in the StorageLoader, we changed the field encloser for Infobright to &lt;code&gt;NULL&lt;/code&gt;, where previously it was &lt;code&gt;&amp;#39;&amp;#39;&lt;/code&gt; (two empty quotes). This was to fix &lt;a href='https://github.com/snowplow/snowplow/issues/88'&gt;issue #88&lt;/a&gt;, where Infobright was throwing an error and dying if a field&amp;#8217;s value started with a double-quote.&lt;/p&gt;

&lt;h3 id='improved_emretlrunner_commandline_options'&gt;Improved EmrEtlRunner command-line options&lt;/h3&gt;

&lt;p&gt;EmrEtlRunner now has some improved command-line options:&lt;/p&gt;

&lt;p&gt;Firstly, the &lt;code&gt;--skip&lt;/code&gt; argument now can take a list of individual steps to skip. So for example you could run &lt;strong&gt;only&lt;/strong&gt; the Hive job with the command-line option:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --skip staging,archive --config ./config.yml &lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Secondly, there is now a new option, &lt;code&gt;--process-bucket&lt;/code&gt;. This runs the Hive job only on the contents of the specified bucket. This implies &lt;code&gt;--skip staging,archive&lt;/code&gt; as well. An example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner --process-bucket s3n://my-logs-to-process --config ./config.yml&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Many thanks to &lt;a href='https://github.com/mtibben'&gt;Mike Tibben&lt;/a&gt; for contributing these new options!&lt;/p&gt;

&lt;h3 id='placeholders_for_event_and_event_id'&gt;Placeholders for event and event_id&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Event&lt;/strong&gt; - we have renamed the &lt;code&gt;event_name&lt;/code&gt; field in Infobright to simply &lt;code&gt;event&lt;/code&gt;. This is still a placeholder (it will be populated in a future version of Snowplow)&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event ID&lt;/strong&gt; - there has been some confusion over the uniqueness of the current &lt;code&gt;txn_id&lt;/code&gt; field - see &lt;a href='https://github.com/snowplow/snowplow/issues/89'&gt;issue #89&lt;/a&gt; for the discussion. We plan on adding a properly unique &lt;code&gt;event_id&lt;/code&gt; for each event in the ETL layer; in the meantime we have added the &lt;code&gt;event_id&lt;/code&gt; field in as a placeholder&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='upgrading_to_the_new_version'&gt;Upgrading to the new version&lt;/h2&gt;

&lt;h3 id='tracker_and_collector'&gt;Tracker and collector&lt;/h3&gt;

&lt;p&gt;We have discussed above how to update your JavaScript tracker and CloudFront collector to support this new version 0.6.1. If you are using &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, the node.js collector, you don&amp;#8217;t have to modify it - it already supports the new &lt;code&gt;i&lt;/code&gt; pixel.&lt;/p&gt;

&lt;h3 id='etl'&gt;ETL&lt;/h3&gt;

&lt;p&gt;To upgrade your ETL system, first re-deploy EmrEtlRunner from GitHub as per the &lt;a href='https://github.com/snowplow/snowplow/wiki/deploying-emretlrunner'&gt;EmrEtlRunner Setup Guide&lt;/a&gt;, and then update the ETL dependencies at the bottom of your &lt;code&gt;config.yml&lt;/code&gt; file like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:snowplow:
  :serde_version: 0.5.2
  :hive_hiveql_version: 0.5.1
  :non_hive_hiveql_version: 0.0.3&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id='storage'&gt;Storage&lt;/h3&gt;

&lt;h4 id='hive'&gt;Hive&lt;/h4&gt;

&lt;p&gt;If you are only using Hive for storage and analytics, you do not need to do anything to support this new release - because we add all new fields to the end of the file format, and field renames (like &lt;code&gt;event_name&lt;/code&gt; to &lt;code&gt;event&lt;/code&gt;) don&amp;#8217;t affect Hive on Amazon EMR.&lt;/p&gt;

&lt;h4 id='infobright'&gt;Infobright&lt;/h4&gt;

&lt;p&gt;If you are using Infobright Community Edition for analysis, you will need to update your table definition. This is a little complex, because Infobright does not support in-place table or column renames. To make this easier for you, we have created a script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;4-storage/infobright-storage/migrate_to_003.sh&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this script will create a new table, &lt;code&gt;events_003&lt;/code&gt; (version 0.0.3 of the table definition) in your &lt;code&gt;snowplow&lt;/code&gt; database, copying across all your data from your existing &lt;code&gt;events&lt;/code&gt; table, which will not be modified in any way.&lt;/p&gt;

&lt;p&gt;Once you have run this, don&amp;#8217;t forget to update your StorageLoader&amp;#8217;s &lt;code&gt;config.yml&lt;/code&gt; to load into the new &lt;code&gt;events_003&lt;/code&gt; table, not your old &lt;code&gt;events&lt;/code&gt; table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:storage:
  :type: infobright
  :database: snowplow
  :table:    events_003 # NOT &amp;quot;events&amp;quot; any more&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems with version 0.6.1, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And do let us know if the new features - such as user fingerprinting - are useful!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/16/integrating-snowplow-with-google-tag-manager"/>
    <title>Integrating Snowplow with Google Tag Manager</title>
    <updated>2012-11-16T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;A month and a half ago, Google launched &lt;a href='http://www.google.com/tagmanager/'&gt;Google Tag Manager&lt;/a&gt; (GTM), a free tag management solution. That was a defining moment in tag management history as it will no doubt bring tag management, until now the preserve of big enterprises, into the mainstream.&lt;/p&gt;

&lt;p&gt;&lt;img alt='gtm' src='/static/img/gtm.JPG' /&gt;&lt;/p&gt;

&lt;p&gt;We have spent some time testing how to get Snowplow tags working well with Google Tag Manager, and have documented our recommended approach to setting up Snowplow with GTM on the &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the course of reading the literature on tag management and Google Tag Manager in particular, we were struck by a number of issues and misconceptions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Setting up a Tag Management System is a big and complicated job. Much of the literature (especially around Google Tag Manager), and discussion (especially amongst members of the web analytics community) suggests it is very easy. Whilst Google Tag Manager is a straightforward product to use, the process of setting it up is difficult, because it involves thinking through, in a very rigorous way, exactly what data should be passed between the website and GTM. If this is not done properly, GTM will not have the relevant data to pass on to the tags it manages, including Snowplow.&lt;/li&gt;

&lt;li&gt;Exacerbating the above, there is a lack of detailed literature on how to go about identifying all the data points that should be passed between your website and tag management solution. Nearly all the guides to setting up GTM that we reviewed covered only the most basic of GTM setups, which is just enough to enable page tracking. Clearly, that is not going to be sufficient for anything but the simplest blogs and brochureware sites.&lt;/li&gt;

&lt;li&gt;One of the tag management features most regularly trumpetted by digital and marketing analysts is that it frees them from having to liaise with their webmasters to add new tags and change existing tag setups. Even with a tag management solution in place, however, this may still be necessary if not all the data that the analyst wants passed through to their analytics package is available in the tag maangement system.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a step towards addressing the above issues, we have produced a step-by-step guide to both &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-setup-gtm'&gt;setting up Google Tag Manager&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager#wiki-snowplow-setup'&gt;setting up Snowplow within GTM&lt;/a&gt;. We hope you find it useful. We plan to produce a similar guide to setting up Snowplow within &lt;a href='http://www.opentag.qubitproducts.com/'&gt;Qubit&amp;#8217;s OpenTag solution&lt;/a&gt; in due course.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re very excited by Google&amp;#8217;s launch of Tag Manager, and recommend all new Snowplow users who are not currently using a tag management system integrate one as part of their Snowplow setup. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The exercise that companies need to go through when setting up a tag management platform i.e. thinking through all the data points that they want to pass to their tag management system so that they can be passed on to whatever tags are managed in the system, is the same process they should go through when they setup Snowplow, with a view to enabling the widest possible set of analyses on their web analytics data. So even though that exercise is not easy, it is valuable&lt;/li&gt;

&lt;li&gt;The processing of mapping that data from the structure defined in the tag management system to one which works with Snowplow&amp;#8217;s data structure is the exact reverse of the analysis process that takes Snowplow data and transforms it back into the structure that&amp;#8217;s most natural for the company performing the analysis.&lt;/li&gt;

&lt;li&gt;Once the tag management system has been installed, it becomes easy to upgrade the tags and / or change the configuration. There are a number of improvements we plan to make to our &lt;a href='https://github.com/snowplow/snowplow/tree/master/1-trackers/javascript-tracker'&gt;Javascript tracker&lt;/a&gt;, and having a tag management program will make it easier for companies to take advantage of those upgrades.&lt;/li&gt;
&lt;/ol&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/12/snowplow-0.6.0-released-with-storage-loader"/>
    <title>Snowplow 0.6.0 released, with the new StorageLoader</title>
    <updated>2012-11-12T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re very pleased to start the week by releasing a new version of Snowplow - version &lt;strong&gt;0.6.0&lt;/strong&gt;. This is a big release for us - as it includes the first version of our all-new StorageLoader. The release also includes a small set of tweaks and bug fixes across the existing Snowplow components, but let&amp;#8217;s start by introducing StorageLoader:&lt;/p&gt;

&lt;h2 id='introducing_storageloader'&gt;Introducing StorageLoader&lt;/h2&gt;

&lt;p&gt;Up until now, Snowplow has stored all its data in S3, where it can be queried in Hive. However, our vision with Snowplow has always been to enable to the broadest set of analyses on Snowplow data as possible. That means making it as easy as possible to keep up to date versions of Snowplow data in many different types of database. The StorageLoader is a key component to fulfilling that vision.&lt;/p&gt;

&lt;p&gt;&lt;img alt='snowplow-loader-image' src='/static/img/SnowplowLoader.jpg' /&gt;&lt;/p&gt;

&lt;p&gt;StorageLoader is a Ruby application that downloads Snowplow event files from S3 and loads them into an alternative database. It has been built to make keeping an up to date version of your Snowplow data in other databases as easy as possible. Currently, it only supports loading the data into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition (ICE)&lt;/a&gt; - a high-performance columnar database based on MySQL. However, we plan to extend it over the next few months to support a range of other databases including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://developers.google.com/bigquery'&gt;Google Big Query&lt;/a&gt; for fast analysis of massive data sets. (This could be very powerful for rapid analytics across Snowplow&amp;#8217;s granular data)&lt;/li&gt;

&lt;li&gt;&lt;a href='http://skydb.io'&gt;SkyDB&lt;/a&gt; for event path analysis and other, broader types of event stream analytics&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.postgresql.org'&gt;PostgreSQL&lt;/a&gt; for web analytics for web properties where the levels of traffic are not Facebook-scale&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;There are significant advantages to storing data in Infobright instead of (or as well as) S3:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In many cases, query times are much faster&lt;/li&gt;

&lt;li&gt;There are a wide range of analytics tools that plug directly into Infobright. (Any tool that plugs into MySQL.) These can now be run directly on top of Snowplow data. (These tools include R and Tableau.)&lt;/li&gt;

&lt;li&gt;For more details on the pros and cons of storage in S3 vs Infobright, see our &lt;a href='https://github.com/snowplow/snowplow/wiki/choosing-a-storage-module'&gt;guide to choosing between the two&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can hopefully get a sense looking at our roadmap for other databases to support, there are obvious advantages to using some of the other databases on our roadmapGoing forwards, we expect that many companies using Snowplow will store that Snowplow data in more than one store, to enable a very broad range of analytics from different types of tools.&lt;/p&gt;

&lt;h2 id='using_the_storageloader'&gt;Using the StorageLoader&lt;/h2&gt;

&lt;p&gt;You can configure StorageLoader with the details of the Infobright table to insert your Snowplow events into, and then you schedule StorageLoader (e.g. in a cronjob) to regularly download your Snowplow events and load them into Infobright. StorageLoader can run as soon as EmrEtlRunner has completed its job (and we include a script to run both in one go).&lt;/p&gt;

&lt;p&gt;With this setup, you will have your Snowplow events easily accessible and queryable in a local Infobright instance - but you can still fall back to querying the data in Hive if you wish.&lt;/p&gt;

&lt;p&gt;The following setup guides should be helpful in terms of setting up StorageLoader:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/infobright-storage-setup'&gt;Infobright storage setup guide&lt;/a&gt; walks you through the process of installing Infobright and setting it up to house Snowplow data&lt;/li&gt;

&lt;li&gt;&lt;a href='https://github.com/snowplow/snowplow/wiki/StorageLoader-setup'&gt;StorageLoader setup guide&lt;/a&gt; walks you through installing and configuring StorageLoader to regularly load Snowplow data into Infobright&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/storage-loader'&gt;4-storage/storage-loader/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting StorageLoader working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='other_fixes_in_060'&gt;Other fixes in 0.6.0&lt;/h2&gt;

&lt;p&gt;We have made a number of other fixes across Snowplow to prepare the ground for StorageLoader:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EmrEtlRunner&lt;/strong&gt; has been bumped to 0.0.5, including upgrading it to Sluice 0.0.4 (which has some bug fixes around S3 path handling).&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Hive deserializer&lt;/strong&gt; has been bumped to 0.5.1, and now outputs booleans such as &lt;code&gt;br_cookies&lt;/code&gt; as 0 or 1 (instead of true or false) for the non-Hive output.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;non-Hive format HiveQL script&lt;/strong&gt; has been bumped to 0.0.2 and now uses the new 0 or 1 approach to booleans. This is necessary so that true/false values can be successfully loaded into Infobright.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;setup_infobright.sql&lt;/strong&gt; script has been bumped to 0.0.2 - we have changed the columns defined as booleans to be tinyint(1)s. This is just a formality, because Infobright creates &amp;#8216;boolean&amp;#8217; columns as tinyint(1)s anyway.&lt;/p&gt;

&lt;p&gt;We will keep you posted as we roll out support for additional database options in StorageLoader! (And welcome suggestinos for other databases we should build support for.)&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/06/snowplow-0.5.2-released-and-introducing-sluice"/>
    <title>Snowplow 0.5.2 released, and introducing the Sluice Ruby gem</title>
    <updated>2012-11-06T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Another week, another release: Snowplow &lt;strong&gt;0.5.2&lt;/strong&gt;! This is a small release, consisting just of a small set of bug fixes and improvements to EmrEtlRunner - although we&amp;#8217;ll also use this post to introduce our new Ruby gem, called Sluice.&lt;/p&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/testower'&gt;Tom Erik Stwer&lt;/a&gt; for his testing of EmrEtlRunner over the weekend, which helped us to identify and fix these bugs:&lt;/p&gt;

&lt;h2 id='bugs_fixed'&gt;Bugs fixed&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/71'&gt;Issue 71&lt;/a&gt;&lt;/strong&gt;: the template &lt;code&gt;config.yml&lt;/code&gt; (in the GitHub repo and in the wiki) was specifying an out-of-date version for the Hive deserializer. We have updated this to specify version &lt;strong&gt;0.5.0&lt;/strong&gt; of the serde, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
:snowplow:
  :serde_version: 0.4.9
...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href='https://github.com/snowplow/snowplow/issues/72'&gt;Issue 72&lt;/a&gt;&lt;/strong&gt;: Tom&amp;#8217;s testing also identified a bug in EmrEtlRunner&amp;#8217;s log archiving, which only occurs if the Processing Bucket contains sub-folders. This has now been fixed too.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='a_new_feature_skip'&gt;A new feature: &amp;#8211;skip&lt;/h2&gt;

&lt;p&gt;A new release which only contains bug fixes is a boring release, so we have also implemented a new &lt;code&gt;--skip&lt;/code&gt; option for EmrEtlRunner (&lt;a href='https://github.com/snowplow/snowplow/issues/58'&gt;issue #58&lt;/a&gt;). You can use this when you call EmrEtlRunner like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bundle exec snowplow-emr-etl-runner &amp;lt;...&amp;gt; --skip staging OR --skip emr&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This option skips the work steps &lt;strong&gt;up to and including&lt;/strong&gt; the specified step. To give an example: &lt;code&gt;--skip emr&lt;/code&gt; skips both moving the raw logs to the Staging Bucket &lt;strong&gt;and&lt;/strong&gt; running the ETL process on Amazon EMR, i.e. EmrEtlRunner will &lt;strong&gt;only&lt;/strong&gt;* perform the final archiving step.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--skip&lt;/code&gt; is useful if you encounter a problem midway through your ETL process: you can fix the problem and then skip the steps which ran okay, rather than re-processing from the start. We find it especially helpful when we&amp;#8217;re testing new versions of EmrEtlRunner.&lt;/p&gt;

&lt;h2 id='and_introducing_sluice'&gt;And introducing Sluice&lt;/h2&gt;

&lt;p&gt;At Snowplow Analytics we are committed to making our software as modular and loosely-coupled as possible. Where we have functionality which could be more widely used, we aim to extract it into standalone modules for developers to use even if they are not implementing Snowplow.&lt;/p&gt;

&lt;p&gt;We have followed this approach with the parallel file-copy code for Amazon S3 added to EmrEtlRunner by community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt;: we have moved this code out of EmrEtlRunner into a new Ruby gem, called Sluice. Sluice now has its own &lt;a href='https://github.com/snowplow/sluice'&gt;GitHub repository&lt;/a&gt;, and has been published on &lt;a href='http://rubygems.org/gems/sluice'&gt;RubyGems.org&lt;/a&gt;. It&amp;#8217;s called Sluice because, like &lt;a href='https://github.com/cwensel'&gt;Chris Wensel&lt;/a&gt; (Cascading), we believe in flowing-water metaphors for ETL tools :-)&lt;/p&gt;

&lt;p&gt;Sluice is used by our EmrEtlRunner, and is also a dependency for the StorageLoader Ruby application which we are currently developing.&lt;/p&gt;

&lt;p&gt;We hope to build out Sluice as a general-purpose Ruby toolkit for cloud-friendly ETL over the coming months - and would love contributors! Our view is that, in a world of cloud services like Amazon S3, Google BigQuery and Elastic MapReduce, it makes most sense to take a programmatic approach to ETL, rather than contort the historic, application-based approach of &lt;a href='http://www.talend.com'&gt;Talend&lt;/a&gt;, &lt;a href='http://www.pentaho.com/explore/pentaho-data-integration/'&gt;Pentaho DI&lt;/a&gt; et al. We see Sluice as part of that toolkit for programmatic ETL, alongside great tools such as &lt;a href='http://www.cascading.org'&gt;Cascading&lt;/a&gt;, Rob Slifka&amp;#8217;s &lt;a href='https://github.com/rslifka/elasticity'&gt;Elasticity&lt;/a&gt; and &lt;a href='http://palletops.com'&gt;Pallet&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/11/01/snowplow-0.5.1-released"/>
    <title>Snowplow 0.5.1 released, with lots of small improvements</title>
    <updated>2012-11-01T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow &lt;strong&gt;0.5.1&lt;/strong&gt;! Rather than one large new feature, version 0.5.1 is an incremental release which contains lots of small fixes and improvements to the ETL and storage sub-systems. The two big themes of these updates are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Improving the robustness of the ETL process&lt;/li&gt;

&lt;li&gt;Laying the foundations for loading Snowplow events into &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To take each of these themes in turn:&lt;/p&gt;

&lt;h2 id='1_a_more_robust_etl_process'&gt;1. A more robust ETL process&lt;/h2&gt;

&lt;p&gt;The Hive deserializer now has improved error handling - many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his help here!&lt;/p&gt;

&lt;p&gt;Firstly, the Hive deserializer is now setup to log warnings (rather than die) on non-critical data quality issues.&lt;/p&gt;

&lt;p&gt;Additionally, there is now an option (switched off by default) to continue processing even on unexpected row-level errors (such as an input file not matching the expected CloudFront format). We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; to support this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :continue_on_unexpected_error: false&lt;/code&gt;&lt;/pre&gt;
&lt;!--more--&gt;
&lt;p&gt;Switch this to &amp;#8216;true&amp;#8217; to continue processing on unexpected row-level errors.&lt;/p&gt;

&lt;h2 id='2_groundwork_for_infobright_compatibility'&gt;2. Groundwork for Infobright compatibility&lt;/h2&gt;

&lt;p&gt;We have added a table definition (and supporting scripts) for setting up a Snowplow events table in Infobright - you can find these in the main repository under &lt;a href='https://github.com/snowplow/snowplow/tree/master/4-storage/infobright-storage'&gt;&lt;code&gt;snowplow/4-storage/infobright-storage&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some early ETL design decisions meant that the Snowplow event files being generated before 0.5.1 were not compatible with being loaded into Infobright (or similar relational databases like Postgres or MySQL). We have made some updates to the ETL process in 0.5.1 to fix this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the Hive deserializer, we now convert tabs to 4 spaces to prevent a stray tab from breaking our load into Infobright&lt;/li&gt;

&lt;li&gt;Databases like Infobright don&amp;#8217;t support Hive&amp;#8217;s &lt;code&gt;ARRAY&amp;lt;STRING&amp;gt;&lt;/code&gt; syntax, so we have updated the Hive deserializer to also output individual booleans for the browser features, alongside the browser features array&lt;/li&gt;

&lt;li&gt;We have created a new HiveQL script which outputs Snowplow event files in a format which can be easily loaded into Infobright - this is called &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/hive-etl/hiveql/non-hive-rolling-etl.q'&gt;&lt;code&gt;non-hive-rolling-etl.q&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;We have added a configuration option to the EmrEtlRunner&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration file&lt;/a&gt; so that you can choose whether to output Hive-format or non-Hive-format event files&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On point 4: we believe that most people will want to load their Snowplow event files into other database systems, such as Infobright (or eventually, Postgres, Google BigQuery, SkyDB etc). Therefore, the default setting for the &lt;a href='https://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml'&gt;configuration option&lt;/a&gt; in the EmrEtlRunner is to output your Snowplow event files in the non-Hive-format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:etl:
  :storage_format: non-hive&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the comment says, if you will &lt;strong&gt;only&lt;/strong&gt; be doing analysis in Hive, you could switch this setting to &amp;#8216;hive&amp;#8217; and benefit from the slightly-tweaked, Hive-friendly file format.&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting version 0.5.1 working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At Snowplow we want to support multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. This version 0.5.1 provides the building blocks for our Infobright support - for the next release, we are working on a Storage Loader component to download your event files from Amazon S3 and load them into your local Infobright instance. We&amp;#8217;ll keep you posted on our progress here!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/31/snowplow-in-a-universal-analytics-world-what-the-new-version-of-google-analytics-means-for-companies-adopting-snowplow"/>
    <title>Snowplow in a Universal Analytics world - what the new version of Google Analytics means for companies adopting Snowplow</title>
    <updated>2012-10-31T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Earlier this week, Google announced a series of significant advances in Google Analytics at the GA Summit, that are collectively referred to as &lt;a href='http://cutroni.com/blog/2012/10/29/universal-analytics-the-next-generation-of-google-analytics/'&gt;Universal Analytics&lt;/a&gt;. In this post, we look at:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what'&gt;The actual features Google has announced&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#whysnowplow'&gt;How those advances change the case for companies considering adopting Snowplow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img alt='universal-analytics-image' src='/static/img/google-analytics-universal-analytics.png' /&gt;&lt;/p&gt;
&lt;a name='what'&gt;&lt;h2&gt;1. What changes has Google announced?&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;The most significant change Google has announced is the new &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt;, which enables businesses using GA to capture much more data. This will make it possible for Google to deliver a much broader range of reports, of higher business value, than was previously possible. To understand the changes, we start by considering what &lt;a href='#new-data-points'&gt;new data points&lt;/a&gt; businesses can &lt;em&gt;feed&lt;/em&gt; GA, before considering &lt;a href='/blog/2012/10/31/snowplow-proposition-in-a-universal-analytics-world-what-the-new-version-of-ga-means-for-snowplow-adoption#reporting-capabilities'&gt;what that means for GA&amp;#8217;s reporting capabilities&lt;/a&gt;.&lt;/p&gt;
&lt;a name='new-data-points'&gt;&lt;h3&gt;1.1 Custom user identifiers&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;The first new data points that businesses can feed into Google Analytics is a user&amp;#8217;s &lt;code&gt;client_id&lt;/code&gt; (basically, a customer ID) as defined on the business&amp;#8217;s own systems.&lt;/p&gt;

&lt;p&gt;Previously, Google Analytics identified unique users using their own &lt;code&gt;cookie_id&lt;/code&gt;s. Google&amp;#8217;s &lt;code&gt;cookie_id&lt;/code&gt;s are an excellent starting point for identifying users, because so many users have Google accounts (thanks to their myriad mass-market services, including Gmail, YouTube, Google Play etc): consumers using these services on multiple devices identify themselves to Google when they login, meaning that Google can marry their &lt;code&gt;cookie_id&lt;/code&gt;s for these users on all the different devices they use. Our assumption is that Google already use this to reliably identify individual users across multiple platforms and devices.&lt;/p&gt;

&lt;p&gt;For businesses using GA, being able to augment Google&amp;#8217;s user identification with their own internal &lt;code&gt;client_id&lt;/code&gt;s is a significant step forwards for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Many GA users (especially those with applications or properties where users login, or those with a loyalty card scheme) can identify their own users reliably on specific platforms, devices and physical stores. By adding this additional user identification data into GA, GA will be more accurate at identifying the same user in different places reliably, moving us further from a world in which we rely on persistent cookies dropped on browsers with unique identifiers, to one where users are more robustly identified via logins, payments and loyalty schemes. These approaches will still use cookies, but as part of a broader set of user identification business processes that actively involve the user in the identification process&lt;/li&gt;

&lt;li&gt;It should make it easier for GA users to join GA data with other customer data sets on those &lt;code&gt;client_id&lt;/code&gt;s. This is more of a nuanced point, as it was still possible previously to add customer IDs to GA as custom variables and use that to do joining&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;&lt;a name='customer-journey'&gt;&lt;h3&gt;1.2 Capturing events across a user's entire customer journey (not just the web, not just digital interactions)&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;We have long argued that web analytics is just one customer data source - and that analysts performing customer analytics need to crunch data covering the customer&amp;#8217;s complete journey, including other digital channels and offline interactions. That means joining data sets from different digital products and offline data sets to generate a single customer view. To date, companies that have implemented &amp;#8220;single customer views&amp;#8221; have typically struggled incorporating web behavior in those views.&lt;/p&gt;

&lt;p&gt;Google has taken a significant step towards enabling businesses to capture much more of their customer&amp;#8217;s journeys in Google Analytics itself. The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; makes it possible to pass offline events into Google: so for example, when a customer buys an item in store, it would be possible to fire an event to Google Analytics recording that sale. If the customer was on a CRM programme (e.g. loyalty scheme), his / her &lt;code&gt;client_id&lt;/code&gt; could be passed in, and then Google Analytics would know that this is the same user who browsed the website on their mobile phone yesterday and viewed it from their office today, prior to coming in store to make the purchase.&lt;/p&gt;

&lt;p&gt;The Measurement Protocol can also be used to capture events on digital platforms that are not so well suited to traditional web analytics solution e.g. mobile applications, set-top box applications, videogames on consoles etc. It thus opens the door for Google Analytics to capture and report on event data from a range of devices, not just those that are web based.&lt;/p&gt;

&lt;p&gt;Taken together, this means it will be possible for Google Analytics to offer reports detailing customer behavior across the complete customer journey. Building on this, it should also be possible for GA to enable analysts to calculate &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;customer lifetime value&lt;/a&gt; (if the value of different events was passed in with the events): this is one of the most important metrics in customer analytics, and one that has been conspicuous by its absence from web analytics outside of solutions like &lt;a href='/analytics/customer-analytics/customer-lifetime-value.html'&gt;Snowplow&lt;/a&gt; until now. The Measurement Protocol potentially means a huge increase in the scope and value of reports that it should be possible to generate in Google Analytics.&lt;/p&gt;
&lt;a name='cost-data'&gt;&lt;h3&gt;1.3 Capturing customer-acquisition cost data&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;One of the most common types of analytics performed on web data is working out the return on marketing investment for different customer-acquisition channels. To perform this analysis, we need to combine:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data on what was spent on each channel - typically cost data from those channels themselves, e.g. display, PPC, affiliate, social etc&lt;/li&gt;

&lt;li&gt;Web analytics data on how many people visited the website in response to those ads and what fraction of them went on to become customers. By dividing the total spent on each channel (1) by the number of customers acquired from each channel (2), we can calculate the cost of acquiring each customer for that channel.&lt;/li&gt;

&lt;li&gt;Financial data on the revenue/profits generated by those customers, over their lifetimes. By comparing the average value of each customer acquired from each channel against the average cost of acquiring each of those customers, we can calculate the return on that acquisition cost&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that Google lets businesses reliably track their users over their entire lifecycles (on and offline), it becomes possible to calculate the user&amp;#8217;s lifetime value, as detailed &lt;a href='#customer-journey'&gt;above&lt;/a&gt;, delivering on point #3. Google has always enabled business to capture #2. Now, Google lets you send the cost data into Google Analytics (point #1) - so that the return of each campaign can be accurately calculated. (Previously, only spend data from AdWords could be imported into GA.) With this information, companies should be better placed to drive marketing spend decisions based on Google Analytics reports. Again though, the reality is more nuanced, because typically those spend decisions have to be made &lt;em&gt;before&lt;/em&gt; a customer&amp;#8217;s lifetime value (#3) can be accurately calculated, so companies really need to develop predictive models of how valuable customers are likely to be. Anything but the most basic models are likely to require tools outside of GA to develop, and then pulling that data out of GA to power those models.&lt;/p&gt;

&lt;h3 id='14_custom_dimensions_and_metrics'&gt;1.4 Custom dimensions and metrics&lt;/h3&gt;

&lt;p&gt;The &lt;a href='https://developers.google.com/analytics/devguides/collection/protocol/v1/'&gt;Measurement Protocol&lt;/a&gt; enables businesses to define and capture their own dimensions and metrics each time an event that is tracked. Those additional metrics and dimensions are then available to report in in GA.&lt;/p&gt;

&lt;p&gt;As well as enabling businesses to add custom dimension and metric values to individual event tracking calls, Google also lets businesses bulk upload multiple dimensions at a time into the GA, if a relationship between those custom dimensions and dimensions already in GA can be defined, and GA knows what values to ascribe events already in it to those new dimensions, based on that defined relationship. To give an example: you could upload the product names/SKUs associated with each web page, enabling reporting on page views by SKU. Or, you could upload a range of product metadata (e.g. book titles and authors) and associate that with an ISBN custom field.&lt;/p&gt;
&lt;a name='reporting-capabilities'&gt;&lt;h3&gt;1.5 What new reporting is enabled through the capture of all these additional data points?&lt;/h3&gt;&lt;/a&gt;
&lt;p&gt;Taken together, the additional data that businesses can feed into Google Analytics gives Google enough to offer a much broader and more valuable range of reporting than was previously possible:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Customer analytics&lt;/strong&gt;. We have long argued that web analytics packages including GA are too focused on sessions, page views and conversions, and neglect the broader, more valuable customer analytics that underpin the most successful businesses in the world. With these new data points, GA has the raw data to produce useful customer reports including customer lifetime value, and analysis of user behaviors over their entire journeys. No longer will web analysts using GA be confined to viewing actions over an isolated session: now they can slice and dice metrics by users over their user journeys spanning multiple site visits.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;Event analytics&lt;/strong&gt; across platforms, on and offline. GA can now report on user&amp;#8217;s complete journey, not just what they do on websites, but also their behaviors on other digital platforms (esp. mobile) and offline.&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='whysnowplow' /&gt;&lt;a name='whysnowplow'&gt;&lt;h2&gt;2. How do the advances in GA change the case for adopting Snowplow?&lt;/h2&gt;&lt;/a&gt;
&lt;p&gt;Prior to the latest announcement, the case for adopting Snowplow alongside your GA implementation was as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The reporting provided by Google Analytics is very limited, with little/no customer analytics, catalogue analytics and platform analytics supported.&lt;/li&gt;

&lt;li&gt;Snowplow enables you to perform all these three types of analytics, by providing you with access to your raw customer-level and event-level clickstream data, so that you can use whatever analytics tool you like to crunch the data and perform that analysis&lt;/li&gt;

&lt;li&gt;Snowplow makes it easier to join your web analytics data sets with other data sets (e.g. marketing data sets, CRM and offline data sets), by enabling businesses to load customer IDs into Snowplow, and then perform the join on the raw data sets. This means that businesses running Snowplow can analyse user behavior across their entire customer journey (on and offline, across all digital and non-digital channels)&lt;/li&gt;

&lt;li&gt;Snowplow makes it easy to warehouse your customer data for posterity: an asset which will doubtless grow in value over time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the latest announcement, some of these arguments fall away:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google has significantly strengthened its customer analytics capability. To what extent is not yet clear - we only know at this stage what extra data points Google Analytics will, hypothetically, let you collect - not what additional reporting UIs GA will provide to process that data&lt;/li&gt;

&lt;li&gt;The additional data points &lt;em&gt;should&lt;/em&gt; improve GA&amp;#8217;s platform and catalogue analytics capabilities; we will only be able to confirm this once we start working with the updated version of GA&lt;/li&gt;

&lt;li&gt;Therefore, the gap between what is possible with GA, and what is possible with Snowplow, has shrunk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nevertheless, the case for implementing Snowplow alongside GA is still compelling, for three main reasons. To take each of these in turn:&lt;/p&gt;

&lt;h3 id='21_analytics_capabilities'&gt;2.1 Analytics capabilities&lt;/h3&gt;

&lt;p&gt;There are several different considerations here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Google Analytics still does not give you access to your customer-level and event-level data. Therefore, &lt;strong&gt;there will always be ways that you can crunch Snowplow data that you cannot accomplish in GA&lt;/strong&gt;: drilling down to segments of one visitor is just the most obvious example&lt;/li&gt;

&lt;li&gt;There are a range of analytics techniques which are hard to imagine Google implementing at all, even with the new data sets that are available. To give just three examples:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;Using machine learning techniques (e.g. &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt;) to &lt;strong&gt;segment audience by behavior&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Performing &lt;strong&gt;event analytics&lt;/strong&gt; / pathing in a way that takes into account the &lt;strong&gt;structure of the website&lt;/strong&gt;. This is described brilliantly by &lt;a href='http://semphonic.blogs.com/about.html'&gt;Gary Angel&lt;/a&gt; on the &lt;a href='http://semphonic.blogs.com/semangel/2011/01/statistical-analysis-functionalism-and-how-web-analytics-works.html'&gt;Semphonic blog&lt;/a&gt;. This methodology includes identifying those events that are predictive of customer lifetime value&lt;/li&gt;

&lt;li&gt;Building and testing models that &lt;strong&gt;predict customer lifetime value ahead of time&lt;/strong&gt;, so that you can quickly (and robustly) calculate the ROI on marketing campaigns, and adjust your spend accordingly&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;There will always be barriers analysts run up against in trying to fit all of their data into Google&amp;#8217;s schema. For example, it&amp;#8217;s not obvious how Google&amp;#8217;s single &lt;code&gt;client_id&lt;/code&gt; will cope with different packages (CRM, email, CMS et al) each having their own internal set of user IDs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='22_creating_live_datadriven_products'&gt;2.2 Creating live, data-driven products&lt;/h3&gt;

&lt;p&gt;There are also important capabilities around using your event data and derived analyses in &lt;strong&gt;live, data-driven products&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having access to the event stream and your own analyses allows you to make use of that data in data-driven products and systems, including &lt;strong&gt;product / content recommendation&lt;/strong&gt;, &lt;strong&gt;user personalisation engines&lt;/strong&gt; and &lt;strong&gt;internal search algorithms&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;Because Snowplow is open-source software which can be installed on your own servers, it should be possible to co-locate Snowplow with your own software (CMSes, ecommerce packages, custom apps etc) and thus tightly integrate these data-driven products into your offering&lt;/li&gt;

&lt;li&gt;Because GA doesn&amp;#8217;t provide the granular customer-level and event-level data, GA data cannot be used to prototype or drive these data-driven services&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id='23_data_ownership_and_technical_architecture'&gt;2.3 Data ownership and technical architecture&lt;/h3&gt;

&lt;p&gt;Finally, there are also a number of &lt;strong&gt;data ownership&lt;/strong&gt; and &lt;strong&gt;architectural issues&lt;/strong&gt; which we believe make a Snowplow solution an important compliment, if not yet a full alternative, to a GA implementation. These relate to the fact that, with GA, businesses get more value out by feeding more and more data in: to realise all of the new potential above, they need to be feeding GA with data covering their &lt;em&gt;complete&lt;/em&gt; set of customer interactions. However:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is the &lt;strong&gt;most valuable data&lt;/strong&gt; your company owns. Does it make sense to leave the warehousing and storage of that data to a third-party who in many cases is providing the service for free? What guarantees does you have that that data will always be available, 3, 5 or 10 years down the line?&lt;/li&gt;

&lt;li&gt;Does it make sense to feed your detailed event- and customer-level data to Google Analytics, when GA does not share that data back with you at the same atomic level of detail. (GA rolls the data up to &lt;strong&gt;aggregates&lt;/strong&gt; which are less flexible to work with from an analytics perspective)&lt;/li&gt;

&lt;li&gt;What happens when &lt;strong&gt;innacurate data&lt;/strong&gt; is loaded into Google Analytics? Without the ablity to query and diligence the data directly, leave alone clean and reprocess data, there are very limited options available for a business that has innaccurate data in GA. This becomes a bigger issue as implementation become more complex (because data is being ingested across digital and offline platforms), and GA becomes the de facto tool for all customer analytics&lt;/li&gt;

&lt;li&gt;If you need to setup regular ETL processes to load the data from all of your third-party systems into GA, you could &lt;strong&gt;expend the same energy setting up Snowplow instead&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='closing_thoughts'&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;We at Snowplow Analytics are enormously excited by the progress Google are making with their Universal Analytics proposition, and especially the good work Google are doing educating the market into the value of customer-centric analytics. But to unleash the full power of that type of customer, platform and catalogue analytics, the serious analyst will still need access to the customer-level and event-level data: ideally on infrastructure that is totally under your own control. Snowplow is still the best way of getting hold and storing that data.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/25/snowplow-0.5.0-released"/>
    <title>Snowplow 0.5.0 released, now with a Ruby gem to run Snowplow's ETL process on Amazon EMR</title>
    <updated>2012-10-25T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow &lt;strong&gt;0.5.0&lt;/strong&gt;, with an all-new component, the Snowplow EmrEtlRunner. EmrEtlRunner is a Ruby application to run Snowplow&amp;#8217;s Hive-based ETL (extract, transform, load) process on &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt; with minimum fuss.&lt;/p&gt;

&lt;p&gt;We are hugely grateful to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for his contributions to EmrEtlRunner: thanks to Michael, EmrEtlRunner is more efficient, more flexible and more robust than it otherwise would have been - and ready sooner. Many thanks Michael!&lt;/p&gt;

&lt;h2 id='using_emretlrunner'&gt;Using EmrEtlRunner&lt;/h2&gt;

&lt;p&gt;EmrEtlRunner is a Ruby application which you can setup on your server to regularly take your raw Snowplow logs (as stored in CloudFront access logs) and apply the Hive-based ETL process to them using &lt;a href='http://aws.amazon.com/elasticmapreduce/'&gt;Amazon Elastic MapReduce&lt;/a&gt;. This ETL process populates a Hive-format events table which you can then use with the HiveQL recipes in our &lt;a href='http://snowplowanalytics.com/analytics/index.html'&gt;Analyst&amp;#8217;s Cookbook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For detailed instructions on installing, running and scheduling EmrEtlRunner on your server, please see the &lt;a href='https://github.com/snowplow/snowplow/wiki/Deploying-EmrEtlRunner'&gt;Deploying EmrEtlRunner&lt;/a&gt; guide on the Snowplow Analytics wiki.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id='the_codebase'&gt;The codebase&lt;/h2&gt;

&lt;p&gt;If you want to take a look at the code, you can find it in the main repository here: &lt;a href='https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner'&gt;&lt;code&gt;3-etl/emr-etl-runner/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id='getting_help'&gt;Getting help&lt;/h2&gt;

&lt;p&gt;If you have any problems getting EmrEtlRunner working, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt; or get in touch with us via &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;the usual channels&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='in_the_pipeline'&gt;In the pipeline&lt;/h2&gt;

&lt;p&gt;At Snowplow we want to support multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. Our first priority is supporting &lt;a href='http://www.infobright.org/'&gt;Infobright Community Edition&lt;/a&gt; (ICE) for event storage and querying; extending the current ETL process to load Snowplow events into ICE will be the focus of our next few releases, so please stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow"/>
    <title>Performing web analytics on Snowplow data using Tableau - a video demo</title>
    <updated>2012-10-24T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;People who see Snowplow for the first time often ask us to &lt;i&gt;&quot;show Snowplow in action&quot;&lt;/i&gt;. It is one thing to tell someone that having access to their customer- and event-level data will open up whole new analysis possibilities, but it is another thing to demonstrate those possibilities.&lt;/p&gt;

&lt;p&gt;Demonstrating Snowplow is tricky because currently, Snowplow only gives you access to data: we have no snazzy front-end UI to show off. The good news is that there are a lot of smart people developing fast, powerful and easy-to-use reporting tools. And because Snowplow gives you access to underlying customer- and event-level data, it is easy to analyse Snowplow data in nearly all of these tools. One such tool is &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; - we like Tableau as it is fast and intuitive, making it easy for us to perform train-of-thought analyses on Snowplow data. (We will explain more on how to connect Tableau to Snowplow data in a future blog post.)&lt;/p&gt;

&lt;p&gt;In the following series of videos, we start to show how Snowplow lets you use &lt;a href=&quot;http://www.tableausoftware.com/&quot;&gt;Tableau&lt;/a&gt; for exploring your web analytics data. In the first video, we introduce Tableau and talk through the Tableau worksheet created with Snowplow data for an online retailer:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-1.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In the second video, we show how to perform an analysis of the drivers of growth of traffic on a website. The video serves to highlight how effective Tableau is at performing train-of-thought analysis:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt; 
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-2.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the third video, we show how to perform an analysis comparing the relative performance of different products in an online retailer's catalogue. This is an example of &lt;strong&gt;catalogue analytics&lt;/strong&gt;, a very important branch of analytics - where we analyse how different products on a retailer's site perform relative to one another, or how different media items (e.g. articles / videos) on a media site perform. Surprisingly, catalogue analytics is not supported by traditional web analytics packages like Google Analytics:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-3.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fourth video, we analyse improvements in conversion rates over time for the retailer. This is a core measure to track in order to understand how improvements to the website and marketing strategy drive increased conversion rates. Again, this is something not supported by Google Analytics out of the box. We show how easy it is with Snowplow and Tableau to identify trends in conversion rates over time:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-4.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the fifth video, we show how to visualise patterns of individual user visits over time. This is an interesting starting point to begin to unpick the patterns that make up successful user engagement:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-5.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the sixth video, we show how to visualise the range of product pages visited by each user. This can help us to understand how successful the retailer is at driving users interested in one product to consider buying other products (cross-selling), and onwards to developing recommendation algorithms (users who liked &lt;i&gt;this&lt;/i&gt; product also liked &lt;i&gt;this&lt;/i&gt; product):&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot; type=&quot;video/webm&quot; /&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-6.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In the final video in the series, we perform an example cohort analysis, with a view to understanding how 'sticky' the online retailer site is, and how its stickiness has improved over time. In this example, we use &lt;i&gt;stickiness&lt;/i&gt; to refer to how good the website is at driving repeat visits:&lt;/p&gt;

&lt;video width=&quot;648&quot; height=&quot;563&quot; controls&gt;

	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;  type=&quot;video/mp4&quot; /&gt;
	&lt;!-- LINK BROKEN - need to fix&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot; type=&quot;video/webm&quot; /&gt; --&gt;
	&lt;source src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;  type=&quot;video/ogg&quot; /&gt;
	&lt;object width=&quot;648&quot; height=&quot;563&quot; type=&quot;application/x-shockwave-flash&quot; data=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot;&gt;
		&lt;param name=&quot;movie&quot; value=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.swf&quot; /&gt;
		&lt;param name=&quot;flashvars&quot; value=&quot;controlbar=over&amp;amp;image=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&amp;amp;file=http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot; /&gt;
		&lt;img src=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7-thumb.jpg&quot; width=&quot;648&quot; height=&quot;563&quot; alt=&quot;Web analytics with Tableau and Snowplow introductory video&quot;
		     title=&quot;No video playback capabilities, please download the video below&quot; /&gt;
	&lt;/object&gt;
&lt;/video&gt;

&lt;p class=&quot;note&quot;&gt;&lt;i&gt;Having trouble viewing the video above? You may download the videos in your format of choice:&lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.mp4&quot;&gt;&quot;MP4&quot;&lt;/a&gt;, &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.ogv&quot;&gt;&quot;Ogg&quot;&lt;/a&gt; or &lt;a href=&quot;http://static.snowplowanalytics.com/videos/tableau/example-cube/tableau-intro-7.webm&quot;&gt;WebM&lt;/a&gt; formats.&lt;/i&gt;&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/21/infobright-ruby-loader-released"/>
    <title>Infobright Ruby Loader Released</title>
    <updated>2012-10-21T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We&amp;#8217;re pleased to start the week with the release of a new Ruby gem, our &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL).&lt;/p&gt;

&lt;p&gt;At Snowplow we&amp;#8217;re committed to supporting multiple different storage and analytics options for Snowplow events, alongside our current Hive-based approach. One of the alternative data stores we are working with is &lt;a href='http://www.infobright.org/'&gt;Infobright&lt;/a&gt;, a columnar database which is available in open source and commercial versions.&lt;/p&gt;

&lt;p&gt;For all but the largest Snowplow users, columnar databases such as Infobright should be an attractive alternative to doing all of your analysis in Hive. The main advantages of columnar databases are as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Scale to terabytes (although not petabytes, unlike Hive)&lt;/li&gt;

&lt;li&gt;Fixed cost (dedicated RAM-heavy analytics server), versus pay-as-you-go querying on Amazon EMR&lt;/li&gt;

&lt;li&gt;Significantly faster query times  typically seconds, not minutes&lt;/li&gt;

&lt;li&gt;Plug in to many analytics front ends e.g. Tableau, Qlikview, R&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, open source columnar databases like Infobright Community Edition (ICE) are a good fit for Snowplow analytics. Unfortunately, when we started to load Snowplow event logs into ICE, we realised that there wasn&amp;#8217;t a good data-loading solution for Infobright in Ruby, our ETL language of choice. So, we built one :-)&lt;/p&gt;

&lt;p&gt;Our freshly minted &lt;a href='https://github.com/snowplow/infobright-ruby-loader'&gt;Infobright Ruby Loader&lt;/a&gt; (IRL) can be used in two different ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;As a command-line tool&lt;/strong&gt; - for manual loading of data into Infobright at the command-line. No Ruby expertise required&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;As part of another application&lt;/strong&gt; - because it&amp;#8217;s a Ruby gem with a Ruby API, IRL can be integrated into larger Ruby ETL processes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will be using IRL at Snowplow as part of our larger ETL process to load Snowplow events into ICE for analysis - we hope to roll this out within the next few weeks.&lt;/p&gt;

&lt;p&gt;In the meantime, we hope that IRL is useful to people in the Infobright community who need to run data loads at the command-line; IRL was inspired by &lt;a href='http://www.infobright.org/Blog/Entry/unscripted/'&gt;ParaFlex&lt;/a&gt;, an excellent Bash script from the Infobright team to perform parallel loading of Infobright, and can be used as a direct alternative to ParaFlex.&lt;/p&gt;

&lt;p&gt;To find out more about our Infobright Ruby Loader, please check out the detailed &lt;a href='https://github.com/snowplow/infobright-ruby-loader/blob/master/README.md'&gt;README&lt;/a&gt; in the GitHub repository. And please direct any questions through the &lt;a href='https://github.com/snowplow/snowplow/wiki/Talk-to-us'&gt;usual channels&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/12/how-the-role-of-hive-is-changing-at-snowplow"/>
    <title>How we use Hive at Snowplow, and how the role of Hive is changing. (Slides from our presentation to Hive London.)</title>
    <updated>2012-10-12T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Last night I gave a presentation to the clever folks at Hive London covering three things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How big data technologies like Apache Hive are transforming web analytics&lt;/li&gt;

&lt;li&gt;Howe we&amp;#8217;ve used Hive in Snowplow development&lt;/li&gt;

&lt;li&gt;How the role of Hive has changed at Snowplow over time, including a comparison of Hive against other technologies.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The slides from the presentation are below. As always, any questions / comments, please post them below.&lt;/p&gt;
&lt;iframe frameborder='0' height='356' marginheight='0' marginwidth='0' scrolling='no' src='http://www.slideshare.net/slideshow/embed_code/14696456' style='border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px' width='427'&gt;  &lt;/iframe&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/snowplow-0.4.10-released"/>
    <title>Snowplow 0.4.10 released</title>
    <updated>2012-10-11T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released version &lt;strong&gt;0.4.10&lt;/strong&gt; of Snowplow - people using 0.4.8 can jump straight to this version. This version updates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;snowplow.js to version 0.7.0&lt;/li&gt;

&lt;li&gt;the Hive deserializer to version 0.4.9&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Big thanks to community members &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; and &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for their most-helpful contributions to this release!&lt;/p&gt;

&lt;h2 id='main_changes'&gt;Main changes&lt;/h2&gt;

&lt;p&gt;The main changes are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The querystring parameter for site ID which the JavaScript tracker sends to your collector is renamed from &lt;code&gt;said&lt;/code&gt; to &lt;code&gt;aid&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;The Hive-based ETL process now extracts the ecommerce tracking fields and the site ID field and adds them into your processed events table&lt;/li&gt;

&lt;li&gt;We fixed a bug in the Hive deserializer where a partially-processed row was returned even if a fatal error was found in the row (now, a null row is returned instead)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest of the changes were all enhancements to the Hive deserializer&amp;#8217;s Specs2 test suite - these improvements should help to accelerate work on the deserializer (we have lots of cool new stuff we want to add to the deserializer!). &lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id='new_event_table_fields'&gt;New event table fields&lt;/h2&gt;

&lt;p&gt;The new fields in the event table all relate directly to additional tracking functionality which was added to the JavaScript tracker in &lt;a href='/blog/2012/09/06/snowplow-0.4.7-released/'&gt;Snowplow 0.4.7&lt;/a&gt;. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;setSiteId()&lt;/code&gt; functionality is now extracted to the &lt;code&gt;app_id&lt;/code&gt; field (short for application ID)&lt;/li&gt;

&lt;li&gt;The ecommerce tracking functionality is now extracted to a set of &lt;code&gt;tr_&lt;/code&gt; and &lt;code&gt;ti_&lt;/code&gt; fields&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For details on the new fields, please review our latest &lt;a href='/analytics/snowplow-table-structure.html'&gt;Hive events table definition&lt;/a&gt; - there is now a column indicating in which version a given field was added.&lt;/p&gt;

&lt;h2 id='how_to_get_the_new_version'&gt;How to get the new version&lt;/h2&gt;

&lt;p&gt;As usual, the new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.9.jar&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The updated snowplow.js is &lt;a href='https://raw.github.com/snowplow/snowplow/master/1-trackers/javascript-tracker/js/snowplow.js'&gt;available in our GitHub repository&lt;/a&gt; for you to minify and upload, or alternatively you can use the one on our CDN:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://d1fc8wv8zag5ca.cloudfront.net/0.7.0/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you have any problems with either of these components, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id='a_note_on_backwards_compatibility_for_the_events_table'&gt;A note on backwards compatibility for the events table&lt;/h2&gt;

&lt;p&gt;We will continue to add extra fields to the Snowplow events table as we add extra capabilities to the ETL process - for example, we are working on functionality to extract geo-location information from IP addresses via MaxMind.&lt;/p&gt;

&lt;p&gt;Starting with our new &lt;code&gt;app_id&lt;/code&gt; field, we will be adding all such new fields to the &lt;strong&gt;end&lt;/strong&gt; of our Hive events table definition. This will mean that you will &lt;strong&gt;not&lt;/strong&gt; have to re-run the ETL process across all your historic raw logs, provided you do &lt;strong&gt;not&lt;/strong&gt; need the data found in the new fields. This is because a Hive query across both the old event table format and the new table format works as long as you don&amp;#8217;t explicitly query a new field.&lt;/p&gt;

&lt;p&gt;In other words, Hive is futureproofed against new fields being added to the end of your underlying data files, and we&amp;#8217;ll take advantage of this to improve backwards compatibility for our events table!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/10/11/attlib-0.0.1-released"/>
    <title>Attlib - an open source library for extracting search marketing attribution data from referrer URLs</title>
    <updated>2012-10-11T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;&lt;strong&gt;Update 17-Dec-12&lt;/strong&gt;: We have renamed Attlib to &lt;a href='https://github.com/snowplow/referer-parser'&gt;referer-parser&lt;/a&gt;, to make it clearer what Attlib does: parse referer URLs. The repository has been updated accordingly. Some of the example code below is out-of-date now: we recommend checking out the &lt;a href='https://github.com/snowplow/referer-parser'&gt;repository&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;Last night we published &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt;, an open source Ruby library for extracting search marketing attribution data from referer (sic) URLs. In this post we talk through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href='#what_attlib_does'&gt;What Attlib does, and how to use it&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#install'&gt;Installing Attlib&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#search_engine_yaml'&gt;The search_engine.yml file&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_stack'&gt;Attlib as part of the Snowplow stack&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#other_languages'&gt;Attlib in other languages&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='#snowplow_components_as_standalone_projects'&gt;Making components of Snowplow available as standalone open source projects&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;a name='what_attlib_does' /&gt;
&lt;h3 id='what_attlib_does_and_how_to_use_it'&gt;What Attlib does, and how to use it&lt;/h3&gt;

&lt;p&gt;Attlib is straightforward Ruby library for extracting seach marketing attribution data from referrer URLs. You give it a referer URL to parse: it then lets you now whether the URL is from a search engine. If it is, it will tell you which search engine it is, and what keywords were typed. (If those keywords are included in the query string - this is no longer the case for users logged in to Google, as documented &lt;a href='http://googlewebmastercentral.blogspot.co.uk/2011/10/accessing-search-query-data-for-your.html'&gt;here&lt;/a&gt;.)&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='nb'&gt;require&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;attlib&amp;#39;&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='no'&gt;Referrer&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;new&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;http://images.google.ca/imgres?q=hermetic+tarot&amp;amp;hl=en&amp;amp;biw=1189&amp;amp;bih=521&amp;amp;tbm=isch&amp;amp;tbnid=BuQ_IyUbc25usM:&amp;amp;imgrefurl=http://www.psychicbazaar.com/tarot-cards/15-the-hermetic-tarot.html&amp;amp;imgurl=http://mdm.pbzstatic.com/tarot/the-hermetic-tarot/card-4.png&amp;amp;w=1064&amp;amp;h=1551&amp;amp;ei=ue9AUMe7Osn9iwLZ-4H4Dw&amp;amp;zoom=1&amp;amp;iact=hc&amp;amp;vpx=107&amp;amp;vpy=48&amp;amp;dur=2477&amp;amp;hovh=271&amp;amp;hovw=186&amp;amp;tx=133&amp;amp;ty=157&amp;amp;sig=115588264602219115047&amp;amp;page=4&amp;amp;tbnh=162&amp;amp;tbnw=120&amp;amp;start=57&amp;amp;ndsp=19&amp;amp;ved=1t:429,r:12,s:57,i:291&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;

&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;is_search_engine?&lt;/span&gt; &lt;span class='c1'&gt;# True&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;search_engine&lt;/span&gt; &lt;span class='c1'&gt;# &amp;#39;Google Images&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;r&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;keywords&lt;/span&gt; 	&lt;span class='c1'&gt;# &amp;#39;hermetic tarot&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;a name='install' /&gt;
&lt;h3 id='installing_attlib'&gt;Installing Attlib&lt;/h3&gt;

&lt;p&gt;Attlib is available via a Ruby Gem. To install, simply run the following at the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo gem install attlib&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sourcecode is available on &lt;a href='https://github.com/snowplow/referer-parser'&gt;Github&lt;/a&gt;&lt;/p&gt;
&lt;a name='search_engine_yaml' /&gt;
&lt;h3 id='the_search_enginesyml_file'&gt;The search_engines.yml file&lt;/h3&gt;

&lt;p&gt;Extracting search engine names and keywords from a referer URL is pretty straightforward. What is more complicated is keeping track of the myriad search engines that are out there, operating in different countries, the myriad domains they operate on, and the different query parameters that each of them uses to store the keywords.&lt;/p&gt;

&lt;p&gt;Because the space is constantly evolving, none of this information (about search engines, parameters and domains) has been hard coded into Attlib. All of it is available in the &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, in the &lt;a href='https://github.com/snowplow/attlib/tree/master'&gt;data&lt;/a&gt; in the repo. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The structure of the YAML file should be straightforward to understand. Each search engine is a top level item. For each search engine, two lists are given: one is a list of parameters used in that search engine&amp;#8217;s query string to identify the keywords entered. The other is the list of domains on which that search engine operates. An extract is shown below:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='yaml'&gt;&lt;span class='l-Scalar-Plain'&gt;Babylon&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;q&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;search.babylon.com&lt;/span&gt;
   &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;searchassist.babylon.com&lt;/span&gt;

&lt;span class='l-Scalar-Plain'&gt;Baidu&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;parameters&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt; 
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;wd&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;word&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;kw&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;k&lt;/span&gt;
  &lt;span class='l-Scalar-Plain'&gt;domains&lt;/span&gt;&lt;span class='p-Indicator'&gt;:&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;www1.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;zhidao.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;tieba.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;news.baidu.com&lt;/span&gt;
    &lt;span class='p-Indicator'&gt;-&lt;/span&gt; &lt;span class='l-Scalar-Plain'&gt;web.gougou.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Keeping this file up to date is a big job: one of our hopes releasing Attlib as an open source, standalone library, is that the community contributes to the file. We are enormously grateful to our friends at &lt;a href='http://piwik.org/'&gt;Piwik&lt;/a&gt; as our initial version of the file is based on the Piwik equivalent &lt;a href='https://github.com/piwik/piwik/blob/master/core/DataFiles/SearchEngines.php'&gt;SearchEngines.php&lt;/a&gt;, for the hard work they put into this version.&lt;/p&gt;
&lt;a name='snowplow_stack' /&gt;
&lt;h3 id='attlib_as_part_of_the_snowplow_stack'&gt;Attlib as part of the Snowplow stack&lt;/h3&gt;

&lt;p&gt;Our intention is to port &lt;a href='https://github.com/snowplow/referer-parser'&gt;Attlib&lt;/a&gt; into Scala and integrate it into the Snowplow stack: specifically the ETL phase. Both Ruby and Scala versions of Attlib will run based on the same &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file.&lt;/p&gt;
&lt;a name='other_languages' /&gt;
&lt;h3 id='attlib_in_other_languages'&gt;Attlib in other languages&lt;/h3&gt;

&lt;p&gt;As well as contributing to the search &lt;a href='https://github.com/snowplow/referer-parser/blob/master/search.yml'&gt;search_engines.yml&lt;/a&gt; file, we also hope that community members will develop versions of Attlib in other languages e.g. Python.&lt;/p&gt;
&lt;a name='snowplow_components_as_standalone_projects' /&gt;
&lt;h3 id='making_components_of_snowplow_available_as_standalone_open_source_projects'&gt;Making components of Snowplow available as standalone open source projects&lt;/h3&gt;

&lt;p&gt;Attlib is the first component in the Snowplow stack that we have released as a standalone library. There are many more in the pipeline. (More on this in future blog posts :-) ). For us, this is a key part of the Snowplow strategy:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Keeping the Snowplow architecture as loosely coupled as possible. We believe this makes Snowplow robust, scalable and extendable&lt;/li&gt;

&lt;li&gt;Grow the userbase of people using and contributing to each component. Processing web analytics data is a big job: there are many individual components involved, and each of them needs to evolve with the changing marketplace. Attlib is concerned today with extracting useful data from search engine referrers: but it is likely that as time goes on, we&amp;#8217;ll want to extend it to capture data from other types of referrers e.g. social networks or affiliate sites. The bigger the community of people on top of those developments, the better for everyone in the web analytics community. Releasing each component as a standalone open source library should help grow that community.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;Any questions about Attlib, or anything else in this post? Then &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; with the Snowplow team.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/24/what-does-snowplow-let-you-do"/>
    <title>Why set your data free?</title>
    <updated>2012-09-24T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;At Saturday&amp;#8217;s &lt;a href='http://ukdaa.co.uk/'&gt;Measure Camp&lt;/a&gt;, I had the chance to introduce Snowplow to a large number of some incredibly thoughtful and insightful people in the web analytics industry.&lt;/p&gt;

&lt;p&gt;With each person, I started by explaining that Snowplow gave them direct access to their customer-level and event-level data. The response I got in nearly all cases was: &lt;strong&gt;what does having direct access to my web analytics data enable me to do, that I can&amp;#8217;t do with Google Analytics / Omniture?&lt;/strong&gt; It&amp;#8217;s such a good question I thought I should publish an answer below:&lt;/p&gt;

&lt;h3 id='1_integrate_web_analytics_data_with_other_data_sources'&gt;1. Integrate web analytics data with other data sources&lt;/h3&gt;

&lt;p&gt;Integrating your web analytics data with other data sets enables you to answer a wide range of valuable business questions:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;Example business questions&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Marketing spend data e.g. AdWords, ad server data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What is the return on my ad spend? How should I optimize my return on ad spend&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Customer data e.g. CRM, loyalty&lt;/td&gt;&lt;td style='text-align: left;'&gt;How does the online behavior of my differnet customer segments vary by segment? Do online promotions drive offline sales? (Or vice versa?)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td style='text-align: left;'&gt;Product / media catalogue data&lt;/td&gt;&lt;td style='text-align: left;'&gt;What are my most profitable product lines? Do different types of products attract different customer segments? What are the products that drive the most visits?&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Snowplow makes integrating web analytics data with other data sources easier in a two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All your Snowplow data is directly accessible in Apache Hive or Infobright. (So no expensive export process is required, prior to linking the data sets.)&lt;/li&gt;

&lt;li&gt;Custom variables and event tracking give you plenty of opportunity to join e.g. customer IDs or campaigns names to enable &lt;code&gt;JOIN&lt;/code&gt;s across data set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more details on how to perform &lt;code&gt;JOIN&lt;/code&gt;s between Snowplow data and other sources, see refer to the guide to &lt;a href='/analytics/customer-analytics/joining-customer-data.html'&gt;joining Snowplow engagement data with other sources of customer data&lt;/a&gt;&lt;/p&gt;

&lt;h3 id='2_slice_and_dice_your_data_by_any_combination_of_dimensions__metrics_you_want'&gt;2. Slice and dice your data by any combination of dimensions / metrics you want&lt;/h3&gt;

&lt;p&gt;Google Analytics in particular only lets users create reports about of set combinations of dimensions and metrics. Examples of combinations that are &lt;strong&gt;not supported&lt;/strong&gt; include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of unique visitors by product page&lt;/li&gt;

&lt;li&gt;Different sources of traffic by product page (and how this changes over time)&lt;/li&gt;

&lt;li&gt;Engagement levels (e.g. number of visits, number of page views, conversion rates) by traffic source&lt;/li&gt;

&lt;li&gt;Improvements to conversion rates over time&lt;/li&gt;
&lt;/ol&gt;
&lt;!--more--&gt;
&lt;p&gt;In contrast, because Snowplow gives you access to the underlying data, it is possible to use BI tools like &lt;a href='http://www.tableausoftware.com/'&gt;Tableau&lt;/a&gt; and &lt;a href='http://www.microsoft.com/en-us/bi/powerpivot.aspx'&gt;PowerPivot&lt;/a&gt; to quickly slice and dice web analytics data by any dimensions / metrics you want. We&amp;#8217;ll be posting examples of how to do this in the next few days.&lt;/p&gt;

&lt;h3 id='3_use_machine_learning_tools_on_your_web_analytics_data'&gt;3. Use machine learning tools on your web analytics data&lt;/h3&gt;

&lt;p&gt;Machine learning tools, and &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in particular, have created some new and exciting opportunities to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop product and content recommendation engines, based on user web behavior. (E.g. users who viewed these content items, also viewed&amp;#8230;)&lt;/li&gt;

&lt;li&gt;Segment your audience by online behavior&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Snowplow makes it easy to extract the core input data you would need to feed a machine learning algorithm in a single query. (E.g. a matrix mapping users to products by page views / add to baskets / purchases etc.) We will be exploring ways to integrate Snowplow with &lt;a href='http://mahout.apache.org/'&gt;Mahout&lt;/a&gt; in a future blog post.&lt;/p&gt;

&lt;h3 id='4_view_data_for_individual_users_over_their_entire_lives'&gt;4. View data for individual users over their entire lives&lt;/h3&gt;

&lt;p&gt;Whereas reports on Google Analytics tend to be about visits, page views or transactions, Snowplow lets you slice data by users over multiple visits, opening up a wide range of possibilities:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Develop accurate models of customer lifetime value&lt;/li&gt;

&lt;li&gt;Develop more rigorous approaches to attribution modelling, by capturing in granular detail which channels touched a user at different points in their lifecycle&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id='5_interested_in_any__all_of_the_above'&gt;5. Interested in any / all of the above?&lt;/h3&gt;

&lt;p&gt;Then &lt;a href='/product/get-started.html'&gt;get started&lt;/a&gt; with Snowplow, or &lt;a href='/about/index.html'&gt;get in touch&lt;/a&gt; to find out more!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/14/snowplow-0.4.8-released"/>
    <title>Snowplow 0.4.8 released</title>
    <updated>2012-09-14T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow version &lt;strong&gt;0.4.8&lt;/strong&gt;, with a set of enhancements to the existing Hive deserializer:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Hive deserializer now supports Amazon&amp;#8217;s new CloudFront log file format (launched 12 September 2012) as well as the older format&lt;/li&gt;

&lt;li&gt;The Hive deserializer now supports a tracking pixel called simply &lt;code&gt;i&lt;/code&gt; (saving some characters versus &lt;code&gt;ice.png&lt;/code&gt;) (&lt;a href='https://github.com/snowplow/snowplow/issues/35'&gt;issue #35&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer now works if the CloudFront distribution has Forward Query String = yes (&lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;)&lt;/li&gt;

&lt;li&gt;The Hive deserializer no longer dies if the calling page&amp;#8217;s querystring is malformed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Many thanks to community member &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; in Melbourne for contributing the Forward Query String = yes fix!&lt;/p&gt;

&lt;h2 id='new_cloudfront_log_file_format'&gt;New CloudFront log file format&lt;/h2&gt;

&lt;p&gt;On 12th September 2012, Amazon &lt;a href='http://aws.amazon.com/about-aws/whats-new/2012/09/04/cloudfront-support-for-cookies-and-price-classes/'&gt;rolled out a new CloudFront log file format&lt;/a&gt;, adding three additional fields onto the end of each line:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;cs(Cookie)&lt;/strong&gt;, the cookie header in the request (if any). Logging of this field is optional.&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-result-type&lt;/strong&gt;, the result type of each HTTP(s) request (for example, cache hit/miss/error).&lt;/li&gt;

&lt;li&gt;&lt;strong&gt;x-edge-request-id&lt;/strong&gt;, an encrypted string that uniquely identifies a request to help AWS troubleshoot/debug any issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, please consult the Amazon CloudFront &lt;a href='http://docs.amazonwebservices.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat'&gt;Developer Guide&lt;/a&gt; for more information on these fields. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;As part of this new &lt;strong&gt;0.4.8&lt;/strong&gt; Snowplow release, the Hive deserializer now supports the new CloudFront format as well as the old format: if you deploy the latest version of the deserializer, you should be able to process both old-format and new-format CloudFront logs without issue.&lt;/p&gt;

&lt;h2 id='support_for__as_the_tracking_pixel'&gt;Support for &lt;code&gt;i&lt;/code&gt; as the tracking pixel&lt;/h2&gt;

&lt;p&gt;Currently the Snowplow JavaScript tracker fires a GET request to a tracking pixel called &lt;code&gt;ice.png&lt;/code&gt;. This works fine, but it makes more sense to call the pixel &lt;code&gt;i&lt;/code&gt;, for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We free up 5 extra characters to use for sending data&lt;/li&gt;

&lt;li&gt;A transparent GIF is smaller to send than a transparent PNG&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/shermozle/'&gt;Simon Rumble&lt;/a&gt; (author of &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;) for pointing this out! In due course we will update the JavaScript tracker and CloudFront collector to implement this change (see issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;#25&lt;/a&gt;), but to start off we have added support for &lt;code&gt;i&lt;/code&gt; to the new version of the Hive deserializer.&lt;/p&gt;

&lt;p&gt;This is a small change, but highlights a wider point for Snowplow development: in general, whenever we have a &amp;#8220;breaking change&amp;#8221; coming upstream, we will try to prepare for this change downstream first, to prevent any disruption to your use of Snowplow.&lt;/p&gt;

&lt;h2 id='support_for_forward_query_string__yes'&gt;Support for Forward Query String = yes&lt;/h2&gt;

&lt;p&gt;Thanks to &lt;a href='https://github.com/mtibben'&gt;Michael Tibben&lt;/a&gt; from &lt;a href='http://99designs.com'&gt;99designs&lt;/a&gt; for spotting that the Hive deserializer does not work if your CloudFront distribution has Forward Query String set to Yes; Michael not only raised the issue but also provided a fix, many thanks Michael!&lt;/p&gt;

&lt;p&gt;Most Snowplow users will have Forward Query String in their CloudFront distribution set to No, so this issue will not arise for them; however this fix will be invaluable for anyone who does have it set to Yes. If you want to read more about this, please check out &lt;a href='https://github.com/snowplow/snowplow/pull/39'&gt;issue #39&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re aware that our guide for setting up the CloudFront distribution is a bit out of date (which is how this issue can arise) - we will be refreshing the tracking pixel guide soon (&lt;a href='https://github.com/snowplow/snowplow/issues/25'&gt;issue #25&lt;/a&gt;)! Many thanks for your patience.&lt;/p&gt;

&lt;h2 id='more_robust_querystring_handling'&gt;More robust querystring handling&lt;/h2&gt;

&lt;p&gt;A small change - we have made the code for extracting marketing attribution more robust. Specifically, the Hive deserializer no longer dies (i.e. throws a non-recoverable &lt;code&gt;SerDeException&lt;/code&gt;) if the calling page&amp;#8217;s URL has a malformed querystring.&lt;/p&gt;

&lt;p&gt;An example of a malformed querystring would be something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://www.psychicbazaar.com/2-tarot-cards?n=48?utmsource=GoogleSearch&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the two &lt;code&gt;?&lt;/code&gt; questionmarks (the second one should be an &lt;code&gt;&amp;amp;&lt;/code&gt; ampersand). In the case of a malformed querystring like this, the five marketing attribution fields in the Hive output format for this row will all be set to null.&lt;/p&gt;

&lt;h2 id='deploying_the_new_version'&gt;Deploying the new version&lt;/h2&gt;

&lt;p&gt;The new version of the Hive deserializer is available from the GitHub repository&amp;#8217;s &lt;a href='https://github.com/snowplow/snowplow/downloads'&gt;Downloads&lt;/a&gt; section as &lt;strong&gt;snowplow-log-deserializers-0.4.8.jar&lt;/strong&gt;. If you have any problems running it, please &lt;a href='https://github.com/snowplow/snowplow/issues'&gt;raise an issue&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/09/06/snowplow-0.4.7-released"/>
    <title>Snowplow 0.4.7 released</title>
    <updated>2012-09-06T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We have just released Snowplow version &lt;strong&gt;0.4.7&lt;/strong&gt;. This release bumps the Snowplow JavaScript tracker to version &lt;strong&gt;0.6&lt;/strong&gt;, with two significant new features:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The ability to set a site ID for your tracking - useful for multi-site publishers&lt;/li&gt;

&lt;li&gt;The ability to log ecommerce transactions - useful for merchants wanting to track orders&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A huge thanks to community member &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; from &lt;a href='http://www.qwaya.com'&gt;Qwaya&lt;/a&gt; for contributing the ecommerce tracking functionality - thank you Simon!&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll take a look at both of these new features in turn:&lt;/p&gt;

&lt;h2 id='site_id'&gt;Site ID&lt;/h2&gt;

&lt;p&gt;The Snowplow JavaScript tracker now lets you set a site identifier before you start logging events. The new method for this is called &lt;code&gt;setSiteId()&lt;/code&gt; - it takes one argument, the identifier you have assigned to this site. For example:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setAccount&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;d3rkrsqld9gmqf&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setSiteId&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;CFe23a&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The querystring passed to your Snowplow collector will now include the following parameter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...&amp;amp;said=CFe23a&amp;amp;...&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;said&lt;/code&gt; stands for &lt;em&gt;Site or App ID&lt;/em&gt; - because we plan on using the same parameter for mobile and desktop app tracking as well. &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;This new feature should be helpful for anyone running multiple sites (or perhaps clients) against the same Snowplow collector - it means that you can easily partition your Snowplow events by site, whilst still being able to run cross-site analyses should you so wish.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting &lt;code&gt;said&lt;/code&gt; to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/33'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='ecommerce_transactions'&gt;Ecommerce transactions&lt;/h2&gt;

&lt;p&gt;To date, we have been analysing e-commerce transactions using Snowplow by:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Logging every &lt;em&gt;product add to basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Logging every &lt;em&gt;product remove from basket&lt;/em&gt; event&lt;/li&gt;

&lt;li&gt;Netting these events off to determine the final contents of the order&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach works, but it adds complexity in the analysis step. Happily community member Simon Andersson has contributed an alternative solution: dedicated Snowplow e-commerce transaction tracking, similar to the functionality found in the Google Analytics JavaScript API.&lt;/p&gt;

&lt;p&gt;The idea is that you add the new tracking code to your shop&amp;#8217;s checkout confirmation page, so that the completed order can be sent to Snowplow. A complete example of the new tracking code looks like this:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;orderId&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;order-123&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

&lt;span class='c1'&gt;// addTrans sets up the transaction, should be called first.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// affiliation or store name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;8000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// total - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// tax&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// shipping&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// city&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// state or province&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;                      &lt;span class='c1'&gt;// country&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// addItem is called for each item in the shopping cart.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1001&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Blue t-shirt&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;         &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;2&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;addItem&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
  &lt;span class='nx'&gt;orderId&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                &lt;span class='c1'&gt;// order ID - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1002&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// SKU - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;Red shoes&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;            &lt;span class='c1'&gt;// product name&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                     &lt;span class='c1'&gt;// category&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;4000&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;                 &lt;span class='c1'&gt;// unit price - required&lt;/span&gt;
  &lt;span class='s1'&gt;&amp;#39;1&amp;#39;&lt;/span&gt;                     &lt;span class='c1'&gt;// quantity - required&lt;/span&gt;
  &lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='c1'&gt;// trackTrans sends the transaction to Snowplow tracking servers.&lt;/span&gt;
&lt;span class='c1'&gt;// Must be called last to commit the transaction.&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackTrans&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The above example creates an order (aka &amp;#8220;transaction&amp;#8221;) with ID &lt;code&gt;order-123&lt;/code&gt; and then adds two line items (two blue t-shirts and one pair of red shoes) as line items to the order. The final &lt;code&gt;trackTrans&lt;/code&gt; call sends this complete order to Snowplow as three separate events - one each for the order and its line items.&lt;/p&gt;

&lt;p&gt;This new functionality should be useful for anybody who wants to track orders transacted in a online shopping cart such as Magento, PrestaShop or Spree.&lt;/p&gt;

&lt;p&gt;Note that we haven&amp;#8217;t yet added extracting these e-commerce orders to our ETL process, but we have an &lt;a href='https://github.com/snowplow/snowplow/issues/34'&gt;open ticket for this&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id='upgrading'&gt;Upgrading&lt;/h2&gt;

&lt;p&gt;We have made the minified JavaScript tracker version 0.6 available on this URL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://d1fc8wv8zag5ca.cloudfront.net/0.6/sp.js&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are no breaking changes with the previous version 0.5, so you can upgrade your existing Snowplow JavaScript tracker without issue.&lt;/p&gt;

&lt;p&gt;Note that we have now added versioning to the JavaScript tracker&amp;#8217;s URL. This is because we have &amp;#8220;breaking changes&amp;#8221; to the JavaScript tracker in the pipeline (see e.g. issues &lt;a href='https://github.com/snowplow/snowplow/issues/29'&gt;#29&lt;/a&gt; and &lt;a href='https://github.com/snowplow/snowplow/issues/32'&gt;#32&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id='thanks'&gt;Thanks&lt;/h2&gt;

&lt;p&gt;A final note to say thanks again to &lt;a href='https://github.com/ramn'&gt;Simon Andersson&lt;/a&gt; for contributing the ecommerce tracking functionality! Community contributors like Simon A and Simon R(umble) are helping us to quickly make the Snowplow vision a reality.&lt;/p&gt;

&lt;p&gt;And of course, we welcome contributions across the five Snowplow sub-systems. If you would like help implementing a new tracker, trying a different ETL approach or loading Snowplow events into an alternative database, please &lt;a href='mailto:contribute@snowplowanalytics.com'&gt;get in touch&lt;/a&gt;!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/21/amazon-glacier-launch"/>
    <title>Amazon announces Glacier - lowers the cost of running Snowplow</title>
    <updated>2012-08-21T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Today Amazon announced the launch of &lt;a href='http://aws.amazon.com/glacier/'&gt;Amazon Glacier&lt;/a&gt;, which is a low-cost data archiving service designed for rarely accessed data.&lt;/p&gt;

&lt;p&gt;As Werner Vogels described it in his &lt;a href='http://www.allthingsdistributed.com/2012/08/amazon-glacier.html'&gt;blog post&lt;/a&gt; this morning:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Amazon Glacier provides the same high durability guarantee as Amazon S3 but relaxes the access times to a few hours. This is the right service for customers who have archival data that requires highly reliable storage but for which immediate access is not needed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first sight, Amazon Glacier looks to be a fantastic fit for archiving the raw event logs generated by the Snowplow collector (whether the CloudFront collector or alternatives such as &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;). Once the nightly Snowplow ETL has been run on your raw event logs, you shouldn&amp;#8217;t need to access those raw logs frequently. However, we would always recommend retaining them, as there may well be a reason to revisit them in the future. We never recommend throwing away atomic source data!&lt;/p&gt;

&lt;p&gt;This is where Amazon Glacier comes in - at the proposed pricing levels for Glacier, you could archive 2 terabytes of raw Snowplow data for around $20 a month; this would be significantly cheaper than storing your raw logs in Amazon S3, which is the current Snowplow approach.&lt;/p&gt;

&lt;p&gt;Moreover, Werner has indicated that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the coming months, Amazon S3 will introduce an option that will allow customers to seamlessly move data between Amazon S3 and Amazon Glacier based on data lifecycle policies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once Amazon has launched this feature, we&amp;#8217;ll get this automatic S3-&amp;gt;Glacier archiving process working internally, and then release a howto for Snowplow users so you can do the same, and start running your Snowplow over Amazon Glacier!&lt;/p&gt;

&lt;p&gt;Exciting times for everybody who likes storing atomic event data cheaply and safely - stay tuned!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/20/snowplow-0.4.6-released"/>
    <title>Snowplow 0.4.6 released</title>
    <updated>2012-08-20T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Over the weekend we released Snowplow version &lt;strong&gt;0.4.6&lt;/strong&gt;. This was a minor release that added a new capability into the Snowplow JavaScript tracker.&lt;/p&gt;

&lt;p&gt;Specifically, with the JavaScript you can now specify your own collector URL, rather than simply pass in an account ID which resolves to a CloudFront bucket.&lt;/p&gt;

&lt;p&gt;You can use this feature in your JavaScript invocation code like so:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='javascript'&gt;&lt;span class='c'&gt;&amp;lt;!--&lt;/span&gt; &lt;span class='nx'&gt;Snowplow&lt;/span&gt; &lt;span class='nx'&gt;starts&lt;/span&gt; &lt;span class='nx'&gt;plowing&lt;/span&gt; &lt;span class='o'&gt;--&amp;gt;&lt;/span&gt;
&lt;span class='o'&gt;&amp;lt;&lt;/span&gt;&lt;span class='nx'&gt;script&lt;/span&gt; &lt;span class='nx'&gt;type&lt;/span&gt;&lt;span class='o'&gt;=&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;text/javascript&amp;quot;&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&lt;/span&gt;
&lt;span class='kd'&gt;var&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='nx'&gt;_snaq&lt;/span&gt; &lt;span class='o'&gt;||&lt;/span&gt; &lt;span class='p'&gt;[];&lt;/span&gt;

&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;setCollectorUrl&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;collector.mydomain.com&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;
&lt;span class='nx'&gt;_snaq&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='nx'&gt;push&lt;/span&gt;&lt;span class='p'&gt;([&lt;/span&gt;&lt;span class='s1'&gt;&amp;#39;trackPageView&amp;#39;&lt;/span&gt;&lt;span class='p'&gt;]);&lt;/span&gt;

&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kd'&gt;function&lt;/span&gt;&lt;span class='p'&gt;()&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
&lt;span class='p'&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;collector.mydomain.com&lt;/code&gt; is the URL to your own collector.&lt;/p&gt;

&lt;p&gt;We added this capability to Snowplow in support of Simon Rumble&amp;#8217;s excellent &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt; prototype node.js collector for Snowplow. Going forwards you can of course use this custom URL to send your Snowplow events to any kind of collector on a domain you control.&lt;/p&gt;

&lt;p&gt;Anyway I hope you like the feature and let us know how you get on with it!&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/14/updated-hive-serde-released"/>
    <title>Updated Hive SerDe released</title>
    <updated>2012-08-14T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;One of the key elements in the Snowplow technology stack is the Hive SerDe. This is what makes it possible for Elastic MapReduce to read the Cloudfront log files generated by the Snowplow javascript trackings tags, extarct the relevant fields and make these available in Hive as a nice, clean query table. (The structure of the Hive table is documented &lt;a href='https://github.com/snowplow/snowplow/wiki/Hive-data-structure'&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;A number of improvements have been made in the new versions. However, the most significant is that the 5 utm_marketing fields have been added, so that campaign attributes are now available for analytics.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/13/introducing-snow-cannon-a-node-js-collector-for-snowplow"/>
    <title>SnowCannon - a node.js collector for Snowplow</title>
    <updated>2012-08-13T00:00:00+00:00</updated>
    <author>
      <name>Alex</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;We are hugely excited to introduce &lt;a href='https://github.com/shermozle/SnowCannon'&gt;SnowCannon&lt;/a&gt;, a Node.js collector for Snowplow, authored by &lt;a href='http://twitter.com/shermozle'&gt;@shermozle&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is an alternative collector to the default cloudfront collector included with Snowplow. It offers a number of significant advantages over the Cloudfront connector:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It allows the use of 3rd party cookies. In particular, this makes it possible to track usage across multiple domains&lt;/li&gt;

&lt;li&gt;It enables real-time analytics. (This is not possible with the Cloudfront-enabled collector, where there&amp;#8217;s a 20-30 minute delay between the javascript tracking event and the associated log being written to S3.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To learn more about SnowCannon, visit the &lt;a href='https://github.com/shermozle/SnowCannon'&gt;Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;SnowCannon is the first user-contributed module for Snowplow, and we are delighted to see community members working to build out the Snowplow platform. There are other contributions in the works, including a Snowplow IOS client, that we hope to be announcing shortly.&lt;/p&gt;

&lt;p&gt;To encourage users to extend Snowplow, we&amp;#8217;ve architected Snowplow in a module way, to enable developers to swap out elements in the Snowplow stack with their own elements or complimenet those already in the stack with parallel implementations. Learn more about the Snowplow architecture &lt;a href='/product/technical-architecture.html'&gt;here&lt;/a&gt;.&lt;/p&gt;</content>
  </entry>
  
  <entry>
    <id>http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled</id>
    <link type="text/html" rel="alternate" href="http://snowplowanalytics.com/blog/2012/08/02/snowplow-setup-documentation-overhauled"/>
    <title>The setup guide has been overhauled</title>
    <updated>2012-08-02T00:00:00+00:00</updated>
    <author>
      <name>Yali</name>
      <uri>http://snowplowanalytics.com/</uri>
    </author>
    <content type="html">&lt;p&gt;Following a lot of invaluable feedback from users setting up Snowplow for the first time, we&amp;#8217;ve updated the Snowplow setup documentation.&lt;/p&gt;

&lt;p&gt;The documentation can be found &lt;a href='https://github.com/snowplow/snowplow/wiki/Snowplow-setup-guide'&gt;here&lt;/a&gt;. Any further feedback would be much appreciated - we want to make it as painless as possible for Snowplow newbies to get up and running&amp;#8230;&lt;/p&gt;</content>
  </entry>
  
 
</feed>