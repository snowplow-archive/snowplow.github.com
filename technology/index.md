---
layout: page
group: tech
title: Technology
description: Snowplow is architected using the best-of-breed in open source and cloud technology, to deliver a highly scalable, robust, cost effective event analytics platform. Our technology stack leverages Amazon Kinesis, Scalding, Cascading, Hadoop, EMR and Amazon Redshift
---

Snowplow consists of five loosely-coupled subsystems.

![architecture][architecture]

### 1. Trackers

* Trackers integrate with your application(s) and/or website(s).
* Trackers generate event data: when an event occurs, they put together a packet of data and send it to a Snowplow collector.
* We have two client-side trackers: a [Javascript tracker] [js-tracker] for tracking user interactions on websites and web apps, and a [No-JS (also called 'pixel') tracker] [no-js-tracker] for tracking user behavior in web-environments that do not support Javascript e.g. email open events.
* We offer a number of server-side trackers including a [Python tracker] [python-tracker], [Ruby tracker] [ruby-tracker], [Java tracker] [java-tracker], [Lua tracker] [lua-tracker] and [Node-JS] [node-js] tracker. A PHP tracker is in development.
* We have both an [iOS tracker][iOS] and an [Android tracker][android] for capturing events from mobile apps
* We offer an [Arduino tracker][arduino-tracker] for capturing data from the Internet of Things.


The [Snowplow Tracker Protocol] [tracker-protocol] provides a standard way for *any* tracker to feed data into Snowplow. It is documented [here] [tracker-protocol].

### 2. Collectors

* Collectors receive Snowplow event data from trackers and push it to a queue to be processed.
* Currently we have a [Cloudfront collector] [cf-collector] for tracking user activity across a single domain, a [Clojure collector] [clj-collector] for tracking activity across multiple domains and a [Scala stream collector] [scala-stream-collector] for tracking users across multiple domains in real-time.
* The Clojure collector runs on [Amazon Elastic Beanstalk] [beanstalk].
* The Scala Stream collector is built to work with [Amazon Kinesis] [kinesis].

### 3. Enrichment

* The enrichment process takes the raw data generated by the collector, validates it, cleans it up  and enriches them. (E.g. infers geographical location from IP addresses, and referer data from referer URLs).
* The enrichment process is written in Scala. It can be run on top of [Scalding] [scalding] / [Cascading][cascading] and [Amazon EMR][emr], as a batch-based process. It can also be run on [Amazon Kinesis] [kinesis], to process incoming data in real time.

### 4. Storage

* Snowplow can be setup to load your event-level and customer-level data into one or more data stores, to enable analytics.
* Snowplow data is delivered into [Amazon S3] [s3] (for processing by [Hive] [hive] / [Pig] [pig] on EMR).
* In addition, Snowplow supports loading the data into [Amazon Redshift] [redshift] and [PostgreSQL] [postgres] for analysis in more traditional tools (e.g. [R] [r], [Looker] [looker] and [Excel] [excel]). Amazon Redshift enables Snowplow users to query Petabytes of Snowplow data quickly and conveniently via its Postgres API.
* Going forwards, we plan to support more storage targets to enable a broader set of analyses, including [Neo4J] [neo4j], [Elastic Search] [elastic-search] and [Google BigQuery] [bigquery].

Snowplow data is stored in each storage option above as close to the [Snowplow Canonical Event Model] [event-model] as possible. The data model is described [here] [event-model].

### 5. Analytics

Once your Snowplow data is available in storage, you can plug it into multiple different tools to crunch that data. Examples include:

* Exploring and mining your data using [Looker][looker]
* Create dashboards and scorecards with the data using [ChartIO] [chartio].
* Perform [OLAP analysis] [olap] (i.e. slice and dice different metrics against different metrics) using [PivotTables in Excel] [excel] or [Tableau] [tableau].
* Mine and model the data, to perform marketing, catalog or platform analytics, using [R] [r] or [Python] [python].
* Develop and run machine learning algorithms, using [Mahout] [mahout], [Python] [python] or [Weka] [weka] to develop recommendation engines or clusters audience by behaviour and interest.

## Learn more

* View the [Github repo] [github-repo] to see the source code for each subsystem listed above.
* View the [technical documentation] [tech-docs] to learn more about each subsystem.

## Built on AWS

Snowplow is built on top AWS, and makes extensive use of Cloudfront, Elastic Beanstalk, Elastic Mapreduce and Amazon Redshift.

<img src="/assets/img/APN_Standard_Technology_Partner.png" title="Amazon Web Services Technology Partner" width="250" />

*We are proud to be an Amazon Web Services Technology Partner.*


[js-tracker]: https://github.com/snowplow/snowplow-javascript-tracker
[no-js-tracker]: https://github.com/snowplow/snowplow/tree/master/1-trackers/no-js-tracker
[java-tracker]: https://github.com/snowplow/snowplow-java-tracker
[ruby-tracker]: https://github.com/snowplow/snowplow-ruby-tracker
[cf-collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors/cloudfront-collector
[clj-collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors/clojure-collector
[scala-stream-collector]: https://github.com/snowplow/snowplow/tree/master/2-collectors/scala-stream-collector
[node-js]: https://github.com/snowplow/snowplow-nodejs-tracker
[iOS]: https://github.com/snowplow/snowplow-ios-tracker
[android]: https://github.com/snowplow/snowplow-android-tracker

[scalding]: https://github.com/twitter/scalding
[cascading]: http://www.cascading.org/
[chartio]: https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data
[tableau]: https://github.com/snowplow/snowplow/wiki/Setting-up-Tableau-to-analyze-your-Snowplow-data
[excel]: https://github.com/snowplow/snowplow/wiki/Setting-up-Excel-to-analyze-Snowplow-data
[r]: https://github.com/snowplow/snowplow/wiki/Setting-up-R-to-perform-more-sophisticated-analysis-on-your-Snowplow-data
[weka]: http://weka.pentaho.com/
[mahout]: http://mahout.apache.org/
[python]: http://scikit-learn.org/stable/
[hive]: http://hive.apache.org/
[pig]: http://pig.apache.org/
[redshift]: http://aws.amazon.com/redshift/
[ice]: http://www.infobright.org/
[s3]: http://aws.amazon.com/s3/
[redshift]: http://aws.amazon.com/redshift/

[github-repo]: http://github.com/snowplow/snowplow
[snowplow-wiki]: http://github.com/snowplow/snowplow/wiki
[setup-guide]: https://github.com/snowplow/snowplow/wiki/Setting-up-Snowplow
[tech-docs]: https://github.com/snowplow/snowplow/wiki/Snowplow%20technical%20documentation
[architecture]: /assets/img/technical-architecture.png
[python-tracker]: https://github.com/snowplow/snowplow-python-tracker
[lua-tracker]: https://github.com/snowplow/snowplow-lua-tracker
[arduino-tracker]: https://github.com/snowplow/snowplow-arduino-tracker
[olap]: /analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html
[roadmap]: https://github.com/snowplow/snowplow/wiki/Product-roadmap

[tracker-protocol]: https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol
[event-model]: https://github.com/snowplow/snowplow/wiki/canonical-event-model
[beanstalk]: http://aws.amazon.com/elasticbeanstalk/
[emr]: http://aws.amazon.com/elasticmapreduce/
[postgres]: http://www.postgresql.org/
[neo4j]: http://www.neo4j.org/
[kinesis]: http://aws.amazon.com/kinesis/
[elastic-search]: http://www.elasticsearch.com/
[bigquery]: https://cloud.google.com/products/bigquery/
[looker]: http://looker.com/
